[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PHCM9795: Foundations of Biostatistics",
    "section": "",
    "text": "Course introduction\nWelcome to PHCM9795 Foundations of Biostatistics.\nThis introductory course in biostatistics aims to provide students with core biostatistical skills to analyse and present quantitative data from different study types. These are essential skills required in your degree and throughout your career.\nWe hope you enjoy the course and will value your feedback and comment throughout the course."
  },
  {
    "objectID": "index.html#course-information",
    "href": "index.html#course-information",
    "title": "PHCM9795: Foundations of Biostatistics",
    "section": "Course information",
    "text": "Course information\nBiostatistics is a foundational discipline needed for the analysis and interpretation of quantitative information and its application to population health policy and practice.\nThis course is central to becoming a population health practitioner as the concepts and techniques developed in the course are fundamental to your studies and practice in population health. In this course you will develop an understanding of, and skills in, the core concepts of biostatistics that are necessary for analysis and interpretation of population health data and health literature.\nIn designing this course, we provide a learning sequence that will allow you to obtain the required graduate capabilities identified for your program. This course is taught with an emphasis on formulating a hypothesis and quantifying the evidence in relation to a specific research question. You will have the opportunity to analyse data from different study types commonly seen in population health research.\nThe course will allow those of you who have covered some of this material in your undergraduate and other professional education to consolidate your knowledge and skills. Students exposed to biostatistics for the first time may find the course challenging at times. Based on student feedback, the key to success in this course is to devote time to it every week. We recommend that you spend an average of 10-15 hours per week on the course, including the time spent reading the course notes and readings, listening to lectures, and working through learning activities and completing your assessments. Please use the resources provided to assist you, including online support."
  },
  {
    "objectID": "index.html#units-of-credit",
    "href": "index.html#units-of-credit",
    "title": "PHCM9795: Foundations of Biostatistics",
    "section": "Units of credit",
    "text": "Units of credit\nThis course is a core course of the Master of Public Health, Master of Global Health and Master of Infectious Diseases Intelligence programs and associated dual degrees, comprising 6 units of credit towards the total required for completion of the study program. A value of 6 UOC requires a minimum of 150 hours work for the average student across the term."
  },
  {
    "objectID": "index.html#course-aim",
    "href": "index.html#course-aim",
    "title": "PHCM9795: Foundations of Biostatistics",
    "section": "Course aim",
    "text": "Course aim\nThis course aims to provide students with the core biostatistical skills to apply appropriate statistical techniques to analyse and present population health data."
  },
  {
    "objectID": "index.html#learning-outcomes",
    "href": "index.html#learning-outcomes",
    "title": "PHCM9795: Foundations of Biostatistics",
    "section": "Learning outcomes",
    "text": "Learning outcomes\nOn successful completion of this course, you will be able to:\n\nSummarise and visualise data using statistical software.\nDemonstrate an understanding of statistical inference by interpreting p-values and confidence intervals.\nApply appropriate statistical tests for different types of variables given a research question, and interpret computer output of these tests appropriately.\nDetermine the appropriate sample size when planning a research study.\nPresent and interpret statistical findings appropriate for a population health audience.\n\n\nChangelog\n2023-07-17\n[Changed]\n\nSection 7.9: Corrected screenshots to test difference in paired proportions in Stata.\n\n2023-07-13\n[Changed]\n\nSection 7.13: tidied up the R function used to calculate the 95% confidence interval for the difference in paired proportions.\n\n2023-07-12\n[Changed]\n\nWorked Example 6.2: removed “This z-statistic does not meet or exceed the critical value of 1.96 for a two tailed test” and re-framed this in terms of interpreting the P-value as calculated from software.\nWorked Example 7.1: corrected column headings for Nausea and No nausea\n\n2023-07-01\n[Changed]\n\nSection 4.2: Fixed typo: “The particular test statistic differs depending on the type of data being analyses analysed”\nSection 4.4: Fixed typo: “This is the situation described in scenario (c) of Figure Figure 4.1.”\nActivity 5.2: Added “or R” to the instruction “Use Stata to conduct an appropriate statistical test”\nActivity 5.3: Added “or R” to the instruction “Use Stata to conduct an appropriate statistical test”\nSection 6.3: Fixed formula for testing one sample proportion to: “\\(z = \\frac{(p_{sample} - p_{population})}{\\text{SE}(p_{population})}\\)”\nActivity 7.2: Added “or R” to the instruction “Using Stata, carry out the appropriate significance test”\n\n2023-07-01\n[Changed]\n\nRenamed “Readings” to “Optional readings”\nModule 4, Section 4.8: Corrected the sentence that describes Figure 4.4. “the shaded region for a one-tailed test would be doubled retained on one side of the distribution and eliminated from the other side of the distribution”.\nModule 9: Added titles to Tables 9.3 and 9.4.\nModule 9, Section 9.4: Corrected the level of evidence: “providing strong evidence of a difference in the median length of stay between the groups.”\nModule 9, Section 9.5.1. Corrected the text under Table 9.4: “The data shows … 10 people who have a negative positive difference.”\n\n2023-06-13\n[Changed]\n\nModule 3. Clarified Worked Example 3.1, and moved the example from Section 3.5.1 to Section 3.5.2.\nSection 3.6: Added Stata output for calculating a 95% confidence interval from individual data.\n\n2023-06-01\n[Changed]\n\nSection 1.14.3: RStudio preferences are now located at Edit &gt; Settings on MacOS, and Tools &gt; Global Options on Windows\nSection 1.14.7.2: Correct layout for commands to install packages (commands must be entered on separate lines)\nSection 1.15.2: Correct the name of the pbc data set to mod_01_pdc.rds\nActivity 2.3(b): Change the request to plot data on a Normal curve, not a standardised Normal curve\nActivity 5.3 and 5.4: added underscores to filenames"
  },
  {
    "objectID": "08-correlation-and-regression.html",
    "href": "08-correlation-and-regression.html",
    "title": "1  Correlation and linear regression",
    "section": "",
    "text": "Stata notes\nWe will demonstrate using R for correlation and simple linear regression using the dataset mod08_lung_function.rds.\nlung &lt;- readRDS(\"data/examples/mod08_lung_function.rds\")"
  },
  {
    "objectID": "08-correlation-and-regression.html#learning-objectives",
    "href": "08-correlation-and-regression.html#learning-objectives",
    "title": "1  Correlation and linear regression",
    "section": "Learning objectives",
    "text": "Learning objectives\nBy the end of this module you will be able to:\n\nExplore the association between two continuous variables using a scatter plot;\nEstimate and interpret correlation coefficients;\nEstimate and interpret parameters from a simple linear regression;\nDecide whether a regression model is valid;\nTest a hypothesis using regression coefficients;\nOutline the concept of multiple regression and its role in investigative epidemiology."
  },
  {
    "objectID": "08-correlation-and-regression.html#optional-readings",
    "href": "08-correlation-and-regression.html#optional-readings",
    "title": "1  Correlation and linear regression",
    "section": "Optional readings",
    "text": "Optional readings\nKirkwood and Sterne (2001); Chapter 10. [UNSW Library Link]\nBland (2015); Chapter 11. [UNSW Library Link]\nAcock (2010); Chapter 8."
  },
  {
    "objectID": "08-correlation-and-regression.html#introduction",
    "href": "08-correlation-and-regression.html#introduction",
    "title": "1  Correlation and linear regression",
    "section": "1.1 Introduction",
    "text": "1.1 Introduction\nIn Module 5, we saw how to test whether the means from two groups are equal - in other words, whether a continuous variable is related to a categorical variable. Sometimes we are interested in how closely two continuous variables are related. For example, we may want to know how closely blood cholesterol levels are related to dietary fat intake in adult men. To measure the strength of association between two continuously distributed variables, a correlation coefficient is used.\nWe may also want to predict a value of a continuous measurement from another continuous measurement. For example, we may want to know predict values of lung capacity from height in a community of adults. A regression model allows us to use one measurement to predict another measurement.\nAlthough both correlation coefficients and regression models can be used to describe the degree of association between two continuous variables, the two methods provide different information. It is important to note that both methods summarise the strength of an association between variables, and do not imply a causal relationship."
  },
  {
    "objectID": "08-correlation-and-regression.html#notation",
    "href": "08-correlation-and-regression.html#notation",
    "title": "1  Correlation and linear regression",
    "section": "1.2 Notation",
    "text": "1.2 Notation\nIn this module, we will be focussing on the association between two variables, denoted \\(x\\) and \\(y\\).\nThere may be cases where it does not matter which variable is denoted \\(x\\) and which is denoted \\(y\\), however this is rare. We are usually interested in whether one variable is associated with another. If we believe that a change in \\(x\\) will lead to a change in \\(y\\), or that \\(y\\) is influenced by \\(x\\), we define \\(y\\) as the outcome variable and \\(x\\) as the explanatory variable."
  },
  {
    "objectID": "08-correlation-and-regression.html#correlation",
    "href": "08-correlation-and-regression.html#correlation",
    "title": "1  Correlation and linear regression",
    "section": "1.3 Correlation",
    "text": "1.3 Correlation\nWe use correlation to measure the strength of a linear relationship between two variables. Before calculating a correlation coefficient, a scatter plot should first be obtained to give an understanding of the nature of the relationship between the two variables.\n\n1.3.1 Worked Example\nThe file mod08_lung_function.csv has information about height and lung function collected from a random sample of 120 adults. Information was collected on height (cm) and lung function, which was measured as forced vital capacity (FVC), measured in litres. We can obtain a scatter-plot shown in Figure 1.1, where the outcome variable (\\(y\\)) is plotted on the vertical axis, and the explanatory variable (\\(x\\)) is plotted on the horizontal axis.\nFigure 1.1 shows that as height increases, lung function also increases, which is as expected. One or two of the data points are separated from the rest of the data but are not so far away as to be considered outliers because they do not seem to stand out of other observations.\n\n\n\n\n\nFigure 1.1: Association between height and lung function in 120 adults\n\n\n\n\n\n\n1.3.2 Correlation coefficients\nA correlation coefficient (r) describes how closely the variables are related, that is the strength of linear association between two continuous variables. The range of the coefficient is from +1 to −1 where +1 is a perfect positive association, 0 is no association and −1 is a perfect inverse association. In general, an absolute (disregarding the sign) r value below 0.3 indicates a weak association, 0.3 to &lt; 0.6 is fair association, 0.6 to &lt; 0.8 is a moderate association, and \\(\\ge\\) 0.8 indicates a strong association.\nThe correlation coefficient is positive when large values of one variable tend to occur with large values of the other, and small values of one variable (y) tend to occur with small values of the other (x) (Figure 1.2 (a and b)). For example, height and weight in healthy children or age and blood pressure.\nThe correlation coefficient is negative when large values of one variable tend to occur with small values of the other, and small values of one variable tend to occur with large values of the other (Figure 1.2 (c and d)). For example, percentage immunised against infectious diseases and under-five mortality rate.\n\n\n\n\n\nFigure 1.2: Scatter plots demonstrating strong and weak, positive and negative associations\n\n\n\n\nIt is possible to calculate a P-value associated with a correlation coefficient to test whether the correlation coefficient is different from zero. However, a correlation coefficient with a large P-value does not imply that there is no relationship between \\(x\\) and \\(y\\), because the correlation coefficient only tests for a linear association and there may be a non-linear relationship such as a curved or irregular relationship.\nThe assumptions for using a Pearson’s correlation coefficient are that:\n\nobservations are independent;\nboth variables are continuous variables;\nthe relationship between the two variables is linear.\n\nThere is a further assumption that the data follow a bivariate normal distribution. This assumes: y follows a normal distribution for given values of x; and x follows a normal distribution for given values of y. This is quite a technical assumption that we do not discuss further.\nThere are two types of correlation coefficients– the correct one to use is determined by the nature of the variables as shown in Table 1.1.\n\n\n\n\nTable 1.1:  Correlation coefficients and their application \n\nCorrelation coefficientApplication\n\nPearson’s correlation coefficient: rBoth variables are continuous and a bivariate normal distribution can be assumed\n\nSpearman’s rank correlation: rhoBivariate normality cannot be assumed. Also useful when at least one of the variables is ordinal\n\n\n\n\n\nSpearman’s \\(\\rho\\) is calculated using the ranks of the data, rather than the actual values of the data. We will see further examples of such methods in Module 9, when we consider non-parametric tests, which are often based on ranks.\nCorrelation coefficients are often presented in the form of a correlation matrix which can display the correlation between a number of variables in a single table (Table 1.2).\n\n\nTable 1.2: Correlation matrix for Height and FVC\n\n\n\n\n\n\n\n\nHeight\nFVC\n\n\nHeight\n1\n0.70\nP &lt; 0.0001\n\n\nFVC\n0.70\nP &lt; 0.0001\n1\n\n\n\n\nThis correlation matrix shows that the Pearson’s correlation coefficient between height and lung function is 0.70 with P&lt;0.0001 indicating very strong evidence of a linear association between height and FVC. A correlation matrix sometimes includes correlations between the same variable, indicated as a correlation coefficient of 1. For example, \\(Height\\) is perfectly correlated with itself (i.e. has a correlation coefficient of 1). Similarly, \\(FVC\\) is perfectly correlated with itself.\nCorrelation coefficients are rarely used as important statistics in their own right because they do not fully explain the relationship between the two variables and the range of the data has an important influence on the size of the coefficient. In addition, the statistical significance of the correlation coefficient is often over interpreted because a small correlation which is of no clinical importance can become statistically significant even with a relatively small sample size. For example, a poor correlation of 0.3 will be statistically significant if the sample size is large enough."
  },
  {
    "objectID": "08-correlation-and-regression.html#linear-regression",
    "href": "08-correlation-and-regression.html#linear-regression",
    "title": "1  Correlation and linear regression",
    "section": "1.4 Linear regression",
    "text": "1.4 Linear regression\nThe nature of a relationship between two variables is more fully described using regression, where the relationship is described by a straight line.\nFigure 1.3 shows our lung data with a fitted regression line.\n\n\n\n\n\nFigure 1.3: Association between height and lung function in 120 adults\n\n\n\n\nThe line through the plot is called the line of ‘best fit’ because the size of the deviations between the data points and the line is minimised in estimating the line.\n\n1.4.1 Regression equations\nThe mathematical equation for the line explains the relationship between two variables: \\(y\\), the outcome variable, and \\(x\\), the explanatory variable. The equation of the regression line is as follows:\n\\[y = \\beta_{0} + \\beta_{1}x\\]\nThis line is shown in Figure 1.4 using the notation shown in Table 1.3.\n\n\n\n\n\nFigure 1.4: Coefficients of a linear regression equation\n\n\n\n\n\n\n\n\nTable 1.3:  Notation for linear regression equation \n\nSymbolInterpretation\n\n\\(y \\)The outcome variable\n\n\\(x \\)The explanatory variable\n\n\\(\\beta_0\\)Intercept of the regression line\n\n\\(\\beta_1\\)Slope of the regression line\n\n\n\n\n\nThe intercept is the point at which the regression line intersects with the y-axis when the value of \\(x\\) is zero. In most cases, the intercept does not have a biologically meaningful interpretation as the explanatory variable cannot take a value of zero. In our working example, the intercept is not meaningful as it is not possible for an adult to have a height of 0cm.\nThe slope of the line is the predicted change in the outcome variable \\(y\\) as the explanatory explanatory variable \\(x\\) increases by 1 unit.\nAn important concept is that regression predicts an expected value of \\(y\\) given an observed value of \\(x\\): any error around the explanatory variable is not taken into account."
  },
  {
    "objectID": "08-correlation-and-regression.html#regression-coefficients-estimation",
    "href": "08-correlation-and-regression.html#regression-coefficients-estimation",
    "title": "1  Correlation and linear regression",
    "section": "1.5 Regression coefficients: estimation",
    "text": "1.5 Regression coefficients: estimation\nThe regression parameters \\(\\beta_{0}\\) and \\(\\beta_{1}\\) are true, unknown quantities (similar to \\(\\mu\\) and \\(\\sigma\\)), which are estimated using statistical software using the method of least squares. This method estimates the intercept and the slope, and also their variability (i.e. standard errors). Software is always used to estimate the regression parameters from a set of data.\nUsing the method of least squares:\n\nthe intercept is estimated as \\(b_0\\);\nthe slope is estimated as \\(b_1\\)."
  },
  {
    "objectID": "08-correlation-and-regression.html#regression-coefficients-inference",
    "href": "08-correlation-and-regression.html#regression-coefficients-inference",
    "title": "1  Correlation and linear regression",
    "section": "1.6 Regression coefficients: inference",
    "text": "1.6 Regression coefficients: inference\nWe can use the estimated regression coefficients and their variability to calculate 95% confidence intervals. Here, a t-value from a t-distribution with \\(n - 2\\) degrees of freedom is used:\n\n95% confidence interval for intercept: \\(b_0 \\pm t_{n-2} \\times SE(b_0)\\)\n95% confidence interval for slope: \\(b_1 \\pm t_{n-2} \\times SE(b_1)\\)\n\nNote that as the constant (\\(b_0\\)) is not often biologically plausible, the 95% confidence interval for the constant is often not reported.\nThe significance of the estimated slope (and less commonly, intercept) can be tested using a t-test. The null hypotheses and the alternative hypothesis for testing the slope of a simple linear regression model are:\n\nH0: \\(\\beta_1 = 0\\)\nH1: \\(\\beta_1 \\ne 0\\)\n\nTo test the null hypothesis for the regression coefficient \\(\\beta_1\\), the following t-test is used:\n\\[t = b_1 /SE(b_1)\\]\nThis will give a t statistic which can be referred to a t distribution with n − 2 degrees of freedom to calculate the corresponding P-value.\nTable 1.4 shows the estimated regression coefficients for our working example.\n\n\n\n\nTable 1.4:  Estimated regression coefficients \n\nTermEstimateStandard errort valueP value95% Confidence interval\n\nIntercept-18.9 2.19 t=-8.60, 118df&lt;0.001-23.22 to -14.53\n\nHeight0.140.013t=10.58, 118df&lt;0.0010.11 to 0.17\n\n\n\n\n\nFrom this output, we see that the slope is estimated as 0.14 with an estimated intercept of -18.87. Therefore, the regression equation is estimated as:\nFVC (L) = − 18.87 + (0.14 \\(\\times\\) Height in cm)\nThere is very strong evidence of a linear association between FVC and height in cm (P &lt; 0.001).\nThis equation can be used to predict FVC for a person of a given height. For example, the predicted FVC for a person 165 cm tall is estimated as:\nFVC = − 18.87347 + (0.1407567 \\(\\times\\) 165.0) = 4.40 L.\nNote that for the purpose of prediction we have kept all the decimal places in the coefficients to avoid rounding error in the intermediate calculation.\n\n1.6.1 Fit of a linear regression model\nAfter fitting a linear regression model, it is important to know how well the model fits the observed data. One way of assessing the model fit is to compute a statistic called coefficient of determination, denoted by \\(R^2\\). It is the square of the Pearson correlation coefficient \\(r: r^2 = R^2\\). Since the range of \\(r\\) is from −1 to 1, \\(R^2\\) must lie between 0 and 1.\n\\(R^2\\) can be interpreted as the proportion of variability in y that can be explained by variability in x. Hence, the following conditions may arise:\nIf \\(R^2 = 1\\), then all variation in y can be explained by variation of x and all data points fall on the regression line.\nIf \\(R^2 = 0\\), then none of the variation in y is related to x at all, and the variable x explains none of the variability in y.\nIf \\(0 &lt; R^2 &lt;1\\), then the variability of y can be partially explained by the variability in x. The larger the \\(R^2\\) value, the better is the fit of the regression model."
  },
  {
    "objectID": "08-correlation-and-regression.html#assumptions-for-linear-regression",
    "href": "08-correlation-and-regression.html#assumptions-for-linear-regression",
    "title": "1  Correlation and linear regression",
    "section": "1.7 Assumptions for linear regression",
    "text": "1.7 Assumptions for linear regression\nRegression is robust to moderate degrees of non-normality in the variables, provided that the sample size is large enough and that there are no influential outliers. Also, the regression equation describes the relationship between the variables and this is not influenced as much by the spread of the data as the correlation coefficient is.\nThe assumptions that must be met when using linear regression are as follows:\n\nobservations are independent;\nthe relationship between the explanatory and the outcome variable is linear;\nthe residuals are normally distributed.\n\nA residual is defined as the difference between the observed and predicted outcome from the regression model. If the predicted value of the outcome variable is denoted by \\(\\hat y\\) then:\n\\[ \\text{Residual} = \\text{observed} - \\text{predicted} = y - \\hat y\\]\nIt is important for regression modelling that the data are collected in a period when the relationship remains constant. For example, in building a model to predict normal values for lung function the data must be collected when the participants have been resting and not exercising and people taking bronchodilator medications that influence lung capacity should be excluded. In regression, it is not so important that the variables themselves are normally distributed, but it is important that the residuals are. Scatter plots and specific diagnostic tests can be used to check the regression assumptions. Some of these will not be covered in this introductory course but will be discussed in detail in the Regression Methods in Biostatistics course.\nThe distribution of the residuals should always be checked. Large residuals can indicate unusual points or points that may exert undue influence on the estimated regression slope.\nThe histogram of residuals from the model is shown in Figure 1.5. The residuals are approximately normally distributed, with no outlying values.\n\n\n\n\n\nFigure 1.5: Histogram of regression residuals"
  },
  {
    "objectID": "08-correlation-and-regression.html#multiple-linear-regression",
    "href": "08-correlation-and-regression.html#multiple-linear-regression",
    "title": "1  Correlation and linear regression",
    "section": "1.8 Multiple linear regression",
    "text": "1.8 Multiple linear regression\nIn the above example, we have only used a simple linear regression model of two continuous variables. Other more complex models can be built from this e.g. if we wanted to look at the effect of gender (male vs. female) as binary indicator in the model while adjusting for the effect of height. In that case we would include both the variables in the model as explanatory variables. In the same way we can include any number of explanatory variables (both continuous and categorical) in the model: this is called a multivariable model. Multivariable models are often used for building predictive equations, for example by using age, height, gender and smoking history to predict lung function, or to adjust for confounding and detect effect modification to investigate the association between an exposure and an outcome factor.\nMultiple regression has an important role in investigating causality in epidemiology. The exposure variable under investigation must stay in the model and the effects of other variables which can be confounders or effect-modifiers are tested. The biological, psychological or social meaning of the variables in the model and their interactions are of great importance for interpreting theories of causality.\nOther multivariable models include binary logistic regression for use with a binary outcome variable, or Cox regression for survival analyses. These models, together with multiple regression, will be taught in PHCM9517: Regression Methods in Biostatistics."
  },
  {
    "objectID": "08-correlation-and-regression.html#creating-a-scatter-plot",
    "href": "08-correlation-and-regression.html#creating-a-scatter-plot",
    "title": "1  Correlation and linear regression",
    "section": "1.9 Creating a scatter plot",
    "text": "1.9 Creating a scatter plot\nWe will demonstrate using Stata for correlation and simple linear regression using the dataset mod08_lung_function.dta.\nTo create a scatter plot to explore the association between height and FVC click: Graphics &gt; Twoway graph (scatter, line, etc.). In the twoway dialog box, click Create…\n\n\n\n\n\nA new dialog box will open. Select the Basic plots radio button and highlight Scatter under Basic plots: (select type). Choose FVC for the Y variable and Height for the X variable.\n\n\n\n\n\nClick the Accept button in the Plot 1 dialog box to return to the twoway dialog box, then click the OK or Submit button to produce the scatter plot shown in Figure 8.1.\n[Command: twoway (scatter FVC Height)]\nTo add a fitted line, go back to the twoway dialog box. If you clicked the OK button, you can go to Graphics &gt; Twoway graph (scatter, line, etc.) to bring it back again.\n\n\n\n\n\nClick Create…, then select the Fit plots radio button and Linear prediction under Fit plots: (select type). Choose FVC for the Y variable and Height for the X variable.\n\n\n\n\n\nClick the Accept button, then the OK or Submit button to produce the scatterplot below.\n[Command: twoway (scatter FVC Height) (lfit FVC Height)]\n\n\n\n\n\nNotice that a legend now appears, and the y-axis title is missing. To add a y-axis title, go to the Y axis tab in the twoway dialog box to enter your title as shown below.\n\n\n\n\n\nYou can click the Submit button to check how the scatter plot looks like. Next go the Legend tab and select the Hide legend radio button.\n\n\n\n\n\nClick the OK or Submit button when you are finished to produce Figure 8.3.\n[Command: twoway (scatter FVC Height) (lfit FVC Height), ytitle(Forced vital capacity (L)) legend(off)]\nTo save your graph, go to File &gt; Save in the Graph window, and be sure to save your file as a PNG file:"
  },
  {
    "objectID": "08-correlation-and-regression.html#calculating-a-correlation-coefficient",
    "href": "08-correlation-and-regression.html#calculating-a-correlation-coefficient",
    "title": "1  Correlation and linear regression",
    "section": "1.10 Calculating a correlation coefficient",
    "text": "1.10 Calculating a correlation coefficient\nTo calculate the Pearson’s correlation using the dataset mod08_lung_function.dta go to: Statistics &gt; Summaries, tables, and tests &gt; Summary and descriptive statistics &gt; Pairwise correlations\nSelect the two variables, FVC and Height in the Variables box. You can click the Submit button to check the output. Next, tick the box for Print significance level for each entry to obtain the P-value and the box for Print number of observations for each entry to obtain the number of observations used as shown below.\n\n\n\n\n\nClick the OK or the Submit button when you are done to produce Output 8.1,\n[Command: pwcorr Height FVC, obs sig]"
  },
  {
    "objectID": "08-correlation-and-regression.html#fitting-a-simple-linear-regression-model",
    "href": "08-correlation-and-regression.html#fitting-a-simple-linear-regression-model",
    "title": "1  Correlation and linear regression",
    "section": "1.11 Fitting a simple linear regression model",
    "text": "1.11 Fitting a simple linear regression model\nWe will fit a simple linear regression model with mod08_lung_function.dta to quantify the relationship between FVC and height.\nChoose Statistics &gt; Linear models and related &gt; Linear regression\nIn the regress dialog box, select FVC as the Dependent variable, and Height as the Independent variable.\n\n\n\n\n\nClick the OK or the Submit button when you are done to produce Outputs 8.2 and 8.3.\n[Command: reg FVC Height]"
  },
  {
    "objectID": "08-correlation-and-regression.html#plotting-residuals-from-a-simple-linear-regression",
    "href": "08-correlation-and-regression.html#plotting-residuals-from-a-simple-linear-regression",
    "title": "1  Correlation and linear regression",
    "section": "1.12 Plotting residuals from a simple linear regression",
    "text": "1.12 Plotting residuals from a simple linear regression\nTo obtain the residuals, go to Statistics &gt; Post estimation after running the regress command.\nIn the Postestimation Selector dialog box, select Predictions and their SEs, leverage statistics, distance statistics, etc. in the list under Predictions as shown below.\n\n\n\n\n\nIn the predict dialog box, choose the Residuals button and enter a New variable name (e.g. FVC_resid) for the residuals from the regression model.\n\n\n\n\n\nClick OK button when you are done.\n[Command: predict FVC_resid, residuals]\nYou can now check the assumption that the residuals are normally distributed by creating a histogram with the normal curve using Graphics &gt; Histogram as shown in Stata Notes section for Module 2. Below is the histogram dialog box used to produce the graph in Figure 8.5.\n\n\n\n\n\n[Command: histogram FVC_resid, bin(12) frequency normal]"
  },
  {
    "objectID": "08-correlation-and-regression.html#creating-a-scatter-plot-1",
    "href": "08-correlation-and-regression.html#creating-a-scatter-plot-1",
    "title": "1  Correlation and linear regression",
    "section": "1.13 Creating a scatter plot",
    "text": "1.13 Creating a scatter plot\nWe can use the plot function to create a scatter plot to explore the association between height and FVC, assigning meaningful labels with the xlab and ylab commands:\n\nplot(x=lung$Height, y=lung$FVC, \n     xlab=\"Height (cm)\", \n     ylab=\"Forced vital capacity (L)\")\n\n\n\n\n\ngf_point(FVC ~ Height, data=lung,\n     xlab=\"Height (cm)\", \n     ylab=\"Forced vital capacity (L)\") +\n  gf_theme(theme_minimal())\n\n\n\n\nTo add a fitted line, we can use the abline() function which adds a straight line to the plot. The equation of this straight line will be determined from the estimated regression line, which we specify with the lm() function, which fits a linear model.\nThe basic syntax of the lm() function is: lm(y ~ x) where y represents the outcome variable, and x represents the explanatory variable. Putting this all together:\n\nplot(x=lung$Height, y=lung$FVC,\n     xlab=\"Height (cm)\",\n     ylab=\"Forced vital capacity (L)\")\n\nabline(lm(lung$FVC ~ lung$Height))\n\n\n\n\n\ngf_point(FVC ~ Height, data=lung,\n     xlab=\"Height (cm)\", \n     ylab=\"Forced vital capacity (L)\") +\n  geom_lm() +\n  gf_theme(theme_minimal())\n\nWarning: Using the `size` aesthetic with geom_line was deprecated in ggplot2 3.4.0.\nℹ Please use the `linewidth` aesthetic instead."
  },
  {
    "objectID": "08-correlation-and-regression.html#calculating-a-correlation-coefficient-1",
    "href": "08-correlation-and-regression.html#calculating-a-correlation-coefficient-1",
    "title": "1  Correlation and linear regression",
    "section": "Calculating a correlation coefficient",
    "text": "Calculating a correlation coefficient\nWe can use the corrMatrix function in the Jamovi package to calculate a Pearson’s correlation coefficient:\n\ncorrMatrix(data=lung, vars=c(Height, FVC))\n\n\n CORRELATION MATRIX\n\n Correlation Matrix                                   \n ──────────────────────────────────────────────────── \n                            Height        FVC         \n ──────────────────────────────────────────────────── \n   Height    Pearson's r             —                \n             df                      —                \n             p-value                 —                \n                                                      \n   FVC       Pearson's r     0.6976280            —   \n             df                    118            —   \n             p-value        &lt; .0000001            —   \n ────────────────────────────────────────────────────"
  },
  {
    "objectID": "08-correlation-and-regression.html#fitting-a-simple-linear-regression-model-1",
    "href": "08-correlation-and-regression.html#fitting-a-simple-linear-regression-model-1",
    "title": "1  Correlation and linear regression",
    "section": "1.14 Fitting a simple linear regression model",
    "text": "1.14 Fitting a simple linear regression model\nWe can use the lm function to fit a simple linear regression model, specifying the model as y ~ x where y represents the outcome variable, and x represents the explanatory variable. Using mod08_lung_function.rds, we can quantify the relationship between FVC and height:\n\nlm(FVC ~ Height, data=lung)\n\n\nCall:\nlm(formula = FVC ~ Height, data = lung)\n\nCoefficients:\n(Intercept)       Height  \n   -18.8735       0.1408  \n\n\nThe default output from the lm function is rather sparse. We can obtain much more useful information by defining the linear regression model as an object, then using the summary() function:\n\nmodel &lt;- lm(FVC ~ Height, data=lung)\nsummary(model)\n\n\nCall:\nlm(formula = FVC ~ Height, data = lung)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.01139 -0.23643 -0.02082  0.24918  1.31786 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -18.87347    2.19365  -8.604 3.89e-14 ***\nHeight        0.14076    0.01331  10.577  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3965 on 118 degrees of freedom\nMultiple R-squared:  0.4867,    Adjusted R-squared:  0.4823 \nF-statistic: 111.9 on 1 and 118 DF,  p-value: &lt; 2.2e-16\n\n\nFinally, we can obtain 95% confidence intervals for the regression coefficients using the confint function:\n\nconfint(model)\n\n                  2.5 %      97.5 %\n(Intercept) -23.2174967 -14.5294444\nHeight        0.1144042   0.1671092"
  },
  {
    "objectID": "08-correlation-and-regression.html#plotting-residuals-from-a-simple-linear-regression-1",
    "href": "08-correlation-and-regression.html#plotting-residuals-from-a-simple-linear-regression-1",
    "title": "1  Correlation and linear regression",
    "section": "1.15 Plotting residuals from a simple linear regression",
    "text": "1.15 Plotting residuals from a simple linear regression\nWe can use the resid function to obtain the residuals from a saved model. These residuals can then be plotted using a histogram in the usual way:\n\nresiduals &lt;- resid(model)\nhist(residuals)\n\n\n\ngf_histogram(~ residuals, bins = 10) +\n  gf_theme(theme_minimal())\n\n\n\n\nA Normal curve can be overlaid if we plot the residuals using a probability scale.\n\nhist(residuals, probability = TRUE,\n     ylim = c(0, 1))\n\ncurve(dnorm(x, mean=mean(residuals), sd=sd(residuals)), \n      col=\"darkblue\", lwd=2, add=TRUE)\n\n\n\ngf_dhistogram(~ residuals, bins = 10)  |&gt; \n  gf_fitdistr(dist = \"dnorm\")\n\n\n\ngf_dens(~ residuals)  |&gt; \n  gf_fitdistr(dist = \"dnorm\")\n\n\n\ngf_dhistogram(~ residuals, bins = 10, alpha=0.3)  |&gt; \n  gf_dens(~ residuals, colour= ~\"Distribution of residuals\")  |&gt; \n  gf_fitdistr(dist = \"dnorm\", colour = ~\"Normal distribution\") |&gt; \n  gf_labs(color = \"\") |&gt; \n  gf_theme(theme_light())"
  },
  {
    "objectID": "98.1-appendix1.html",
    "href": "98.1-appendix1.html",
    "title": "Appendix",
    "section": "",
    "text": "Analysis flowchart"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Acock, Alan C. 2010. A Gentle Introduction to\nStata. 3rd ed. College Station, Tex:\nStata Press.\n\n\nBland, Martin. 2015. An Introduction to Medical\nStatistics. 4th ed. Oxford, New York:\nOxford University Press.\n\n\nKirkwood, Betty, and Jonathan Sterne. 2001. Essentials of\nMedical Statistics. 2nd ed. Malden, Mass:\nWiley-Blackwell."
  }
]