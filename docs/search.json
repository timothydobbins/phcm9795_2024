[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PHCM9795: Foundations of Biostatistics",
    "section": "",
    "text": "Course introduction\nWelcome to PHCM9795 Foundations of Biostatistics.\nThis introductory course in biostatistics aims to provide students with core biostatistical skills to analyse and present quantitative data from different study types. These are essential skills required in your degree and throughout your career.\nWe hope you enjoy the course and will value your feedback and comment throughout the course."
  },
  {
    "objectID": "index.html#course-information",
    "href": "index.html#course-information",
    "title": "PHCM9795: Foundations of Biostatistics",
    "section": "Course information",
    "text": "Course information\nBiostatistics is a foundational discipline needed for the analysis and interpretation of quantitative information and its application to population health policy and practice.\nThis course is central to becoming a population health practitioner as the concepts and techniques developed in the course are fundamental to your studies and practice in population health. In this course you will develop an understanding of, and skills in, the core concepts of biostatistics that are necessary for analysis and interpretation of population health data and health literature.\nIn designing this course, we provide a learning sequence that will allow you to obtain the required graduate capabilities identified for your program. This course is taught with an emphasis on formulating a hypothesis and quantifying the evidence in relation to a specific research question. You will have the opportunity to analyse data from different study types commonly seen in population health research.\nThe course will allow those of you who have covered some of this material in your undergraduate and other professional education to consolidate your knowledge and skills. Students exposed to biostatistics for the first time may find the course challenging at times. Based on student feedback, the key to success in this course is to devote time to it every week. We recommend that you spend an average of 10-15 hours per week on the course, including the time spent reading the course notes and readings, listening to lectures, and working through learning activities and completing your assessments. Please use the resources provided to assist you, including online support."
  },
  {
    "objectID": "index.html#units-of-credit",
    "href": "index.html#units-of-credit",
    "title": "PHCM9795: Foundations of Biostatistics",
    "section": "Units of credit",
    "text": "Units of credit\nThis course is a core course of the Master of Public Health, Master of Global Health and Master of Infectious Diseases Intelligence programs and associated dual degrees, comprising 6 units of credit towards the total required for completion of the study program. A value of 6 UOC requires a minimum of 150 hours work for the average student across the term."
  },
  {
    "objectID": "index.html#course-aim",
    "href": "index.html#course-aim",
    "title": "PHCM9795: Foundations of Biostatistics",
    "section": "Course aim",
    "text": "Course aim\nThis course aims to provide students with the core biostatistical skills to apply appropriate statistical techniques to analyse and present population health data."
  },
  {
    "objectID": "index.html#learning-outcomes",
    "href": "index.html#learning-outcomes",
    "title": "PHCM9795: Foundations of Biostatistics",
    "section": "Learning outcomes",
    "text": "Learning outcomes\nOn successful completion of this course, you will be able to:\n\nSummarise and visualise data using statistical software.\nDemonstrate an understanding of statistical inference by interpreting p-values and confidence intervals.\nApply appropriate statistical tests for different types of variables given a research question, and interpret computer output of these tests appropriately.\nDetermine the appropriate sample size when planning a research study.\nPresent and interpret statistical findings appropriate for a population health audience.\n\n\nChangelog\n2023-07-17\n[Changed]\n\nSection 7.9: Corrected screenshots to test difference in paired proportions in Stata.\n\n2023-07-13\n[Changed]\n\nSection 7.13: tidied up the R function used to calculate the 95% confidence interval for the difference in paired proportions.\n\n2023-07-12\n[Changed]\n\nWorked Example 6.2: removed “This z-statistic does not meet or exceed the critical value of 1.96 for a two tailed test” and re-framed this in terms of interpreting the P-value as calculated from software.\nWorked Example 7.1: corrected column headings for Nausea and No nausea\n\n2023-07-01\n[Changed]\n\nSection 4.2: Fixed typo: “The particular test statistic differs depending on the type of data being analyses analysed”\nSection 4.4: Fixed typo: “This is the situation described in scenario (c) of Figure Figure 4.1.”\nActivity 5.2: Added “or R” to the instruction “Use Stata to conduct an appropriate statistical test”\nActivity 5.3: Added “or R” to the instruction “Use Stata to conduct an appropriate statistical test”\nSection 6.3: Fixed formula for testing one sample proportion to: “\\(z = \\frac{(p_{sample} - p_{population})}{\\text{SE}(p_{population})}\\)”\nActivity 7.2: Added “or R” to the instruction “Using Stata, carry out the appropriate significance test”\n\n2023-07-01\n[Changed]\n\nRenamed “Readings” to “Optional readings”\nModule 4, Section 4.8: Corrected the sentence that describes Figure 4.4. “the shaded region for a one-tailed test would be doubled retained on one side of the distribution and eliminated from the other side of the distribution”.\nModule 9: Added titles to Tables 9.3 and 9.4.\nModule 9, Section 9.4: Corrected the level of evidence: “providing strong evidence of a difference in the median length of stay between the groups.”\nModule 9, Section 9.5.1. Corrected the text under Table 9.4: “The data shows … 10 people who have a negative positive difference.”\n\n2023-06-13\n[Changed]\n\nModule 3. Clarified Worked Example 3.1, and moved the example from Section 3.5.1 to Section 3.5.2.\nSection 3.6: Added Stata output for calculating a 95% confidence interval from individual data.\n\n2023-06-01\n[Changed]\n\nSection 1.14.3: RStudio preferences are now located at Edit &gt; Settings on MacOS, and Tools &gt; Global Options on Windows\nSection 1.14.7.2: Correct layout for commands to install packages (commands must be entered on separate lines)\nSection 1.15.2: Correct the name of the pbc data set to mod_01_pdc.rds\nActivity 2.3(b): Change the request to plot data on a Normal curve, not a standardised Normal curve\nActivity 5.3 and 5.4: added underscores to filenames"
  },
  {
    "objectID": "_revision-progress.html",
    "href": "_revision-progress.html",
    "title": "PHCM9795: Foundations of Biostatistics",
    "section": "",
    "text": "Module\nRevise main notes\nRevise Stata notes\nRevise R notes\nRevise activities\nRevise solutions\nReview\n\n\n\n\n1\n\n\n\n\n\n\n\n\n2\n\n\n\n\n\n\n\n\n3\nY\nY\nY\n\n\n\n\n\n4\nY\nY\nY\n\n\n\n\n\n5\nY\nY\nY\n\n\n\n\n\n6\nY\nY\nY\n\n\n\n\n\n7\nY\nY\nY\n\n\n\n\n\n8\nY\nY\nY\n\n\n\n\n\n9\nY\nY\nY\n\n\n\n\n\n10\nY\nY\nY"
  },
  {
    "objectID": "02-probability.html",
    "href": "02-probability.html",
    "title": "2  Probability and probability distributions",
    "section": "",
    "text": "Stata notes"
  },
  {
    "objectID": "02-probability.html#learning-objectives",
    "href": "02-probability.html#learning-objectives",
    "title": "2  Probability and probability distributions",
    "section": "Learning objectives",
    "text": "Learning objectives\nBy the end of this module you will be able to:\n\nDescribe the concept of probability;\nDescribe the characteristics of a binomial distribution and a Normal distribution;\nCompute binomial probabilities using Stata;\nCompute and use Z-scores to obtain probabilities;\nDecide when to use parametric or non-parametric statistical methods;\nBriefly outline other types of distributions."
  },
  {
    "objectID": "02-probability.html#optional-readings",
    "href": "02-probability.html#optional-readings",
    "title": "2  Probability and probability distributions",
    "section": "Optional readings",
    "text": "Optional readings\nKirkwood and Sterne (2001); Chapters 5, 14 and 15. [UNSW Library Link]\nBland (2015); Chapters 6 and 7. [UNSW Library Link]"
  },
  {
    "objectID": "02-probability.html#introduction",
    "href": "02-probability.html#introduction",
    "title": "2  Probability and probability distributions",
    "section": "2.1 Introduction",
    "text": "2.1 Introduction\nIn Module 1, we looked at how to summarise data numerically and graphically. In this module, we will introduce the concept of probability which underpins the theoretical basis of statistics, and then introduce the concept of probability distributions. We will look at the binomial distribution, and then look at the most important distribution in statistics: the Normal distribution. Finally, we introduce some other probability distributions commonly used in biostatistics."
  },
  {
    "objectID": "02-probability.html#probability",
    "href": "02-probability.html#probability",
    "title": "2  Probability and probability distributions",
    "section": "2.2 Probability",
    "text": "2.2 Probability\nProbability is defined as:\n\nthe chance of an event occurring, where an event is the result of an observation or experiment, or the description of some potential outcome.\n\nProbabilities range from 0 (where the event will never occur) to 1 (where the event will always occur). For example, tossing a coin is an experiment; one event is the coin landing with head up, while the other event is the coin landing tails up. The set of all possible outcomes in an experiment is called the sample space. For example, by tossing a coin you can get either a head or a tail (called mutually exclusive events); and by rolling a die you can get any of the six sides. Thus, for a die the sampling space is: S = {1, 2, 3, 4, 5, 6}\nWith a fair (unbiased) die, the probability of each outcome occurring is 1/6 and its probability distribution is simply a probability of 1/6 for each of the six numbers on a die.\n\n2.2.1 Additive law of probability\nHow do we work out the probability that one roll of a die will turn out to be a 3 or a 6? To do that, we first need to work out whether the events (3 or 6 on the roll of a die) are mutually exclusive. Events are mutually exclusive if they are events which cannot occur at the same time. For example, rolling a die once and getting a 3 and 6 are mutually exclusive events (you can roll one or the other but not both in a single roll).\nTo obtain the probability of one or the other of two mutually exclusive events occurring, the sum of the probabilities of each is taken. For example, the probability of the roll of a die being a 3 or a 6 is the sum of the probability of the die being 3 (i.e. 1/6) and the probability of the die being 6 (also 1/6). With a fair die:\nProbability of a die roll being 3 or 6 = \\(1/6 + 1/6 = 1/3\\)\nAnother way of putting it is:\nP(die roll =3 or die roll =6) = P(die roll=3) + P(die roll=6) = \\(1/6 + 1/6 = 1/3\\)\n\nExample: Additive law for mutually exclusive events\nConsider that blood type can be organised into the ABO system (blood types A, B, AB or O) An individual may only have one blood type.\nUsing the information from https://www.donateblood.com.au/learn/about-blood let’s consider the ABO blood type system. The frequency distribution (prevalence) of the ABO blood type system in the population represents the probability of each of the outcomes. If we consider all possible blood type outcomes, then the total of the probabilities will sum to 1 (100%).\n\n\n\n\nTable 2.1:  Frequency of blood types \n\nBlood Type% of populationProbability   \n\nA38%0.38\n\nB10%0.1 \n\nAB3%0.03\n\nO49%0.49\n\nTotal100%1   \n\n\n\n\n\nIn this example we consider: What is the probability that an individual will have either blood group O or A?\nSince blood type is mutually exclusive, the probability that either one or the other occurs is the sum of the individual probabilities. These are mutually exclusive events so we can say P(O or A) = P(O) + P(A)\nThus, the answer is: P(Blood type O) + P(Blood type A) = 0.49 + 0.38 = 0.87\n\n\n\n2.2.2 Multiplicative law of probability\nThe additive law of probability lets us consider the probability of different outcomes in a single experiment. The multiplicative law lets us consider the probability of multiple events occurring in a particular order. For example: if I roll a die twice, what is the probability of rolling a 3 and then a 6?\nThese events are independent: the probability of rolling a 6 on the second roll is not affected by the first roll.\nThe multiplicative law of probability states:\n\nIf A and B are independent, then P(A and B) = P(A) \\(\\times\\) P(B).\n\nSo, the probability of rolling a 3 and then a 6 is: P(3 and 6) = \\(1/6 \\times 1/6 = 1/36\\).\nNote here that the order matters – we are considering the probability of rolling a 3 and then a 6, not the probability of rolling a 6 and then a 3."
  },
  {
    "objectID": "02-probability.html#probability-distributions",
    "href": "02-probability.html#probability-distributions",
    "title": "2  Probability and probability distributions",
    "section": "2.3 Probability distributions",
    "text": "2.3 Probability distributions\n\nA probability distribution is a table or a function that provides the probabilities of all possible outcomes for a random event.\n\nFor example, the probability distribution for a single coin toss is straightforward: the probability of obtaining a head is 0.5, and the probability of obtaining a tail is 0.5, and this can be summarised in Table 2.2.\n\n\n\n\nTable 2.2:  Probability distribution for a single coin toss \n\nCoin faceProbability\n\nHeads0.5\n\nTails0.5\n\n\n\n\n\nSimilarly, the probability distribution for a single roll of a die is straightforward: each face has a probability of 1/6 (Table 2.3).\n\n\n\n\nTable 2.3:  Probability distributions for a single roll of a die \n\nFace of a dieProbability\n\n11/6\n\n21/6\n\n31/6\n\n41/6\n\n51/6\n\n61/6\n\n\n\n\n\nThings become more complicated when we consider multiple coin-tosses, or rolls of a die. These series of events can be summarised by considering the number of times a certain outcome is observed. For example, the probability of obtaining three heads from five coin tosses.\nProbability distributions can be used in two main ways:\n\nTo calculate the probability of an event occurring. This seems trivial for the coin-toss and die-roll examples above. However, we can consider more complex events, as below.\nTo understand the behaviour of a sample statistic. We will see in Modules 3 and 4 that we can assume the mean of a sample follows a probability distribution. We can obtain useful information about the sample mean by using properties of the probability distribution."
  },
  {
    "objectID": "02-probability.html#discrete-random-variables-and-their-probability-distributions",
    "href": "02-probability.html#discrete-random-variables-and-their-probability-distributions",
    "title": "2  Probability and probability distributions",
    "section": "2.4 Discrete random variables and their probability distributions",
    "text": "2.4 Discrete random variables and their probability distributions\nRather than thinking of random events, we often use the term random variable to describe a quantity that can have different values determined by chance.\nA discrete random variable is a random variable that can take on only countable values (that is, non-negative whole numbers). An example of a discrete random variable is the number of heads observed in a series of coin tosses.\nA discrete random variable can be summarised by listing all the possible values that the variable can take. As defined earlier, a table, formula or graph that presents these possible values, and their associated probabilities, is called a probability distribution.\nExample: let’s consider the number of heads in a series of three coin tosses. We might observe 0 heads, or 1 head, or 2, or 3 heads. If we let X denote the number of heads in a series of three coin tosses, then possible values of X are 0, 1, 2 or 3.\nWe write the probability of observing x heads as P(X=x). So P(X=0) is the probability that the three tosses has no heads. Similarly, P(X=1) is the probability of observing one head.\nThe possible combinations for three coin tosses are as follows:\n\n\n\n\nTable 2.4:  The number of heads from three coin tosses \n\nPatternNumber of heads\n\nTail, Tail, Tail0\n\nHead, Tail, Tail\n\nTail, Head, Tail1\n\nTail, Tail, Head\n\nHead, Head, Tail\n\nHead, Tail, Head2\n\nTail, Head, Head\n\nHead, Head, Head3\n\n\n\n\n\nThere are eight possible outcomes from three coin tosses (permutations). If we assume an equal chance of observing a head or a tail, each permutation above is equally likely, and so has a probability of 1/8.\nIf we consider the possibility of observing just one head out of the three tosses, this can happen in three ways (HTT, THT, TTH). So the probability of observing one head is calculated using the additive law: P(X=1) = \\(\\tfrac{1}{8} + \\tfrac{1}{8} + \\tfrac{1}{8} = \\tfrac{3}{8}\\).\nTherefore, the probability distribution for X, the number of heads from three coin tosses, is as follows:\n\n\n\n\nTable 2.5:  Probability distribution for the number of heads from three coin\ntosses \n\nx (number of heads observed)P(X=x)\n\n01/8\n\n11/8 + 1/8 + 1/8 = 3/8\n\n21/8 + 1/8 + 1/8 = 3/8\n\n31/8\n\n\n\n\n\nNote that the probabilities sum to 1.\nThe above example was based on a coin toss, where flipping a head or a tail is equally likely (both have probabilities of 0.5). Let’s consider a case where the probability of an event is not equal to 0.5: having blood type A.\nFrom Table 2.1, the probability that a person has Type A blood is 0.38, and therefore, the probability that a person does not have Type A blood is 0.62 (1–0.38). If we considered taking a random sample of three people, the probability that all three would have Type A blood is 0.38 × 0.38 × 0.38 (using the multiplicative rule above) – and there is only one way this could happen.\nThe number of ways two people out of three could have Type A blood is 3, and each permutation is listed in Table 2.6. The probability of observing each of the three patterns is the same, and can be calculated using the multiplicative rule: 0.38 × 0.38 × 0.62 = 0.0895.\n\n\n\n\nTable 2.6:  Combinations and probabilities of Type A blood in three people \n\nPerson 1Person 2Person 3Probability\n\nAAA0.38 × 0.38 × 0.38 = 0.0549\n\nAANot A0.38 × 0.38 × 0.62 = 0.0895\n\nANot AA0.38 × 0.62 × 0.38 = 0.0895\n\nNot AAA0.62 × 0.38 × 0.38 = 0.0895\n\nANot ANot A0.38 × 0.62 × 0.62 = 0.1461\n\nNot AANot A0.62 × 0.38 × 0.62 = 0.1461\n\nNot ANot AA0.62 × 0.62 × 0.38 = 0.1461\n\nNot ANot ANot A0.62 × 0.62 × 0.62 = 0.2383\n\n\n\n\n\nTable 2.7 gives the probability of each of the blood type combinations we could observe in three people. The probability of observing a certain number of people (say, k) with Type A blood from a sample of three people can be calculated by summing the combinations:\n\n\n\n\nTable 2.7:  Probabilities of observing numbers of people with Type A blood in a\nsample of three people \n\nNumber of people with Type A bloodProbability of each pattern     \n\n30.0549\n\n20.0895 + 0.0895 + 0.0895 = 0.2689\n\n10.1461 + 0.1461 + 0.1461 = 0.4382\n\n00.2383"
  },
  {
    "objectID": "02-probability.html#binomial-distribution",
    "href": "02-probability.html#binomial-distribution",
    "title": "2  Probability and probability distributions",
    "section": "2.5 Binomial distribution",
    "text": "2.5 Binomial distribution\nThe above are examples of the binomial distribution. The binomial distribution is used when we have a collection of random events, where each random event is binary (e.g. Heads vs Tails, Type A blood vs Not Type A blood, Infected vs Not infected). The binomial distribution calculates (in general terms):\n\nthe probability of observing k successes\nfrom a collection of n trials\nwhere the probability of a success in one trial is p.\n\nThe terms used here can be defined as: - a success is simply an event of interest from a binary random event. In the coin-toss example, “success” was tossing a Head. In the blood type example, we were only interested in whether someone was Type A or not Type A, so “success” was a blood of Type A. We tend to use the word “success” to mean “an event of interest”, and “failure” as “an event not of interest”. - the number of trials refers to the number of random events observed. In both examples, we observed three events (three coin tosses, three people). - the probability of a success (p) simply refers to the probability of the event of interest. In the coin toss example, this was the probability of tossing a Heads (=0.5); for the blood-type example, this was the probability of having Type A blood (0.38).\nPutting all this together, we say that we have a binomial experiment. To satisfy the assumptions of a binomial distribution, our experiment must satisfy the following criteria:\n\nThe experiment consists of fixed number (n) of trials.\nThe result of each trial falls into only one of two categories – the event occurred (“success”) or the event did not occur (“failure”).\nThe probability, p, of the event occurring remains constant for each trial.\nEach trial of the experiment is independent of the other trials.\n\nWe have shown in the examples above how we can calculate the probabilities for small experiments (n=3). Once n becomes large, constructing such probability distribution tables becomes difficult. The general formula for calculating the probability of observing k successes from n trials, where each trial has a probability of success of p is given by:\n\\[ P(X=k) = \\frac{n!}{k! (n-k)!} \\times p^k \\times (1-p)^{n-k} \\]\nwhere \\(n! = n \\times (n-1) \\times (n-2) \\times \\dots \\times 2 \\times 1\\).\nNote that this formula is almost never calculated by hand. Instructions for calculating binomial probabilities are given in the Stata and R notes at the end of this Module.\n\n2.5.1 Mean and variance of a binomial variable\nThe properties of the binomial distribution are useful in the statistical modelling of prevalence data. If X has a binomial distribution, then the mean of X is:\n\\[ E(X) = n \\times p\\] and the variance is:\n\\[ var(X) = n \\times p \\times (1-p) \\] where n = the number of trials, and p = the probability of the event occurring (or success).\n\nWorked example\nA population-based survey conducted by the AIHW (2008) of a random sample of the Australian population estimated that in 2007, 19.8% of the Australian population were current smokers.\n\nFrom a random sample of 6 people from the Australian population in 2007, what is the probability that 3 of them will be smokers?\nWhat is the probability that among the six persons, at least 4 will be smokers?\nWhat is the probability that at most, 2 will be smokers?\n\n\n\nStata Solution\n\nThe computation for binomial probabilities can be done using Stata from the main menu Data &gt; Other utilities &gt; Hand calculator as shown in the Stata Notes section.\n\nOf the three binomial functions in the Hand Calculator, we choose the binomialp function, which gives “the probability of observing k successes in n trials when the probability of a success on one trial is p”.\nWe complete the function using n=6, k=3, and p=0.198. This gives an answer of 0.08. [Command: display binomialp(6, 4, 0.198)]\n\nIn common language, getting “at least 4” smokers means getting 4, 5 or 6 smokers. Since these are mutually exclusive events, we can apply the additive law to find the probability of getting at least 4 smokers:\n\n\\[ P(X \\ge 4) = P(X=4) + P(X=5) + P(X=6) \\] Using the same binomial probability function as in the previous question,\n\nP(X=4) = 0.015 [Command: display binomialp(6, 4, 0.198)]\nP(X=5) = 0.001 [Command: display binomialp(6, 5, 0.198)]\nP(X=6) = 0.00006 [Command: display binomialp(6, 6, 0.198)]\n\nAnswer: \\(P(X \\ge 4) = 0.00006025 + 0.00146437 + 0.0148282 = 0.016\\)\nAlternatively, we can use the binomialtail function (which gives “the probability of observing k or more successes in n trials when the probability of a success on one trial is p”).\nHere, n=6, k=4 and p=0.198, giving the same answer as above: 0.016. [Command: display binomialtail(6, 4, 0.198)]\n\nObserving at most two means observing 0, 1 or 2 smokers. Therefore, the probability of observing at most 2 smokers is:\n\n\nP(X \\(\\le\\) 2) = P(X=0) + P(X=1) + P(X=2)\nP(X=0) = 0.266 [Command: display binomialp(6,0,.198)]\nP(X=1) = 0.394 [Command: display binomialp(6,1,.198)]\nP(X=2) = 0.243 [Command: display binomialp(6,2,.198)]\n\nAnswer: P(X \\(\\le\\) 2) = 0.266+0.394+0.243=0.903\nThis can also be done by using the binomial function (which gives “the probability of observing k or fewer successes in n trials when the probability of a success on one trial is p”).\nHere, n=6, k=2 and p=0.198, giving the same answer as above: 0.904 (note the discrepancy due to rounding error above). [Command: display binomial(6, 2, 0.198)]\n\n\nR Solution\nThe computation for binomial probabilities can be done in R using the dbinom and pbinom functions.\n\nOf the two binomial functions, we choose the dbinom(x, size, prob) function, which gives the probability of observing x successes in size trials when the probability of a success on one trial is prob.\n\nWe complete the function using x=6, size=3, and prob=0.198. This gives an answer of 0.08:\n\ndbinom(x=3, size=6, prob=0.198)\n\n[1] 0.08008454\n\n\n\nIn common language, getting “at least 4” smokers means getting 4, 5 or 6 smokers. Since these are mutually exclusive events, we can apply the additive law to find the probability of getting at least 4 smokers:\n\n\\[ P(X \\ge 4) = P(X=4) + P(X=5) + P(X=6) \\] Using the same binomial probability function as in the previous question,\n\nP(X=4) = 0.015 [Command: dbinom(x=4, size=6, prob=0.198)]\nP(X=5) = 0.001 [Command: dbinom(x=5, size=6, prob=0.198)]\nP(X=6) = 0.00006 [Command: dbinom(x=6, size=6, prob=0.198)]\n\nAnswer: \\(P(X \\ge 4) = 0.0148282 + 0.00146437 + 0.00006025 = 0.016\\)\nAlternatively, we can use the pbinom(q, size, prob, lower.tail=FALSE) function (which gives the probability of observing more than q successes in size trials when the probability of a success on one trial is prob).\nHere, q=3, size=6 and prob=0.198, giving the same answer as above: 0.016:\n\npbinom(q=3, size=6, prob=0.198, lower.tail=FALSE)\n\n[1] 0.01635325\n\n\n\nObserving at most two means observing 0, 1 or 2 smokers. Therefore, the probability of observing at most 2 smokers is:\n\n\nP(X \\(\\le\\) 2) = P(X=0) + P(X=1) + P(X=2)\nP(X=0) = 0.266 [Command: dbinom(x=0, size=6, prob=0.198)]\nP(X=1) = 0.394 [Command: dbinom(x=1, size=6, prob=0.198)]\nP(X=2) = 0.243 [Command: dbinom(x=2, size=6, prob=0.198)]\n\nAnswer: \\(P(X \\le 2) = 0.266 + 0.394 + 0.243 = 0.903\\)\nThis can also be done by using the pbinom(q, size, prob) function (which gives the probability of observing q or fewer successes in size trials when the probability of a success on one trial is prob).\nHere, q=2, size=6 and p=0.198, giving the same answer as above: 0.904 (note the discrepancy due to rounding error above).\n\npbinom(q=2, size=6, prob=0.198)\n\n[1] 0.9035622"
  },
  {
    "objectID": "02-probability.html#normal-distribution",
    "href": "02-probability.html#normal-distribution",
    "title": "2  Probability and probability distributions",
    "section": "2.6 Normal distribution",
    "text": "2.6 Normal distribution\nThe frequency plot for many biological and clinical measurements (for example blood pressure and height) follow a bell shape where the curve is symmetrical about the mean value and has tails at either end. Figure 2.1 1 and Figure 2.2 2 demonstrate this type of distribution.\n\n\n\n\n\nFigure 2.1: Distribution of diastolic blood pressure, 2017–18 Australian Bureau of Statistics National Health Survey\n\n\n\n\n\n\n\n\n\nFigure 2.2: Distribution of male and female heights\n\n\n\n\nThe Normal distribution, also called the Gaussian distribution (named after Johann Carl Friedrich Gauss, 1777–1855), has been shown to fit the frequency distribution of many naturally occurring variables. It is characterised by its bell-shaped, symmetric curve and its tails that approach zero on either side.\nThere are two reasons for the importance of the Normal distribution in biostatistics (Kirkwood and Sterne, 2003). The first is that many variables can be modelled reasonably well using the Normal distribution. Even if the observed data were not Normally distributed, it can often be made reasonably Normal after applying some transformation of the data. The second (and possible most important) reason, is based on the central limit theorem and will be discussed in Module 3.\nThe Normal distribution is characterised by two parameters: the mean (\\(\\mu\\)) and the standard deviation (\\(\\sigma\\)). The mean defines where the middle of the Normal distribution is located, and the standard deviation defines how wide the tails of the distribution are.\nFor a Normal distribution, about 68% of the observations lie between \\(- \\sigma\\) and \\(\\sigma\\) of the mean; 95% of the observations lie between \\(−1.96 \\times \\sigma\\) and \\(1.96 \\times \\sigma\\) from the mean; and almost all the observations (99.7%) lie between \\(-3 \\times \\sigma\\) and \\(3 \\times \\sigma\\) (Figure 2.3). Also note that the mean is the same as the median, as the curve is symmetric about its mean.\n\n\n\n\n\nFigure 2.3: Characteristics of a Normal distribution"
  },
  {
    "objectID": "02-probability.html#the-standard-normal-distribution",
    "href": "02-probability.html#the-standard-normal-distribution",
    "title": "2  Probability and probability distributions",
    "section": "2.7 The Standard Normal distribution",
    "text": "2.7 The Standard Normal distribution\nAs each Normal distribution is defined by its mean and standard deviation, there are an infinite number of possible Normal distributions. However, every Normal distribution can be transformed to what we call the Standard Normal distribution, which has a mean of zero (\\(\\mu = 0\\)) and a standard deviation of one (\\(\\sigma = 1\\)). The Standard Normal distribution is so important that it has been assigned its own symbol: Z.\nEvery observation from a Normal distribution \\(X\\) with a mean \\(\\mu\\) and a standard deviation \\(\\sigma\\) can be transformed to a z-score (also called a Standard Normal deviate) by the formula:\n\\[ z = \\frac{x - \\mu}{\\sigma} \\]\nThe z-score is simply how far an observation lies from the population mean value, scaled by the population standard deviation.\nWe can use z-scores to estimate probabilities, as shown in Worked Example 2.2.\n\nWorked Example\nThis example extends the example of diastolic blood pressure shown in Figure 2.1. Assume that the mean diastolic blood pressure for men is 77.9 mmHg, with a standard deviation of 11. What is the probability that a man selected at random will have high blood pressure (i.e. diastolic blood pressure ≥ 90)?\nTo estimate the probability that diastolic blood pressure ≥ 90 (i.e. the upper tail probability), we first need to calculate the z-score that corresponds to 90 mmHg.\nUsing the z-score formula, with x=90, \\(\\mu\\)=77.9 and \\(\\sigma\\)=11:\n\\[ z = \\frac{90 - 77.9}{11} = 1.1 \\] Thus, a blood pressure of 90 mmHg corresponds to a z-score of 1.1, or a value 1.1 \\(\\times \\sigma\\) above the mean weight of the population.\nUsing a table of Z-scores (Appendix Table 1), we find the probability that a person has a diastolic blood pressure of 90 mmHg or more as P(Z ≥ 1.1) = 0.136.\nAn extract from Appendix Table 1 is shown in Figure 2.4 to demonstrate how to find the probability from the look-up table.\n\n\n\n\n\nFigure 2.4: Obtaining a probability from a table of Standard Normal distribution\n\n\n\n\nFigure 2.5 shows the probability of a diastolic blood pressure of 90 mmHg or more in the population for a Z-score of greater than 1.1 on a Standard Normal distribution.\n\n\n\n\n\nFigure 2.5: Area under the Standard Normal curve (as probability) for Z &gt; 1.1\n\n\n\n\nApart from calculating probabilities, Z-scores are most useful for comparing measurements taken from a sample to a known population distribution. It allows measurements to be compared to one another despite being on different scales or having different predicted values.\nFor example, if we take a sample of children and measure their weights, it is useful to describe those weights as Z-scores from the population weight distribution for each age and gender. Such distributions from large population samples are widely available. This allows us to describe a child’s weight in terms of how much it is above or below the population average. For example, if mean weights were compared, children aged 5 years would be on average heavier than the children aged 3 years simply because they are older and therefore larger. To make a valid comparison, we could use the Z-scores to say that children aged 3 years tend to be more overweight than children aged 5 years because they have a higher mean Z-score for weight."
  },
  {
    "objectID": "02-probability.html#assessing-normality",
    "href": "02-probability.html#assessing-normality",
    "title": "2  Probability and probability distributions",
    "section": "2.8 Assessing Normality",
    "text": "2.8 Assessing Normality\nThere are several ways to assess whether a continuous variable is Normally distributed. The simple process of plotting a histogram and boxplot and comparing estimates of the centre of the data (mean and median) provide valuable information about the way in which the data are distributed.\nOther more formal measures of Normality such as skewness (whether the distribution is symmetrical or asymmetrical) and kurtosis (whether the distribution is flat or peaked) can be obtained from Stata (see Stata Notes section on Producing summary statistics in Module 1).\n\n2.8.1 Skewness and kurtosis\nSkewness is a measure of the lack of symmetry of a distribution. If the distribution is symmetric, the coefficient of skewness is 0. If the coefficient is negative, the median is usually greater than the mean and the distribution is said to be skewed left. If the coefficient is positive, the median is usually less than the mean and the distribution is said to be skewed right.\nKurtosis (from the Greek kyrtosis, meaning curvature) is a measure of peakiness of a distribution. The smaller the coefficient of kurtosis, the flatter the distribution. The Normal distribution has a coefficient of kurtosis of 3 (0 in R) and provides a convenient benchmark. If the distribution is more spread out, then the kurtosis will be greater than 3 (0 in R).\n\n\n\n\nTable 2.8:  Interpretation of skewness statistics \n\nSkewnessInterpretation\n\n0The distribution is symmetric\n\nIf skewness is between -0.5 and 0.5, the distribution is approximately symmetric\n\nNegativeDistribution is skewed to the left (mean is usually less than the median) – aka negative skew (longer tail to the left)\n\nIf it is less than −1, the distribution is highly skewed to the left\n\nIf it is between −1 and −0.5, the distribution is moderately skewed to the left\n\nPositiveDistribution is skewed to the right (mean is usually greater than the median) – aka positive skew (longer tail to the right)\n\nIf it is greater than 1, the distribution is highly skewed to the right\n\nIf it is between 1 and 0.5, the distribution is moderately skewed to the right\n\n\n\n\n\nFor your information: There are formal tests in Stata and R that test for Normality. These tests are beyond the scope of this course and will be discussed in the Advanced course.\nThe histogram for our 30 weights is approximately bell-shaped and roughly symmetrical. The mean and median (50th percentile) values are identical, as would be expected for a Normal distribution. These statistics indicate that the data are Normally distributed. Finally, you can look at the histogram and boxplot (Figure 2.6) to assess symmetry and outliers at either end of the distribution.\n\n\n\n\n\nFigure 2.6: Histogram and boxplot of weight of 30 students attending a gym\n\n\n\n\nA summary of how to explore Normality is shown in Table 2.9.\n\n\n\n\nTable 2.9:  Methods to assess Normality \n\nMethodIndication of Normality\n\nExamine histogramApproximately bell shaped and symmetrical; may be difficult to determine if the sample size is small\n\nCompare mean and median valuesValues are approximately equal\n\nExamine box plotBox plot symmetrical with no outliers\n\n\n\n\n\nIt is important to look at all these measures together, and not rely on a single measure when assessing whether a sample is approximately Normally distributed. For small samples, it can be very difficult to determine whether the data are approximately Normally distributed."
  },
  {
    "objectID": "02-probability.html#non-normally-distributed-measurements",
    "href": "02-probability.html#non-normally-distributed-measurements",
    "title": "2  Probability and probability distributions",
    "section": "2.9 Non-Normally distributed measurements",
    "text": "2.9 Non-Normally distributed measurements\nIn the above example, diastolic blood pressure was Normally distributed with an approximately bell-shaped frequency histogram. However, not all measurements are Normally distributed, and the symmetry of the bell shape may be distorted by the presence of some very small or very large values. Non-Normal distributions such as this are called skewed distributions.\nWhen there are some very large values, the distribution is said to be positively skewed.This often occurs when measuring variables related to time, such as days of hospital stay, where most patients have short stays (say 1 - 5 days) but a few patients with serious medical conditions have very long lengths of hospital stay (say 20 - 100 days).\nIn practice, most parametric summary statistics are quite robust to minor deviations from Normality and non-parametric statistical methods are only required when the sample size is small and/or the data are obviously skewed with some influential outliers.\nWhen the data are markedly skewed, histograms and boxplots can look very different. For example, data of length of hospital stay in a sample of children are shown as a histogram and as a box plot in Figure 2.7.\n\n\n\n\n\nFigure 2.7: Histogram and boxplot of length of stay\n\n\n\n\nIn the histogram of Figure 2.7, there is a tail of values to the right, so we would conclude that the distribution is skewed to the right. In the boxplot, the whiskers appear to be fairly symmetric, but there are some unusual values (denoted by dots) above the box and its whiskers. Stata defines these unusual values as being more than 1.5 times the IQR from the edge of the box.\nThe presence of unusual values may be an indication that the data are not Normally distributed. Both the histogram and the box plot show that the distribution has a marked tail towards high values and that non-parametric statistics should be used to generate summary statistics and analyse the data.\nNote that Stata has defined points as being unusual, or outliers. Outliers can be problematic and the decision to include them or omit them from further analyses can be difficult. After detecting any outliers or extreme values, you should not automatically exclude them from the analysis, particularly if the sample was selected randomly from a population. First, it is important to check the original data collection form or questionnaire to rule out the possibility of a data entry error. If the outlier is not a data entry error, it is then important to decide whether the observation is biologically plausible and, if it is, it should be included in the analysis.\n\n2.9.1 Which measure of central tendency to use\nIt is most appropriate to use the mean when the data exhibit a symmetric or bell-shaped distribution. For skewed distributions (where there are more values on the higher (negative skew) or lower side (positive skew) of the scale) the mean is not a good measure of the centre, as the calculation will be influenced by the extreme values. The median is the preferred statistic for describing central tendency in a skewed distribution.\nIf the data exhibits a Normal distribution, we use the standard deviation as the measure of spread. Otherwise, the interquartile range is preferred."
  },
  {
    "objectID": "02-probability.html#parametric-and-non-parametric-statistical-methods",
    "href": "02-probability.html#parametric-and-non-parametric-statistical-methods",
    "title": "2  Probability and probability distributions",
    "section": "2.10 Parametric and non-parametric statistical methods",
    "text": "2.10 Parametric and non-parametric statistical methods\nMany statistical methods are based on assumptions about the distribution of the variable – these methods are known as parametric statistical methods. Many methods of statistical inferences based on theoretical sampling properties that are derived from a Normal distribution with the characteristics described above. Thus, it is important that measurements approximate to a Normal distribution before these parametric methods are used. The methods are called ‘parametric’ because they are based on the parameters – the mean and standard deviation - that underlie a Normal distribution. Statistics which do not assume a particular distribution are called distribution-free statistics, or ‘non-parametric statistics’.\nIn this course, you will learn about both parametric and non-parametric statistical methods. Parametric summary statistical methods include those based on the mean, standard deviation and range (Module 1), and standard error and 95% confidence interval (Module 3). Parametric statistical tests also include t-tests which will be covered in Modules 4 and 5, and correlation and regression described in Module 8.\nNon-parametric summary statistical methods are often based on ranks, and may use such statistics as the median, mode and inter-quartile range (Module 1). Non-parametric statistical tests that use ranking are described in Module 9."
  },
  {
    "objectID": "02-probability.html#other-types-of-probability-distributions",
    "href": "02-probability.html#other-types-of-probability-distributions",
    "title": "2  Probability and probability distributions",
    "section": "2.11 Other types of probability distributions",
    "text": "2.11 Other types of probability distributions\nIn this module we have considered a Normal probability distribution and how to use it to measure the precision of continuously distributed measurements. Data also follow other types of distributions which are briefly described below. In other modules in this course, we will be looking at a range of methods to analyse health data and will refer back to these different distributions.\nNormal approximation of binomial: When the sample size becomes large, it becomes cumbersome to calculate the exact probability of an event using the binomial distribution. Conveniently, with large sample sizes, the binomial distribution approximates a Normal distribution. The mean and SD of a binomial distribution can be used to calculate the probability of the event as though it was from a Normal distribution.\nPoisson distribution: is another distribution which is often used in health research for modelling count data. The Poisson distribution is followed when a number of events happen in a fixed time interval. This distribution is useful for describing data such as deaths in the population in a time period. For example, the number of deaths from breast cancer in one year in women over 50 years old will be an observation from a Poisson distribution. We can also use this to make comparisons of mortality rates between populations.\nMany other probability distributions can be derived for functions which arise in statistical analyses but the chi-squared, t and F distributions are the three distributions that are most widely used. These have many applications, some of which are described in later modules.\nThe chi-squared distribution is a skewed distribution which allows us to determine the probability of a deviation between a count that we observe and a count that we expect for categorical data. One use of this is in conducting statistical tests for categorical data. See Module 7.\nA t-distribution is used when the population standard deviation is not known. The t-distribution is appropriate for small samples (&lt;30) and its distribution is bell shaped similar to a Normal distribution but slightly flatter. The t-distribution is useful for comparing mean values. See Module 4 and Module 5."
  },
  {
    "objectID": "02-probability.html#importing-data-into-stata",
    "href": "02-probability.html#importing-data-into-stata",
    "title": "2  Probability and probability distributions",
    "section": "2.12 Importing data into Stata",
    "text": "2.12 Importing data into Stata\nWe have described previously how to open data that have been saved as Stata .dta files. It is quite common to have data saved in other file types, such as Microsoft Excel, or plain text files. In this section, we will demonstrate how to import data from other packages into Stata using commands in the File &gt; Import menu.\n\n2.12.1 Importing plain text data into Stata\nA csv file, or a “comma separated variables” file is commonly used to store data. These files have a very simple structure: they are plain text files, where data are separated by commas. csv files have the advantage that, as they are plain text files, they can be opened by a large number of programs (such as Notepad in Windows, TextEdit in MacOS, Microsoft Excel - even Microsoft Word). While they can be opened by Microsoft Excel, they can be opened by many other programs: the csv file can be thought of as the lingua-franca of data.\nIn this demonstration, we will use data on the weight of 1000 people entered in a csv file called mod02_weight_1000.csv available on Moodle. To confirm that the file is readable by any text editor, here are the first ten lines of the file, opened in Notepad on Microsoft Windows, and TextEdit on MacOS.\n\n\n\n\n\nTo import it into Stata, use File &gt; Import &gt; Text data (delimited, .csv, …) to bring up the import delimited text data dialog box. Click the Open button (the folder icon on the right-hand side of the window) and select the csv file you downloaded. The dialog box should look like below:\n\n\n\n\n\nHere, Stata has (correctly) decided that the first row of the dataset contains the variable names. You may need to change this if the Preview window does not look correct. Click OK or Submit button to import the data."
  },
  {
    "objectID": "02-probability.html#checking-your-data-for-errors-in-stata",
    "href": "02-probability.html#checking-your-data-for-errors-in-stata",
    "title": "2  Probability and probability distributions",
    "section": "2.13 Checking your data for errors in Stata",
    "text": "2.13 Checking your data for errors in Stata\nBefore you start describing and analysing your data, it is important to make sure that no errors have been made during the data entry process. Basically, you are looking for values that are outside the range of possible or plausible values for that variable.\nIf an error is found, the best method for correcting the error is to go back to the original data e.g. the hard copy questionnaire, to obtain the original value, entering the correct value into Stata. If the original data is not available or the original data is also incorrect, the erroneous value is often excluded from the dataset.\nFor continuous variables, the easiest methods are to examine a boxplot and histogram. For example, a boxplot and histogram for the weight variable we just imported appear as:\n\n\n\n\n\nThere is a clear outlying point shown in the boxplot. Although not obvious, the same point is shown in the histogram as a bar around 700 with a very short height.\nTo identify the outlying observation in the dataset, we can sort your data in ascending order and check the minimum and maximum values in the Data Editor. You will need to decide if these values are a data entry error or are biologically plausible. If an extreme value or “outlier”, is biologically plausible, it should be included in all analyses.\nTo sort data, you must be in the Data Editor (Edit) window. Select Data &gt; Sort data… and choose weight as shown in the sort dialog box below. Click OK to do an ascending sort (arranging weight from smallest to largest). [Command: sort weight]\n\n\n\n\n\nBy scrolling through the data, you will see that the values range from 53.8kg to 85.8kg with a very high value of 700.2kg. A value as high as 700kg is likely to be a data entry error (e.g. error in entering an extra zero) and is not a plausible weight value. Here, you should check your original data. You might find that the original weight was recorded as 70.2kg. You can change this in Stata by deleting a zero from cell with 700.2 in the Data Editor (Edit) window.\nNote: if an extreme value lies within the range of biological plausibility it should not be removed from analysis.\nOnce you have checked your data for errors, you are ready to start analysing your data."
  },
  {
    "objectID": "02-probability.html#overlaying-a-normal-curve-on-a-histogram",
    "href": "02-probability.html#overlaying-a-normal-curve-on-a-histogram",
    "title": "2  Probability and probability distributions",
    "section": "2.14 Overlaying a Normal curve on a Histogram",
    "text": "2.14 Overlaying a Normal curve on a Histogram\nIt can be useful to produce a histogram with an overlayed Normal curve to assess whether our sample appears approximately Normally distributed. First go to menu Graphics &gt; Histogram. In the Histogram dialog box, select weight into the variable box and choose the Frequency radio button for ease of interpretation.\n\n\n\n\n\nYou can click the Submit button to check how the plot looks like. To superimpose the normal curve, go to the Density plots and tick Add normal-density plot, then click the Submit or OK button. You can change the X-axis label in the X axis tab, e.g. to Weight (kg).\n\n\n\n\n\n[Command: histogram weight, frequency normal xtitle(Weight (kg))]\nYour histogram should look like this:"
  },
  {
    "objectID": "02-probability.html#descriptive-statistics-for-checking-normality",
    "href": "02-probability.html#descriptive-statistics-for-checking-normality",
    "title": "2  Probability and probability distributions",
    "section": "2.15 Descriptive statistics for checking normality",
    "text": "2.15 Descriptive statistics for checking normality\nAll the descriptive statistics including Skewness and Kurtosis discussed in this module can be obtained using the summarize command from Statistics &gt; Summaries, tables, and tests &gt; Summary and descriptive statistics &gt; Summary statistics and specifying detail as an option in Stata (as shown in the Stata Notes for Module 1).\n[Command: summarize weight, detail]\n\n\n\n\n\nYou can similarly repeat this using the dataset mod02_weight_30.dta used for creating these in the course notes for this module."
  },
  {
    "objectID": "02-probability.html#importing-excel-data-into-stata",
    "href": "02-probability.html#importing-excel-data-into-stata",
    "title": "2  Probability and probability distributions",
    "section": "2.16 Importing Excel data into Stata",
    "text": "2.16 Importing Excel data into Stata\nAnother common type of file that data are stored in is a Microsoft Excel file (.xls or .xlsx). In this demonstration, we will import a selection of records from a large health survey, stored in the file mod02_health_survey.xlsx.\nThe health survey data contains 1140 records, comprising:\n\nsex: 1 = respondent identifies as male; 2 = respondent identifies as female\nheight: height in meters\nweight: weight in kilograms\n\nTo import data from Microsoft Excel, we use File &gt; Import &gt; Excel spreadsheet. First click Browse to locate the file to be imported:\n\n\n\n\n\nTake special note of the Preview at the bottom of the dialog box. Our dataset has the variable names listed in the first row of the spreadsheet, so we tick the Import first row as variable names checkbox. We don’t want to Import all data as strings (strings are character, or text variables), so we leave this unchecked. We finalise the process by clicking OK.\nAs always, check the Data Browser to confirm that the data were imported successfully."
  },
  {
    "objectID": "02-probability.html#generating-new-variables",
    "href": "02-probability.html#generating-new-variables",
    "title": "2  Probability and probability distributions",
    "section": "2.17 Generating new variables",
    "text": "2.17 Generating new variables\nOur health survey data contains information on height and weight. We often summarise body size using BMI: body mass index which is calculated as: \\(\\frac{\\text{weight (kg)}}{(\\text{height (m)})^2}\\)\nTo generate a new variable, we use Data &gt; Create or change data &gt; Create new variable.\n\n\n\n\n\n\ntype the name of the new variable, BMI, in the Variable name box.\nenter the formula to calculate the variable in “Specify a value or an expression”. This can be done in one of two ways:\n\nOption 1: click Create to open the Expression Builder dialog box. This box allows you to build the formula interactively:\n\n\n\n\n\nClicking Variables in the lower-left-hand pane shows a list of variables in the lower-middle pane of the window. We want to create a new formula of \\(\\text{weight } \\div \\text{ height} ^2\\). Double-clicking a variable name moves that variable into the formula builder. To build our BMI formula:\n\ndouble-click weight\nclick the / button (to represent \\(\\div\\)). Note that you can type the / symbol instead of clicking the button\ndouble-click height\nclick the ^ button (to represent “to the power of”) - again you can type the ^ symbol\nenter 2 (as ^2 represents “to the power of 2”, or “squared”\n\nYour completed window should look like:\n\n\n\n\n\nClick OK to return to the generate dialog box, which now should appear as:\n\n\n\n\n\nOption 2: if you are entering a simple formula, you can type the formula directly into the expression using your keyboard.\nClick OK to generate your new variable.\n[Command: generate BMI = weight / height ^ 2]\nWe should check the construction of the new variable by examining the Data Browser:\n\n\n\n\n\nIn the general population, BMI ranges between about 15 to 30. It appears that BMI has been correctly generated in this example."
  },
  {
    "objectID": "02-probability.html#summarising-data-by-another-variable",
    "href": "02-probability.html#summarising-data-by-another-variable",
    "title": "2  Probability and probability distributions",
    "section": "2.18 Summarising data by another variable",
    "text": "2.18 Summarising data by another variable\nWe will often want to calculate the same summary statistics by another variable. For example, we might want to calculate summary statistics for BMI for males and females separately. We can do this in Stata by defining sex as a by-variable. Many Stata commands (and dialog boxes) allow for a by-variable. For example, to obtain summary statistics for BMI by sex (note that sex has been given a value label, as described in Module 1):\n\nDefine BMI as our Variable in the Statistics &gt; Summaries, tables, and tests &gt; Summary and descriptive statistics &gt; Summary statistics dialog box.\n\n\n\n\n\n\n\nClick the by/if/in tab, tick Repeat command by groups, and select the variable you want to be the by-variable. In this case, we choose sex and then click OK.\n\n\n\n\n\n\n[Command: by sex, sort : summarize BMI]\nThe by-variable will tell Stata to repeat the summarize command for each distinct value of sex. Hence, your by-variable must be a discrete variable with only a relatively small number of categories.\nThe following results appear:\n\n. by sex, sort : summarize BMI\n\n-----------------------------------------------------------------------------------\n-&gt; sex = Male\n\n    Variable |        Obs        Mean    Std. Dev.       Min        Max\n-------------+---------------------------------------------------------\n         BMI |        513    28.29561    5.204975   16.47519   57.23643\n\n-----------------------------------------------------------------------------------\n-&gt; sex = Female\n\n    Variable |        Obs        Mean    Std. Dev.       Min        Max\n-------------+---------------------------------------------------------\n         BMI |        627    27.81434    6.380523   9.209298   52.59515\n\nHere, we have calculated summary statistics separately for Males and Females. A form of the by tab appears in many Stata windows. For example, we can create separate histograms for males and females using the same two-step process:\n\nDefine BMI as the variable to be plotted\n\n\n\n\n\n\n\nDefine sex as a by-variable by completing the By tab:\n\n\n\n\n\n\nThis produces the following graph:"
  },
  {
    "objectID": "02-probability.html#recoding-data",
    "href": "02-probability.html#recoding-data",
    "title": "2  Probability and probability distributions",
    "section": "2.19 Recoding data",
    "text": "2.19 Recoding data\nOne task that is common in statistical computing is to recode variables. For example, we might want to group some categories of a categorical variable, or to present a continuous variable in a categorical way.\nIn this example, we can recode BMI into the following categories as suggested by the World Health Organisation [footnote]:\n\nUnderweight: BMI &lt; 18.5\nNormal weight: 18.5 \\(\\le\\) BMI &lt; 25\nPre-obesity: 25 \\(\\le\\) BMI &lt; 30\nObesity Class I: 30 \\(\\le\\) BMI &lt; 35\nObesity Class II: 35 \\(\\le\\) BMI &lt; 40\nObesity Class III: BMI \\(\\ge\\) 40\n\nTo recode data in Stata, we use the Recode categorical variable command. It may seem odd to use a command called Recode categorical variable to recode BMI, a continuous variable, but that is just the name Stata uses for its recode command.\n\nClick Data &gt; Create or change data &gt; Other variable-transformation commands &gt; Recode categorical variable to open the recode dialog box.\nChoose BMI as the variable to be recoded from.\nEnter the recoding rules. To recode from a range of values to a new category while assigning a value label, we enter the recoding rule in the following form:\n\n(a / b = c “label”) where:\n\na is the lower limit of the range to be recoded from\nb is the upper limit of the range to be recoded from\nc is the value of the new category to be recoded to\nlabel is the label to assign to the category c\n\nIt is important to note that the values of a and b are inclusive; and we can use the words min and max to represent the smallest and largest values respectively.\nSo to recode the underweight category, we would enter: (min / 18.4999 = 1 \"Underweight\") Similarly, for the normal weight category, we would enter: (18.5 / 24.9999 = 2 \"Normal weight\")\nYour completed recode dialog box should look as follows:\n\n\n\n\n\n\n\n\n\n\nDo not forget this step! To recode into a new variable, we must click the options tab to give the name of the new variable. If this step is not performed, Stata will overwrite the initial variable with the recoded version. Here, we tell Stata to generate a new variable called bmi_category:\n\n\n\n\n\n\n\n\n\n\nClick OK to complete the recoding.\nNote that the default label for the new recoded variable bmi_category is RECODE of BMI. This can be relabelled in the usual way: click on bmi_category in the Variables window, then enter a name such as BMI Category in the Label box in the Properties window."
  },
  {
    "objectID": "02-probability.html#computing-binomial-probabilities-using-the-hand-calculator-in-stata",
    "href": "02-probability.html#computing-binomial-probabilities-using-the-hand-calculator-in-stata",
    "title": "2  Probability and probability distributions",
    "section": "2.20 Computing binomial probabilities using the Hand calculator in Stata",
    "text": "2.20 Computing binomial probabilities using the Hand calculator in Stata\nThere are three Stata functions that we can use to calculate probabilities based on the binomial distribution: binomialp, binomial and binomialtail. Stata provides a description of the function in the Expression Builder, summarised below.\n\n\n\n\nSummary of Stata's binomial functions\nFunctionDescriptionExample\n\nbinomialp(n,k,p)The probability of observing k successes in n trials when the probability of a success on one trial is pProbability of observing 3 smokers from 6 people\n\nbinomial(n,k,p)The probability of observing k or fewer successes in n trials when the probability of a success on one trial is pProbability of observing 2 or fewer smokers from 6 people\n\nbinomialtail(n,k,p)The probability of observing k or more successes in n trials when the probability of a success on one trial is pProbability of observing 4 or more smokers from 6 people\n\n\n\n\nTo do the computation for part (a) in Worked Example 2.1, go to Data &gt; Other utilities &gt; Hand calculator from the main menu in Stata.\n\n\n\n\n\nIn the dialog box, click Create… to bring up the Expression Builder dialog box. In the Categories box, click on Functions to expand the list, then scroll down to Statistical, expand the list again and select Binomial.\n\n\n\n\n\nA list of functions will pop up to the right of the Categories box. Double click on binomialp() which will populate the top of the expression builder box with binomialp(n,k,p). Following the instructions at the bottom of the box:\n\nk is the number of successes, here, the number of smokers (i.e. k=3);\nn is the number of trials (i.e. n=6);\nand p is probability of drawing a smoker from the population, which is 19.8% (i.e. p=0.198).\n\nReplace each of these with the appropriate number into the box as shown below.\n\n\n\n\n\nClick the OK button in the Expression Builder box, then click OK or Submit in the main dialog box. The probability is now displayed on your Results screen as .08008454 which rounds off to 0.08 as shown in part (a) of Worked Example 2.1. (Note that Stata doesn’t print the 0 in front of the decimal point in the output.)\n[Command: display binomialp(6, 3, 0.198)]\nTo calculate the upper tail of probability in part (b), go to Data &gt; Other utilities &gt; Hand calculator from the main menu and bring up the Expression Builder box again. You may need to clear the text in your Expression box from your previous display command.\nIn the Categories box, navigate to Binomial and select it as you had before. Now double click on binomialtail() which will populate the top of the expression builder box with binomialtail(n,k,p). To obtain P(X \\(\\ge\\) 4) from 6 trials, n=6, k=4 and p=0.198. When you are done, click the OK button in the Expression Builder box to get the below in the display dialog box:\n\n\n\n\n\nClick OK or Submit when you are done. [Command: display binomialtail(6, 4, 0.198)]\nFor the lower tail for part (c), choose or double-click on binomial() from the list of functions. To obtain P(X \\(\\le\\) 2) from 6 trials, enter n=6, k=2 and p=0.198.\n\n\n\n\n\nClick OK or Submit when you are done. [Command: display binomial(6, 2, 0.198)]"
  },
  {
    "objectID": "02-probability.html#computing-probabilities-from-the-normal-distribution-using-the-hand-calculator-in-stata",
    "href": "02-probability.html#computing-probabilities-from-the-normal-distribution-using-the-hand-calculator-in-stata",
    "title": "2  Probability and probability distributions",
    "section": "2.21 Computing probabilities from the normal distribution using the Hand calculator in Stata",
    "text": "2.21 Computing probabilities from the normal distribution using the Hand calculator in Stata\nFrom Stata, the probability using a normal distribution can be obtained from the display command via Data &gt; Other utilities &gt; Hand calculator as shown in Worked example 2.1. In the dialog box, click the Create… button to bring up the Expression Builder dialog box. In the Categories box, click on Functions to expand the list, then scroll down to Statistical and expand the list again and select Normal this time. Double-click on normal() to populate the top of the expression builder box with normal(z).\nThe instructions at the bottom of the box explain that this is the cumulative standard normal distribution which means it gives P(Z&lt;z) in Stata. In other words, the normal() function gives you the lower tail probability from the standard normal distribution. To obtain P(Z&gt;0.5), edit the box to 1-normal(0.5) as shown below.\n\n\n\n\n\nClick OK, then Submit to obtain the same value as from the table. [Command: display 1-normal(0.5)]"
  },
  {
    "objectID": "02-probability.html#importing-data-into-r",
    "href": "02-probability.html#importing-data-into-r",
    "title": "2  Probability and probability distributions",
    "section": "2.22 Importing data into R",
    "text": "2.22 Importing data into R\nWe have described previously how to import data that have been saved as R .rds files. It is quite common to have data saved in other file types, such as Microsoft Excel, or plain text files. In this section, we will demonstrate how to import data from other packages into R.\nThere are two useful packages for importing data into R: haven (for data that have been saved by Stata, SAS or SPSS) and readxl (for data saved by Microsoft Excel). Additionally, the labelled package is useful in working with data that have been labelled in Stata.\n\n2.22.1 Importing plain text data into R\nA csv file, or a “comma separated variables” file is commonly used to store data. These files have a very simple structure: they are plain text files, where data are separated by commas. csv files have the advantage that, as they are plain text files, they can be opened by a large number of programs (such as Notepad in Windows, TextEdit in MacOS, Microsoft Excel - even Microsoft Word). While they can be opened by Microsoft Excel, they can be opened by many other programs: the csv file can be thought of as the lingua-franca of data.\nIn this demonstration, we will use data on the weight of 1000 people entered in a csv file called mod02_weight_1000.csv available on Moodle.\nTo confirm that the file is readable by any text editor, here are the first ten lines of the file, opened in Notepad on Microsoft Windows, and TextEdit on MacOS.\n\n\n\n\n\nWe can use the read.csv function:\n\nsample &lt;- read.csv(\"data/examples/mod02_weight_1000.csv\")\n\nHere, the read.csv function has the default that the first row of the dataset contains the variable names. If your data do not have column names, you can use header=FALSE in the function.\nNote: there is an alternative function read_csv which is part of the readr package (a component of the tidyverse). Some would argue that the read_csv function is more appropriate to use because of an issue known as strings.as.factors. The strings.as.factors default was removed in R Version 4.0.0, so it is less important which of the two functions you use to import a .csv file. More information about this issue can be found here and here."
  },
  {
    "objectID": "02-probability.html#checking-your-data-for-errors-in-r",
    "href": "02-probability.html#checking-your-data-for-errors-in-r",
    "title": "2  Probability and probability distributions",
    "section": "2.23 Checking your data for errors in R",
    "text": "2.23 Checking your data for errors in R\nBefore you start describing and analysing your data, it is important to make sure that no errors have been made during the data entry process. Basically, you are looking for values that are outside the range of possible or plausible values for that variable.\nIf an error is found, the best method for correcting the error is to go back to the original data e.g. the hard copy questionnaire, to obtain the original value, entering the correct value into R If the original data is not available or the original data is also incorrect, the erroneous value is often excluded from the dataset.\nFor continuous variables, the easiest methods are to examine a boxplot and histogram. For example, a boxplot and histogram for the weight variable we just imported appear as:\n\nhist(sample$weight, xlab=\"Weight (kg)\", main=\"Histogram of 1000 weights\")\n\n\n\nboxplot(sample$weight, xlab=\"Weight (kg)\", main=\"Boxplot of 1000 weights\")\n\n\n\n\nThere is a clear outlying point shown in the boxplot. Although not obvious, the same point is shown in the histogram as a bar around 700 with a very short height.\nWe can identify any outlying observations in the dataset using the subset function. You will need to decide if these values are a data entry error or are biologically plausible. If an extreme value or “outlier”, is biologically plausible, it should be included in all analyses.\nFor example, to list any observations from the sample dataset with a weight larger than 200:\n\nsubset(sample, weight&gt;200)\n\n\n\n\nidweight\n\n58700\n\n\n\n\nWe see that there is a very high value of 700.2kg. A value as high as 700kg is likely to be a data entry error (e.g. error in entering an extra zero) and is not a plausible weight value. Here, you should check your original data.\nYou might find that the original weight was recorded in medical records as 70.2kg. You can change this in R by writing code.\nNote: many statistical packages will allow you to view a spreadsheet version of your data and edit values in that spreadsheet. This is not best practice, as corrected observations may revert to their original values depending on whether the edited data have been saved or not. By using code-based recoding, the changes will be reproduced the next time the code is run.\nWe will use an ifelse statement to recode the incorrect weight of 700.2kg into 70.2kg. The form of the ifelse statement is as follows:\nifelse(test, value_if_true, value_if_false)\nOur code will create a new column (called weight_clean) in the sample dataframe. We will test whether weight is equal to 700.2; if this is true, we will assign weight_clean to be 70.2, otherwise weight_clean will equal the value of weight.\nPutting it all together:\n\nsample$weight_clean = ifelse(sample$weight==700.2, 70.2, sample$weight)\n\nNote: if an extreme value lies within the range of biological plausibility it should not be removed from analysis.\nOnce you have checked your data for errors, you are ready to start analysing your data.\n\n2.23.1 What on earth: == ?\nIn R, the test of equality is denoted by two equal signs: ==. So we would use == to test whether an observation is equal to a certain value. Let’s see an example:\n\n# Test whether 6 is equal to 6\n6 == 6\n\n[1] TRUE\n\n# Test whether 6 is equal to 42\n6 == 42\n\n[1] FALSE\n\n\nYou can read the == as “is equal to”. So the code sample$weight == 700.2 is read as: “is the value of weight from the data frame sample equal to 700.2?”. In our ifelse statement above, if this condition is true, we replace weight by 70.2; if it is false, we leave weight as is."
  },
  {
    "objectID": "02-probability.html#overlaying-a-normal-curve-on-a-histogram-1",
    "href": "02-probability.html#overlaying-a-normal-curve-on-a-histogram-1",
    "title": "2  Probability and probability distributions",
    "section": "2.24 Overlaying a Normal curve on a histogram",
    "text": "2.24 Overlaying a Normal curve on a histogram\nIt can be useful to produce a histogram with an overlayed Normal curve to assess whether our sample appears approximately Normally distributed. We can do this by plotting a histogram using the hist() function. As we’re overlaying a probability distribution, we request the histogram be plotted on a probability scale, rather than a frequency scale, using probability=TRUE.\nWe then request a curve be overlayed using the curve() function:\n\nthe curve should be based on the Normal distribution (dnorm);\n\nwith a mean equal to the mean of the cleaned weight: mean(sample$weight_clean));\nand a standard deviation equal to the standard deviation of the cleaned weight: sd(sample$weight_clean))\n\nusing a dark-blue colour;\nand added to the previous histogram (rather than plotting the curve by itself): add=TRUE\n\n\nhist(sample$weight_clean,\n     xlab=\"Weight (kg)\",\n     main=\"Histogram of 1000 weights\",\n     probability = TRUE)\n\ncurve(dnorm(x,\n            mean=mean(sample$weight_clean),\n            sd=sd(sample$weight_clean)),\n      col=\"darkblue\",\n      add=TRUE)\n\n\n\n\nNotice that the top of the curve is chopped off. We can plot the whole curve by extending the y-axis of the histogram to 0.1:\n\nhist(sample$weight_clean, \n     xlab=\"Weight (kg)\",\n     main=\"Histogram of 1000 weights\",\n     probability = TRUE,\n     ylim=c(0,0.1))\n\ncurve(dnorm(x,\n            mean=mean(sample$weight_clean),\n            sd=sd(sample$weight_clean)),\n      col=\"darkblue\",\n      add=TRUE)"
  },
  {
    "objectID": "02-probability.html#descriptive-statistics-for-checking-normality-1",
    "href": "02-probability.html#descriptive-statistics-for-checking-normality-1",
    "title": "2  Probability and probability distributions",
    "section": "2.25 Descriptive statistics for checking normality",
    "text": "2.25 Descriptive statistics for checking normality\nAll the descriptive statistics including skewness and kurtosis discussed in this module can be obtained using the descriptives function from the jmv package. In particular, skewness and kurtosis can be requested in addition to the default statistics by including: skew=TRUE, kurt=TRUE:\n\nlibrary(jmv)\n\ndescriptives(data=sample, vars=weight_clean, skew=TRUE, kurt=TRUE)\n\n\n DESCRIPTIVES\n\n Descriptives                            \n ─────────────────────────────────────── \n                          weight_clean   \n ─────────────────────────────────────── \n   N                              1000   \n   Missing                           0   \n   Mean                       69.76450   \n   Median                     69.80000   \n   Standard deviation         5.052676   \n   Minimum                    53.80000   \n   Maximum                    85.80000   \n   Skewness                 0.07360659   \n   Std. error skewness      0.07734382   \n   Kurtosis                 0.05418774   \n   Std. error kurtosis       0.1545343   \n ───────────────────────────────────────"
  },
  {
    "objectID": "02-probability.html#importing-excel-data-into-r",
    "href": "02-probability.html#importing-excel-data-into-r",
    "title": "2  Probability and probability distributions",
    "section": "2.26 Importing Excel data into R",
    "text": "2.26 Importing Excel data into R\nAnother common type of file that data are stored in is a Microsoft Excel file (.xls or .xlsx). In this demonstration, we will import a selection of records from a large health survey, stored in the file mod02_health_survey.xlsx.\nThe health survey data contains 1140 records, comprising:\n\nsex: 1 = respondent identifies as male; 2 = respondent identifies as female\nheight: height in meters\nweight: weight in kilograms\n\nTo import data from Microsoft Excel, we can use the read_excel() function in the readxl package.\n\nlibrary(readxl)\n\nsurvey &lt;- read_excel(\"data/examples/mod02_health_survey.xlsx\")\nsummary(survey)\n\n      sex           height          weight      \n Min.   :1.00   Min.   :1.220   Min.   : 22.70  \n 1st Qu.:1.00   1st Qu.:1.630   1st Qu.: 68.00  \n Median :2.00   Median :1.700   Median : 79.40  \n Mean   :1.55   Mean   :1.698   Mean   : 81.19  \n 3rd Qu.:2.00   3rd Qu.:1.780   3rd Qu.: 90.70  \n Max.   :2.00   Max.   :2.010   Max.   :213.20  \n\n\nWe can see that sex has been entered as a numeric variable. We should transform it into a factor so that we can assign labels to each category:\n\nsurvey$sex &lt;- factor(survey$sex, level=c(1,2), labels=c(\"Male\", \"Female\"))\n\nsummary(survey$sex)\n\n  Male Female \n   513    627 \n\n\nWe also note that height looks like it has been entered as meters, and weight as kilograms."
  },
  {
    "objectID": "02-probability.html#generating-new-variables-1",
    "href": "02-probability.html#generating-new-variables-1",
    "title": "2  Probability and probability distributions",
    "section": "2.27 Generating new variables",
    "text": "2.27 Generating new variables\nOur health survey data contains information on height and weight. We often summarise body size using BMI: body mass index which is calculated as: \\(\\frac{\\text{weight (kg)}}{(\\text{height (m)})^2}\\)\nWe can create a new column in our data frame in many ways, such as using the following approach:\ndataframe$new_column &lt;- &lt;formula&gt;\nFor example:\n\nsurvey$bmi &lt;- survey$weight / (survey$height^2)\n\nWe should check the construction of the new variable by examining some records. The head() and tail() functions list the first and last 6 records in any dataset. We can also examine a histogram and boxplot:\n\nhead(survey)\n\n\n\n\nsexheightweightbmi\n\nMale1.6381.730.8\n\nMale1.6368  25.6\n\nMale1.8597.128.4\n\nMale1.7889.828.3\n\nMale1.7370.323.5\n\nFemale1.5785.734.8\n\n\n\ntail(survey)\n\n\n\n\nsexheightweightbmi\n\nFemale1.6595.735.2\n\nMale1.8 79.424.5\n\nFemale1.7383  27.7\n\nFemale1.5761.224.8\n\nMale1.7 73  25.3\n\nFemale1.5591.238  \n\n\n\nhist(survey$bmi)\n\n\n\nboxplot(survey$bmi)\n\n\n\n\nIn the general population, BMI ranges between about 15 to 30. It appears that BMI has been correctly generated in this example. We should investigate the very low and some of the very high values of BMI, but this will be left for another time."
  },
  {
    "objectID": "02-probability.html#summarising-data-by-another-variable-1",
    "href": "02-probability.html#summarising-data-by-another-variable-1",
    "title": "2  Probability and probability distributions",
    "section": "2.28 Summarising data by another variable",
    "text": "2.28 Summarising data by another variable\nWe will often want to calculate the same summary statistics by another variable. For example, we might want to calculate summary statistics for BMI for males and females separately. We can do this in in the descriptives function by defining sex as a splitBy variable:\n\nlibrary(jmv)\ndescriptives(data=survey, vars=bmi, splitBy = sex)\n\n\n DESCRIPTIVES\n\n Descriptives                                 \n ──────────────────────────────────────────── \n                         sex       bmi        \n ──────────────────────────────────────────── \n   N                     Male           513   \n                         Female         627   \n   Missing               Male             0   \n                         Female           0   \n   Mean                  Male      28.29561   \n                         Female    27.81434   \n   Median                Male      27.39592   \n                         Female    26.66667   \n   Standard deviation    Male      5.204975   \n                         Female    6.380523   \n   Minimum               Male      16.47519   \n                         Female    9.209299   \n   Maximum               Male      57.23644   \n                         Female    52.59516   \n ────────────────────────────────────────────"
  },
  {
    "objectID": "02-probability.html#summarising-a-single-column-of-data",
    "href": "02-probability.html#summarising-a-single-column-of-data",
    "title": "2  Probability and probability distributions",
    "section": "2.29 Summarising a single column of data",
    "text": "2.29 Summarising a single column of data\nIn Module 1, we started with a very simple analysis: reading in six ages, and them using summary() to calculate descriptive statistics. We then went on to use the decriptives() function in the jmv package as more flexible way of calculating descriptive statistics. Let’s revisit this analysis:\n\n# Author: Timothy Dobbins\n# Date: 5 April 2022\n# Purpose: My first R script\nlibrary(jmv)\n\nage &lt;- c(20, 25, 23, 29, 21, 27)\n\n# Use \"summary\" to obtain descriptive statistics\nsummary(age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  20.00   21.50   24.00   24.17   26.50   29.00 \n\n# Use the \"descriptives\" function from jmv to obtain descriptive statistics\ndescriptives(age)\n\nError: Argument 'data' must be a data frame\n\n\nThe summary() function has worked correctly, but the descriptives() function has given an error: Error: Argument 'data' must be a data frame. What on earth is going on here?\nThe error gives us a clue here - the descriptives() function requires a data frame for analysis. We have provided the object age: a vector. As we saw in ?sec-data-structures, a vector is a single column of data, while a data frame is a collection of columns.\nIn order to summarise a vector using the descriptives() function, we must first convert the vector into a data frame using as.data.frame(). For example:\n\n# Author: Timothy Dobbins\n# Date: 5 April 2022\n# Purpose: My first R script\nlibrary(jmv)\n\nage &lt;- c(20, 25, 23, 29, 21, 27)\n\n# Use \"summary\" to obtain descriptive statistics\nsummary(age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  20.00   21.50   24.00   24.17   26.50   29.00 \n\n# Create a new data frame from the vector age:\n\nage_df &lt;- as.data.frame(age)\n\n# Use \"descriptives\" to obtain descriptive statistics for age_df\ndescriptives(age_df)\n\n\n DESCRIPTIVES\n\n Descriptives                       \n ────────────────────────────────── \n                         age        \n ────────────────────────────────── \n   N                            6   \n   Missing                      0   \n   Mean                  24.16667   \n   Median                24.00000   \n   Standard deviation    3.488075   \n   Minimum               20.00000   \n   Maximum               29.00000   \n ──────────────────────────────────"
  },
  {
    "objectID": "02-probability.html#plotting-data-by-another-variable",
    "href": "02-probability.html#plotting-data-by-another-variable",
    "title": "2  Probability and probability distributions",
    "section": "2.30 Plotting data by another variable",
    "text": "2.30 Plotting data by another variable\nUnfortunately, it is not straight-forward to create separate plots for every level of another variable. We will demonstrate by plotting BMI by sex using our health survey data.\nThe following steps are not the most efficient way of doing this, but are easy to follow and understand. We first begin by creating two new data frames, for males and females separately, using the subset() function:\n\nsurvey_males &lt;- subset(survey, sex==\"Male\")\nsurvey_females &lt;- subset(survey, sex==\"Female\")\n\nNote that we use the label for sex, not the underlying numeric value, as sex is a factor.\nWe can now create hisotgrams and boxplots of BMI for males and females separately. To place the graphs next to each other in a single figure, we can use the par function, which sets the graphics parameters. Essentially, we want to tell R to split a plot window into a matrix with nr rows and nc columns, and we fill the cells by rows (mfrow) or columns (mfcols).\nFor example, to plot four figures in a single plot, filled by rows, we use par(mfrow=c(2,2)).\nWhen we are done plotting multiple graphs, we can reset the graphics parameters by submitting par(mfrow=c(1,1)).\n\n# Set the graphics parameters to plot 2 rows and 2 columns:\npar(mfrow=c(2,2))\n\n# Specify each plot separately\nhist(survey_males$bmi, xlab=\"BMI (kg/m2)\", main=\"Males\")\nhist(survey_females$bmi, xlab=\"BMI (kg/m2)\", main=\"Females\")\n\nboxplot(survey_males$bmi, ylab=\"BMI (kg/m2)\", main=\"Males\")\nboxplot(survey_females$bmi, ylab=\"BMI (kg/m2)\", main=\"Females\")\n\n\n\n# Reset graphics parameters\npar(mfrow=c(1,1))"
  },
  {
    "objectID": "02-probability.html#sec-recoding-data",
    "href": "02-probability.html#sec-recoding-data",
    "title": "2  Probability and probability distributions",
    "section": "2.31 Recoding data",
    "text": "2.31 Recoding data\nOne task that is common in statistical computing is to recode variables. For example, we might want to group some categories of a categorical variable, or to present a continuous variable in a categorical way.\nIn this example, we can recode BMI into the following categories as suggested by the World Health Organisation [footnote]:\n\nUnderweight: BMI &lt; 18.5\nNormal weight: 18.5 \\(\\le\\) BMI &lt; 25\nPre-obesity: 25 \\(\\le\\) BMI &lt; 30\nObesity Class I: 30 \\(\\le\\) BMI &lt; 35\nObesity Class II: 35 \\(\\le\\) BMI &lt; 40\nObesity Class III: BMI \\(\\ge\\) 40\n\nThe quickest way to recode a continuous variable into categories is to use the cut command which takes a continuous variable, and “cuts” it into groups based on the specified “cutpoints”\n\nsurvey$bmi_cat &lt;- cut(survey$bmi, \n                      breaks = c(0, 18.5, 25, 30, 35, 40, 100))\n\nNotice that lower (BMI=0) and upper (BMI=100) bounds have been specified, as both a lower and upper limit must be defined for each group.\nIf we examine the new bmi_cat variable:\n\nsummary(survey$bmi_cat)\n\n (0,18.5] (18.5,25]   (25,30]   (30,35]   (35,40]  (40,100] \n       18       362       411       205        97        47 \n\n\nwe see that each group has been labelled (a, b]. This notation is equivalent to: greater than a, and less than or equal to b. The cut function excludes the lower limit, but includes the upper limit. Our BMI ranges have been defined to include the lower limit, and exclude the upper limit (for example, greater than or equal to 30 and less than 35).\nWe can specify this recoding using the right=FALSE option:\n\nsurvey$bmi_cat &lt;- cut(survey$bmi,\n                      breaks = c(0, 18.5, 25, 30, 35, 40, 100),\n                      right=FALSE)\n\nsummary(survey$bmi_cat)\n\n [0,18.5) [18.5,25)   [25,30)   [30,35)   [35,40)  [40,100) \n       18       362       411       201       101        47 \n\n\nFinally, we can specify labels for the groups using the labels option:\n\nsurvey$bmi_cat &lt;- cut(survey$bmi,\n                      breaks = c(0, 18.5, 25, 30, 35, 40, 100),\n                      right=FALSE,\n                      labels = c(\"Underweight\", \"Normal\", \"Pre-obesity\",\n                                 \"Obesity Class I\", \"Obesity Class II\",\n                                 \"Obesity Class III\"))\n\nsummary(survey$bmi_cat)\n\n      Underweight            Normal       Pre-obesity   Obesity Class I \n               18               362               411               201 \n Obesity Class II Obesity Class III \n              101                47"
  },
  {
    "objectID": "02-probability.html#computing-binomial-probabilities-using-r",
    "href": "02-probability.html#computing-binomial-probabilities-using-r",
    "title": "2  Probability and probability distributions",
    "section": "2.32 Computing binomial probabilities using R",
    "text": "2.32 Computing binomial probabilities using R\nThere are two R functions that we can use to calculate probabilities based on the binomial distribution: dbinom and pbinom:\n\ndbinom(x, size, prob) gives the probability of obtaining x successes from size trials when the probability of a success on one trial is prob;\npbinom(q, size, prob) gives the probability of obtaining q or fewer successes from size trials when the probability of a success on one trial is prob;\npbinom(q, size, prob, lower.tail=FALSE) gives the probability of obtaining more than qsuccesses from size trials when the probability of a success on one trial is prob.\n\nTo do the computation for part (a) in Worked Example 2.1, we will use the dbinom function with:\n\nx is the number of successes, here, the number of smokers (i.e. k=3);\nsize is the number of trials (i.e. n=6);\nand prob is probability of drawing a smoker from the population, which is 19.8% (i.e. p=0.198).\n\nReplace each of these with the appropriate number into the formula:\n\ndbinom(x=3, size=6, prob=0.198)\n\n[1] 0.08008454\n\n\nTo calculate the upper tail of probability in part (b), we use the pbinom(lower.tail=FALSE) function. Note that the pbinom(lower.tail=FALSE) function does not include q, so to obtain 4 or more successes, we need to enter q=3:\n\npbinom(q=3, size=6, prob=0.198, lower.tail=FALSE)\n\n[1] 0.01635325\n\n\nFor the lower tail for part (c), we use the pbinom function:\n\npbinom(q=2, size=6, prob=0.198)\n\n[1] 0.9035622"
  },
  {
    "objectID": "02-probability.html#computing-probabilities-from-a-normal-distribution",
    "href": "02-probability.html#computing-probabilities-from-a-normal-distribution",
    "title": "2  Probability and probability distributions",
    "section": "2.33 Computing probabilities from a Normal distribution",
    "text": "2.33 Computing probabilities from a Normal distribution\nWe can use the pnorm function to calculate probabilities from a Normal distribution:\n\npnorm(q, mean, sd) calculates the probability of observing a value of q or less, from a Normal distribution with a mean of mean and a standard deviation of sd. Note that if mean and sd are not entered, they are assumed to be 0 and 1 respectively (i.e. a standard normal distribution.)\npnorm(q, mean, sd, lower.tail=FALSE) calculates the probability of observing a value of more than q, from a Normal distribution with a mean of mean and a standard deviation of sd.\n\nTo obtain the probability of obtaining 0.5 or greater from a standard normal distribution:\n\npnorm(0.5, lower.tail=FALSE)\n\n[1] 0.3085375\n\n\nTo calculate the worked example: Assume that the mean diastolic blood pressure for men is 77.9 mmHg, with a standard deviation of 11. What is the probability that a man selected at random will have high blood pressure (i.e. diastolic blood pressure greater than or equal to 90)?\n\npnorm(90, mean=77.9, sd=11, lower.tail=FALSE)\n\n[1] 0.1356661"
  },
  {
    "objectID": "02-probability.html#footnotes",
    "href": "02-probability.html#footnotes",
    "title": "2  Probability and probability distributions",
    "section": "",
    "text": "Source: https://www.aihw.gov.au/reports/risk-factors/high-blood-pressure/contents/high-blood-pressure (accessed March 2021)↩︎\nSource: https://ourworldindata.org/human-height (accessed March 2021)↩︎"
  },
  {
    "objectID": "05-comparing-two-means.html",
    "href": "05-comparing-two-means.html",
    "title": "3  Comparing the means of two groups",
    "section": "",
    "text": "Stata notes"
  },
  {
    "objectID": "05-comparing-two-means.html#learning-objectives",
    "href": "05-comparing-two-means.html#learning-objectives",
    "title": "3  Comparing the means of two groups",
    "section": "Learning objectives",
    "text": "Learning objectives\nBy the end of this module you will be able to:\n\nDecide whether to use an independent samples t-test or a paired t-test to compare two the means of two groups;\nConduct and interpret the results from an independent samples t-test;\nDescribe the assumptions of an independent samples t-test;\nConduct and interpret the results from a paired t-test;\nDescribe the assumptions of a paired t-test;\nConduct an independent samples t-test and a paired t-test using software;\nReport results and provide a concise summary of the findings of statistical analyses."
  },
  {
    "objectID": "05-comparing-two-means.html#optional-readings",
    "href": "05-comparing-two-means.html#optional-readings",
    "title": "3  Comparing the means of two groups",
    "section": "Optional readings",
    "text": "Optional readings\nKirkwood and Sterne (2001); Sections 7.1 to 7.5. [UNSW Library Link]\nBland (2015); Section 10.3. [UNSW Library Link]\nAcock (2010); Section 7.7, 7.8."
  },
  {
    "objectID": "05-comparing-two-means.html#introduction",
    "href": "05-comparing-two-means.html#introduction",
    "title": "3  Comparing the means of two groups",
    "section": "3.1 Introduction",
    "text": "3.1 Introduction\nIn Module 4, a one-sample t-test was used for comparing a single mean to a hypothesised value. In health research, we often want to compare the means between two groups. For example, in an observational study, we may want to compare cholesterol levels in people who exercise regularly to the levels in people who do not exercise regularly. In a clinical trial, we may want to compare cholesterol levels in people who have been randomised to a dietary modification or to usual care. In this module, we show how to compare the means of two groups where the analysis variable is normally distributed.\nFrom the decision tree presented in the Appendix, we can see that if we have a continuous outcome measure and two categorical groups that are not related, i.e. a binary exposure measurement, the test for such data is an independent samples t-test. The test is also sometimes called a 2-sample t-test.\nIn research, data are often ‘paired’ or ‘matched’, that is the two data points are related to one another. This occurs when measurements are taken:\n\nFrom each participant on two occasions, e.g. at baseline and follow-up in an experimental study or in a longitudinal cohort study;\nFrom related people, e.g. a mother and daughter or a child and their sibling;\nFrom related sites in the same person, e.g. from both limbs, eyes or kidneys;\nFrom matched participants e.g. in a matched case-control study;\nIn cross-over clinical trials where the patient receives both drugs, often in random order.\n\nAn independent samples t-test cannot be used for analysing paired or matched data because the assumption that the two groups are independent is violated. Treating paired or matched measurements as independent samples would artificially inflate the sample size and lead to inaccurate P values. When the data are related in a paired or matched way and the outcome is continuous, a paired t-test is the appropriate statistic to use if the data are normally distributed."
  },
  {
    "objectID": "05-comparing-two-means.html#independent-samples-t-test",
    "href": "05-comparing-two-means.html#independent-samples-t-test",
    "title": "3  Comparing the means of two groups",
    "section": "3.2 Independent samples t-test",
    "text": "3.2 Independent samples t-test\nAn independent samples t-test is a parametric test that is used to assess whether the mean values of two groups are different from one another. Thus, the test is used to assess whether two mean values are similar enough to have come from the same population or whether the difference between them is so large that the two groups can be considered to have come from separate populations with different characteristics.\nThe null hypothesis is that the mean values of the two groups are not different, that is:\nH0: (\\(\\mu_1 - \\mu_2\\)) = 0\nRejecting the null hypothesis using an independent samples t-test indicates that the difference between the means of the two groups is large in relation to the variability in the samples and is unlikely to be due to chance or to sampling variation.\n\n3.2.1 Assumptions for an independent samples t-test\nThe assumptions that must be met before an independent samples t-test can be used are:\n\nThe two groups are independent\nThe measurements are independent\nThe analysis variable must be continuous and must be normally distributed in each group\n\nThe first two assumptions are determined by the study design. The two samples must be independent, i.e. if a person is in one group then they cannot be included in the other group, and the measurements within a sample must be independent, i.e. each person must be included in their group once only.\nThe third assumption of normality is important although t-tests are robust to some degree of non-normality as long as there are no influential outliers and, more importantly, if the sample size is large. We examined how to assess normality in Module 2. If the data are not normally distributed, it may be possible to transform them using a mathematical function such as a logarithmic transformation. If not, then we may need to use non-parametric tests. This is examined in Module 9.\nTraditionally, the variance of the analysis variable in each group was assumed to be equal. However, this assumption can be relaxed by using Welch’s variation of the t-test. It has been recommended that this unequal-variances t-test be used in most, if not all situations (West 2021; Delacre, Lakens, and Leys 2017; Ruxton 2006).\n\n\n3.2.2 Worked Example\nIn an observational study of a random sample of 100 full term babies from the community, birth weight and gender were measured. There were 44 male babies and 56 female babies in the sample. The research question asked whether there was a difference in birth weights between boys and girls. The two groups are independent of each other and therefore an independent samples t-test can be used to test the null hypothesis that there is no difference in weight between the genders.\nSome preliminary descriptive statistics of the distribution of the variable of interest in each group should always be obtained before a t-test is undertaken to ensure that the assumptions are met. Box plots and histograms are ideal for this. Histrograms and box plots of the data obtained in Stata using Graphics &gt; Box plot is shown in Figure 3.1. The datasets mod05_birthweight.dta and mod05_birthweight.rds are available on Moodle.\n\n\n\n\n\nFigure 3.1: Histograms and box plots of birth weight by gender\n\n\n\n\nThe plots show that the data are approximately normally distributed: the histograms are relatively bell shaped and symmetric, and the boxes are fairly symmetrical, there are no outliers, and the spread is similar in both groups as the similar length of the whiskers suggesting that the variance is equal between groups.\nWe can also describe the data using summary statistics:\n\n\n\n\nTable 3.1:  Summary of birthweight by gender \n\nCharacteristic\nFemale\nMale\n\n\nBirthweight\n\nN5644\n\nMean (SD)3.59 (0.36)3.42 (0.35)\n\nMedian (IQR)3.53 (3.33, 3.87)3.43 (3.16, 3.63)\n\nRange2.95, 4.252.75, 4.10\n\n\n\n\n\nThe table shows that girls have a mean weight of 3.59 kg (SD 0.36) and boys have a mean weight of 3.42 kg (SD 0.35) with females being heavier than males. The variabilities of birth weight, as indicated by the standard deviations, are similar.\n\n\n3.2.3 Conducting and interpreting an independent samples t-test\nAn independent samples t-test provides us with a t statistic from which we can compute a P value. The computation of the t statistic is as follows:\n\\[t = \\frac{{\\overline{x}}_{1} - {\\overline{x}}_{2}}{SE({\\overline{x}}_{1} - {\\overline{x}}_{2})}\\]\nwith the standard error and degrees of freedom calculated from software. Note that by using Welch’s t-test, the degrees of freedom will usually not be a whole number, and will appear with decimals.\nLooking at the formula for the t-statistic, we can see that the \\(t\\) is an estimate of how different the mean values are compared to the variability of the difference in means. So \\(t\\) will become larger as the difference in means increases with respect to the variability.\nStatistical software will calculate both the t and P values. If the t-value is large, the P value will be small, providing evidence against the null hypothesis of no difference between the groups.\nTable 3.2 summarises the results of an independent samples t-test using mod05_birthweight.dta or mod05_birthweight.rds. The process of conducting the t-test is summarised for Stata and R in the following sections.\n\n\n\n\nTable 3.2:  Birthweight (kg) by sex \n\nSexnMean (SE)95% Confidence Interval\n\nFemale563.59 (0.049)3.49 to 3.68\n\nMale443.42 (0.053)3.31 to 3.53\n\nDifference0.17 (0.072)0.02 to 0.31\n\n\n\n\n\nHere we see that girls are heavier than boys, and the mean difference in weights between the genders is 0.17 kg (95% CI 0.02, 0.31). We are 95% confident that the true mean difference of weight between girls and boys lies between 0.02 and 0.31 kg. Note that this interval does not contain the null value of 0.\nHere we are testing the null hypothesis of no difference in mean birthweights between females and males: a two-sided test. The t-value is calculated as 2.30 with 93.5 degrees of freedom, and yields a two-sided P value of 0.023, providing evidence of a difference in mean birthweight between sex."
  },
  {
    "objectID": "05-comparing-two-means.html#paired-t-tests",
    "href": "05-comparing-two-means.html#paired-t-tests",
    "title": "3  Comparing the means of two groups",
    "section": "3.3 Paired t-tests",
    "text": "3.3 Paired t-tests\nIf the outcome of interest is the difference in the continuously outcome measurement between each pair of observations, a paired t-test is used. In effect, a paired t-test is used to assess whether the mean of the differences between the two related measurements is significantly different from zero. In this sense, a paired t-test is very closely aligned with a one sample t-test.\nWhen using a paired t-test, the variation between the pairs of measurements is the most important statistic. The variation between the participants is of little interest.\nFor related measurements, the data for each pair of values must be entered on the same row of the spreadsheet. Thus, the number of rows in the data sheet is the number of pairs of observations. Thus, the effective sample size is the total number of pairs and not the total number of measurements.\n\n3.3.1 Assumptions for a paired t-test\nThe assumptions for a paired t-test are:\n\nthe outcome variable is continuous\nthe differences between the pair of the measurements are normally distributed\n\nFor a paired samples t-test, it is important to test whether the differences between the two measurements are normally distributed. If the assumptions for a paired t-test cannot be met, a non-parametric equivalent is a more appropriate test to use (Module 9).\n\n\n3.3.2 Computing a paired t-test\nThe null hypothesis for using a paired t-test is as follows:\nH0: Mean (Measurement1 – Measurement2) = 0\nTo compute a t-value, the size of the mean difference between the two measurements is compared to the standard error of the paired differences, i.e.\n\\[t = \\frac{\\overline{d}}{SE(\\overline{d})}\\]\nwith n–1 degrees of freedom, where n is the number of pairs.\nBecause the standard error becomes smaller as the sample size becomes larger, the t-value increases as the sample size increases for the same mean difference.\n\n\n3.3.3 Worked Example 5.2\nA total of 107 people were recruited into a study to assess whether ankle blood pressure measured in two different sites would be the same. For each person, systolic blood pressure (SBP) was measured in two sites: dorsalis pedis and tibialis posterior.\nThe dataset mod05_ankle_bp.xls is available on Moodle. First, we need to compute the pairwise difference between SBP measured in the two sites in Stata using the generate command. This is shown in the Stata manual at the end of this module (Checking the assumptions for a Paired t-test). The distribution of the difference between SBP measured in dorsalis pedis and tibialis posterior is shown in Figure 3.2. The differences approximate a normal distribution and therefore a paired t-test can be used.\n\n\n\n\n\nFigure 3.2: Distribution of differences in ankle SBP between two sites of 107 participants\n\n\n\n\nThe paired t-test can be performed using statistical software, with a summary of the results presented in Table 3.3. We can see that the mean SBP is very similar in the two sites.\n\n\n\n\nTable 3.3:  Systolic blood pressure (mmHg) measured at two sites on the ankle \n\nSitenMean (SE)95% Confidence Interval\n\nDorsalis pedis107116.7 (3.46)(109.9 to 123.6)\n\nTibialis posterior107118.0 (3.43)(111.2 to 124.8)\n\nDifference107-1.3 (1.31)(-3.9 to 1.3)\n\n\n\n\n\nThe t-value is calculated as −0.96 with 106 degrees of freedom, providing a two-sided P-value of 0.34. Thus these data provide no evidence of a difference in systolic blood pressure between the two sites."
  },
  {
    "objectID": "05-comparing-two-means.html#setting-an-observation-to-missing",
    "href": "05-comparing-two-means.html#setting-an-observation-to-missing",
    "title": "3  Comparing the means of two groups",
    "section": "3.4 Setting an observation to missing",
    "text": "3.4 Setting an observation to missing\nSetting an incorrect observation to missing is straightforward in Stata by using the Data Editor (make sure you are in Edit mode). Click the cell containing the data to be set to missing, press the Delete key, and the press Enter."
  },
  {
    "objectID": "05-comparing-two-means.html#checking-data-for-the-independent-samples-t-test",
    "href": "05-comparing-two-means.html#checking-data-for-the-independent-samples-t-test",
    "title": "3  Comparing the means of two groups",
    "section": "3.5 Checking data for the independent samples t-test",
    "text": "3.5 Checking data for the independent samples t-test\n\n3.5.1 Producing histograms and boxplots by a second variable\nTo obtain the histograms in Figure 3.1 using the mod05_birthweight.dta data, go to Graphics &gt; Histogram. Select birthweight as the Variable in the Main tab. Next go to the By tab, tick Draw subgraphs for unique values of variable and select gender as the variable as shown below:\n\n\n\n\n\nNote that we have also improved the basic histogram definition on the Main tab, by defining the Lower limit of first bin to be 2.5, and the Bin width to be 0.25kg:\n\n\n\n\n\nA similar process is used to obtain the boxplots shown in Figure 3.1: go to Graphics &gt; Box plot. In the graph box – Box plots dialog box, select birthweight as the Variable in the Main tab. Next go to the Categories tab, tick Group 1 and select gender as the Grouping variable as shown below.\n\n\n\n\n\nClick OK or Submit to produce the box plot.\n[Command: graph box birthweight, over(gender)]\n\n\n3.5.2 Producing split summary statistics\nTo produce summary statistics for a continuous variable, split by a second binary categorical variable, as in Worked Example 5.1 using mod05_birthweight.dta, go to Statistics &gt; Summaries, tables, and tests &gt; Summary and descriptive statistics &gt; Summary statistics . In the summarize dialog box, select birthweight as the Variable in the Main tab of the dialog box. Next go to the by/if/in tab, tick Repeat command by groups and select gender as the Variable that define groups as shown below.\n\n\n\n\n\nClick OK or Submit to obtain the output as shown below.\n[Command: by gender, sort : summary birthweight]\n\n. by gender, sort : summ birthweight\n\n--------------------------------------------------------------------------------\n-&gt; gender = Female\n\n    Variable |        Obs        Mean    Std. Dev.       Min        Max\n-------------+---------------------------------------------------------\n birthweight |         56    3.587411    .3629788       2.95       4.25\n\n--------------------------------------------------------------------------------\n-&gt; gender = Male\n\n    Variable |        Obs        Mean    Std. Dev.       Min        Max\n-------------+---------------------------------------------------------\n birthweight |         44    3.421364    .3536165       2.75        4.1\n\nThe output above is easy for copying into a report. You could also submit the summarize command with the detail option to compare the mean with the median (50th percentile) and check the minimum and maximum values for implausible values.\n[Command: by gender, sort : sum birthweight , detail]\n\n. by gender, sort : summ birthweight, detail\n\n--------------------------------------------------------------------------------\n-&gt; gender = Female\n\n                         Birthweight\n-------------------------------------------------------------\n      Percentiles      Smallest\n 1%         2.95           2.95\n 5%         3.03           2.97\n10%         3.14           3.03       Obs                  56\n25%        3.325           3.07       Sum of Wgt.          56\n\n50%         3.53                      Mean           3.587411\n                        Largest       Std. Dev.      .3629788\n75%         3.88            4.2\n90%         4.15            4.2       Variance       .1317536\n95%          4.2            4.2       Skewness       .2453238\n99%         4.25           4.25       Kurtosis       1.962126\n\n--------------------------------------------------------------------------------\n-&gt; gender = Male\n\n                         Birthweight\n-------------------------------------------------------------\n      Percentiles      Smallest\n 1%         2.75           2.75\n 5%         2.82           2.79\n10%         2.85           2.82       Obs                  44\n25%         3.15           2.85       Sum of Wgt.          44\n\n50%         3.43                      Mean           3.421364\n                        Largest       Std. Dev.      .3536165\n75%        3.635           3.94\n90%          3.9           3.97       Variance       .1250446\n95%         3.97           4.06       Skewness      -.0895932\n99%          4.1            4.1       Kurtosis       2.325761"
  },
  {
    "objectID": "05-comparing-two-means.html#independent-samples-t-test-1",
    "href": "05-comparing-two-means.html#independent-samples-t-test-1",
    "title": "3  Comparing the means of two groups",
    "section": "3.6 Independent samples t-test",
    "text": "3.6 Independent samples t-test\nTo carry out an independent sample t-test, go to Statistics &gt; Summaries, tables, and tests &gt; Classical tests of hypotheses &gt; t test (mean-comparison test). In the ttest dialog box, choose the Two-sample using groups button, then select birthweight as the Variable name and gender as the Group variable name as shown below. Because we don’t assume equal variances of birthweight for males and females, we tick the Unequal variances box.\n\n\n\n\n\nClick OK or Submit to obtain the following output:\n\nTwo-sample t test with unequal variances\n------------------------------------------------------------------------------\n   Group |     Obs        Mean    Std. Err.   Std. Dev.   [95% Conf. Interval]\n---------+--------------------------------------------------------------------\n  Female |      56    3.587411    .0485051    .3629788    3.490204    3.684617\n    Male |      44    3.421364    .0533097    .3536165    3.313854    3.528873\n---------+--------------------------------------------------------------------\ncombined |     100     3.51435    .0366567    .3665666    3.441615    3.587085\n---------+--------------------------------------------------------------------\n    diff |            .1660471     .072074                .0229333    .3091609\n------------------------------------------------------------------------------\n    diff = mean(Female) - mean(Male)                              t =   2.3038\nHo: diff = 0                     Satterthwaite's degrees of freedom =  93.5438\n\n    Ha: diff &lt; 0                 Ha: diff != 0                 Ha: diff &gt; 0\n Pr(T &lt; t) = 0.9883         Pr(|T| &gt; |t|) = 0.0234          Pr(T &gt; t) = 0.0117\n\n[Command: ttest birthweight, by(gender) unequal]"
  },
  {
    "objectID": "05-comparing-two-means.html#checking-the-assumptions-for-a-paired-t-test",
    "href": "05-comparing-two-means.html#checking-the-assumptions-for-a-paired-t-test",
    "title": "3  Comparing the means of two groups",
    "section": "3.7 Checking the assumptions for a Paired t-test",
    "text": "3.7 Checking the assumptions for a Paired t-test\nBefore performing a paired t-test, you must check that the assumptions for the test have been met. Using the dataset mod05_ankle_bp.xls to show that the difference between the pair of measurements between the sites is normally distributed, we first need to compute a new variable of the differences.\nGo to Data &gt; Create new variable.\nIn the generate – Create a new variable dialog box, assign a name (e.g. difference) to your new variable in the Variable name box. We want to compute difference as sbp_dp − sbp_tp, so we enter this in the Specify a value or an expression box as shown below.\n\n\n\n\n\nClick Submit or OK.\n[Command: generate difference = sbp_dp-sbp_tp]\nCheck that the new variable appears in your Data Editor window.\nCreate a histogram with a normal curve under Graphics &gt; Histogram, as shown previously in Module 2. You might want to vary the minimum bar value and the bar width: for example, Figure 3.2 was defined to have a lower limit of -40 and a bin width of 10. We have also requested a Normal curve be plotted in the Density plots tab.\n\n\n\n\n\n[Command: histogram difference, width(10) start(-40) frequency normal]"
  },
  {
    "objectID": "05-comparing-two-means.html#paired-t-test",
    "href": "05-comparing-two-means.html#paired-t-test",
    "title": "3  Comparing the means of two groups",
    "section": "3.8 Paired t-Test",
    "text": "3.8 Paired t-Test\nTo perform a paired t-test we will use the dataset mod05_ankle_bp.xls. For a paired t-test, data must be organised into two variables (i.e. two columns). Go to Statistics &gt; Summaries, tables, and tests &gt; Classical tests of hypotheses &gt; t test (mean-comparison test) as you had for the Independent samples t-test. In the ttest dialog box, choose the Paired button then Select sbp_dp from the dropdown list under First variable and select sbp_tp from the dropdown list under Second variable. The dialog box will look like:\n\n\n\n\n\nClick Submit or OK. The output will look as follows:\n\nPaired t test\n------------------------------------------------------------------------------\nVariable |     Obs        Mean    Std. Err.   Std. Dev.   [95% Conf. Interval]\n---------+--------------------------------------------------------------------\n  sbp_dp |     107     116.729    3.460296    35.79358    109.8686    123.5893\n  sbp_tp |     107    117.9907    3.431356    35.49422    111.1877    124.7937\n---------+--------------------------------------------------------------------\n    diff |     107   -1.261682    1.311368    13.56489   -3.861596    1.338232\n------------------------------------------------------------------------------\n     mean(diff) = mean(sbp_dp - sbp_tp)                           t =  -0.9621\n Ho: mean(diff) = 0                              degrees of freedom =      106\n\n Ha: mean(diff) &lt; 0           Ha: mean(diff) != 0           Ha: mean(diff) &gt; 0\n Pr(T &lt; t) = 0.1691         Pr(|T| &gt; |t|) = 0.3382          Pr(T &gt; t) = 0.8309\n\n[Command: ttest sbp_dp == sbp_tp]"
  },
  {
    "objectID": "05-comparing-two-means.html#setting-an-observation-to-missing-1",
    "href": "05-comparing-two-means.html#setting-an-observation-to-missing-1",
    "title": "3  Comparing the means of two groups",
    "section": "3.9 Setting an observation to missing",
    "text": "3.9 Setting an observation to missing\nSetting an incorrect observation to missing is straightforward in Stata by using the Data Editor. While RStudio allows browsing a data set as a spreadsheet, it will not let a user replace an observation with a missing value: this should be done using code.\nA missing value in R is denoted NA, and this is consistent for any variable type: continuous, string (i.e. character) and even a factor.\nRecall the weights data used in Module 2. In viewing a boxplot of weight, we saw an obvious outlier of 700.2kg for ID 58:\n\nlibrary(jmv)\n\nsample &lt;- read.csv(\"data/examples/mod02_weight_1000.csv\")\n\nboxplot(sample$weight, xlab=\"Weight (kg)\", main=\"Boxplot of 1000 weights\")\n\n\n\nsubset(sample, weight&gt;200)\n\n\n\n\nidweight\n\n58700\n\n\n\n\nWe previously set this value to 70.2kg using an ifelse() command. Here, let’s create a new, cleaned weight variable, and set the incorrect value to missing:\n\nsample$weight_clean = ifelse(sample$weight==700.2, NA, sample$weight)\n\nOur code will create a new column (called weight_clean) in the sample dataframe. We will test whether weight is equal to 700.2; if this is true, we will assign weight_clean to be NA (i.e. missing), otherwise weight_clean will equal the value of weight.\nLet’s view the data from ID 58, and summarise the cleaned weight variable using descriptives() and a boxplot:\n\nsubset(sample, sample$id==58)\n\n\n\n\nidweightweight_clean\n\n58700\n\n\n\ndescriptives(data=sample, vars=weight_clean)\n\n\n DESCRIPTIVES\n\n Descriptives                           \n ────────────────────────────────────── \n                         weight_clean   \n ────────────────────────────────────── \n   N                              999   \n   Missing                          1   \n   Mean                      69.76406   \n   Median                    69.80000   \n   Standard deviation        5.055188   \n   Minimum                   53.80000   \n   Maximum                   85.80000   \n ────────────────────────────────────── \n\nboxplot(sample$weight_clean, xlab=\"Weight (kg)\", main=\"Boxplot of 999 weights\",\n        sub=\"(Excluding 1 observation of 700.2kg)\")"
  },
  {
    "objectID": "05-comparing-two-means.html#checking-data-for-the-independent-samples-t-test-1",
    "href": "05-comparing-two-means.html#checking-data-for-the-independent-samples-t-test-1",
    "title": "3  Comparing the means of two groups",
    "section": "3.10 Checking data for the independent samples t-test",
    "text": "3.10 Checking data for the independent samples t-test\n\n3.10.1 Producing histograms and boxplots by a second variable\nWe have seen how to create histograms and boxplots separated by a second variable in Module 2 (REF). We will demonstrate using the birthweight data in mod05_birthweight.rds.\n\nlibrary(jmv)\n\nbwt &lt;- readRDS(\"data/examples/mod05_birthweight.rds\")\n\nsummary(bwt)\n\n    gender    birthweight   \n Female:56   Min.   :2.750  \n Male  :44   1st Qu.:3.257  \n             Median :3.450  \n             Mean   :3.514  \n             3rd Qu.:3.772  \n             Max.   :4.250  \n\nsummary(bwt$gender)\n\nFemale   Male \n    56     44 \n\n\nWe can create subsets of the birthweight data, subsetted for males and females separately. Note here that gender is a factor, so we need to select based on the factor labels, not the underlying numeric code.\n\nbwt_m &lt;- subset(bwt, bwt$gender==\"Male\")\nbwt_f &lt;- subset(bwt, bwt$gender==\"Female\")\n\nWe can now create histograms and boxplots for males and females separately, in the usual way, using the par function to set the graphics parameters to display graphs in a 2-by-2 grid:\n\npar(mfrow=c(2,2))\nhist(bwt_m$birthweight, xlim=c(2.5, 4.5), xlab=\"Birthweight (kg)\", main=\"Males\")\nhist(bwt_f$birthweight, xlim=c(2.5, 4.5), xlab=\"Birthweight (kg)\", main=\"Females\")\n\nboxplot(bwt_m$birthweight, ylim=c(2.5, 4.5), ylab=\"Birthweight (kg)\", main=\"Males\")\nboxplot(bwt_f$birthweight, ylim=c(2.5, 4.5), ylab=\"Birthweight (kg)\", main=\"Females\")\n\n\n\npar(mfrow=c(1,1))\n\nlibrary(ggformula)\n\nLoading required package: ggplot2\n\n\n\nAttaching package: 'ggplot2'\n\n\nThe following object is masked from 'package:huxtable':\n\n    theme_grey\n\n\nLoading required package: scales\n\n\n\nAttaching package: 'scales'\n\n\nThe following object is masked from 'package:huxtable':\n\n    number_format\n\n\nLoading required package: ggridges\n\n\n\nNew to ggformula?  Try the tutorials: \n    learnr::run_tutorial(\"introduction\", package = \"ggformula\")\n    learnr::run_tutorial(\"refining\", package = \"ggformula\")\n\ngf_histogram(~ birthweight, data=bwt, bins=10, colour=\"black\") |&gt; \n  gf_theme(theme_light())\n\n\n\ngf_histogram(~ birthweight | gender, data=bwt, bins=10, colour=\"black\") |&gt; \n  gf_theme(theme_light())\n\n\n\ngf_boxplot(birthweight ~ ., data=bwt) |&gt; \n  gf_refine(scale_x_discrete()) |&gt; \n  gf_theme(theme_light())\n\n\n\ngf_boxplot(birthweight ~ . | gender, data=bwt) |&gt; \n  gf_refine(scale_x_discrete()) |&gt; \n  gf_theme(theme_light())\n\n\n\n\nWhen we are done plotting multiple graphs, we can reset the plot window by submitting par(mfrow=c(1,1)).\n\n\n3.10.2 Producing split summary statistics\nThe descriptives function within the jmv function allows summary statistics to be calculated within subgroups using the splitBy argument:\n\ndescriptives(data=bwt, vars=birthweight, splitBy=gender)\n\n\n DESCRIPTIVES\n\n Descriptives                                    \n ─────────────────────────────────────────────── \n                         gender    birthweight   \n ─────────────────────────────────────────────── \n   N                     Female             56   \n                         Male               44   \n   Missing               Female              0   \n                         Male                0   \n   Mean                  Female       3.587411   \n                         Male         3.421364   \n   Median                Female       3.530000   \n                         Male         3.430000   \n   Standard deviation    Female      0.3629788   \n                         Male        0.3536165   \n   Minimum               Female       2.950000   \n                         Male         2.750000   \n   Maximum               Female       4.250000   \n                         Male         4.100000   \n ───────────────────────────────────────────────"
  },
  {
    "objectID": "05-comparing-two-means.html#independent-samples-t-test-2",
    "href": "05-comparing-two-means.html#independent-samples-t-test-2",
    "title": "3  Comparing the means of two groups",
    "section": "3.11 Independent samples t-test",
    "text": "3.11 Independent samples t-test\nWe can use the ttestIS() (t-test, independent samples) function from the jmv package to perform the independent samples t-test. We include the meanDiff=TRUE and ci=TRUE options to obtain the difference in means, with its 95% confidence interval. We can request a Welch’s test (which does not assume equal variances) by the welchs=TRUE option:\n\nttestIS(data=bwt, vars=birthweight, group=gender, meanDiff=TRUE, ci=TRUE, welchs=TRUE)\n\n\n INDEPENDENT SAMPLES T-TEST\n\n Independent Samples T-Test                                                                                                          \n ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n                                 Statistic    df          p            Mean difference    SE difference    Lower         Upper       \n ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n   birthweight    Student's t     2.296556    98.00000    0.0237731          0.1660471       0.07230265    0.02256481    0.3095293   \n                  Welch's t       2.303840    93.54377    0.0234458          0.1660471       0.07207403    0.02293328    0.3091609   \n ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n   Note. Hₐ μ &lt;sub&gt;Female&lt;/sub&gt; ≠ μ &lt;sub&gt;Male&lt;/sub&gt;\n\n\nThere is no built-in function to calculate an independent t-test from summarised data, nor is there a function within jmv. We can use the tsum.test() function within the BSDA package, with the following syntax:\ntsum.test(mean.x=, s.x=, n.x=,\n          mean.y=, s.y=, n.y=,\n          mu=0, alternative=\"two.sided\", var.equal = FALSE)\nHere we specify the mean, standard deviation and sample size for the first group (on the first line) and the second group (on the second line)."
  },
  {
    "objectID": "05-comparing-two-means.html#checking-the-assumptions-for-a-paired-t-test-1",
    "href": "05-comparing-two-means.html#checking-the-assumptions-for-a-paired-t-test-1",
    "title": "3  Comparing the means of two groups",
    "section": "3.12 Checking the assumptions for a Paired t-test",
    "text": "3.12 Checking the assumptions for a Paired t-test\nBefore performing a paired t-test, you must check that the assumptions for the test have been met. Using the dataset mod05_ankle_bp.xls to show that the difference between the pair of measurements between the sites is normally distributed, we first need to compute a new variable of the differences and examine its histogram.\n\nlibrary(readxl)\n\nsbp &lt;- read_excel(\"data/examples/mod05_ankle_bp.xlsx\")\nsbp$diff = sbp$sbp_dp - sbp$sbp_tp\nhist(sbp$diff, xlab=\"Blood pressure (mmHg)\", main=\"Difference in systolic blood pressure\")\n\n\n\n\nWe might want to plot a Normal curve over this distribution, as we did in Module 2:\n\nhist(sbp$diff,\n     xlab=\"Systolic blood pressure (mmHg)\",\n     main=\"Difference in systolic blood pressure\",\n     probability = TRUE)\n\ncurve(dnorm(x,\n            mean=mean(sbp$diff),\n            sd=sd(sbp$diff)),\n      col=\"darkblue\",\n      add=TRUE)\n\n\n\n\nWhile there is a large difference in blood pressure (around 60 mmHg) that warrents further checking, the curve is roughly symmetric with an approximately Normal distribution."
  },
  {
    "objectID": "05-comparing-two-means.html#paired-t-test-1",
    "href": "05-comparing-two-means.html#paired-t-test-1",
    "title": "3  Comparing the means of two groups",
    "section": "3.13 Paired t-Test",
    "text": "3.13 Paired t-Test\nTo perform a paired t-test we will use the dataset mod05_ankle_bp.xls. We can perform a paired t-test using the ttestPS() function within the jmv package, where we defined the paired observations as: `pairs=list(list(i1 = ‘variable1’, i2 = ‘variable2’))\n\nttestPS(data=sbp, pairs=list(list(i1 = 'sbp_dp', i2 = 'sbp_tp')), meanDiff=TRUE, ci=TRUE)\n\n\n PAIRED SAMPLES T-TEST\n\n Paired Samples T-Test                                                                                                                   \n ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n                                      statistic     df          p            Mean difference    SE difference    Lower        Upper      \n ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n   sbp_dp    sbp_tp    Student's t    -0.9621117    106.0000    0.3381832          -1.261682         1.311368    -3.861596    1.338232   \n ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n   Note. Hₐ μ &lt;sub&gt;Measure 1 - Measure 2&lt;/sub&gt; ≠ 0\n\n\nThe syntax of the ttestPS function is a little cumbersome. The t.test function can be used as an alternative:\n\nt.test(sbp$sbp_dp, sbp$sbp_tp, paired=TRUE)\n\n\n    Paired t-test\n\ndata:  sbp$sbp_dp and sbp$sbp_tp\nt = -0.96211, df = 106, p-value = 0.3382\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -3.861596  1.338232\nsample estimates:\nmean difference \n      -1.261682"
  },
  {
    "objectID": "06-proportions.html",
    "href": "06-proportions.html",
    "title": "4  Summary statistics for binary data",
    "section": "",
    "text": "Stata notes"
  },
  {
    "objectID": "06-proportions.html#learning-objectives",
    "href": "06-proportions.html#learning-objectives",
    "title": "4  Summary statistics for binary data",
    "section": "Learning objectives",
    "text": "Learning objectives\nBy the end of this module you will be able to:\n\nCompute and interpret 95% confidence intervals for proportions;\nConduct and interpret a significance test for a one-sample proportion;\nUse statistical software to compute 95% confidence intervals for a difference in proportions, a relative risk and an odds ratio."
  },
  {
    "objectID": "06-proportions.html#optional-readings",
    "href": "06-proportions.html#optional-readings",
    "title": "4  Summary statistics for binary data",
    "section": "Optional readings",
    "text": "Optional readings\nKirkwood and Sterne (2001); Chapter 16 [UNSW Library Link]\nBland (2015); Section 8.6, Section 13.7 [UNSW Library Link]\nAcock (2010); Section 7.5."
  },
  {
    "objectID": "06-proportions.html#introduction",
    "href": "06-proportions.html#introduction",
    "title": "4  Summary statistics for binary data",
    "section": "4.1 Introduction",
    "text": "4.1 Introduction\nIn Modules 4 and 5, we discussed methods used to test hypotheses when the data are continuous. In Modules 6 and 7, we will focus on hypothesis testing for binary categorical data.\nIn health research, we often collect information that can be put into two categories, e.g. male and female, disease present or disease absent etc. Binary categorical variables such as these are summarised using proportions."
  },
  {
    "objectID": "06-proportions.html#calculating-proportions-and-95-confidence-intervals",
    "href": "06-proportions.html#calculating-proportions-and-95-confidence-intervals",
    "title": "4  Summary statistics for binary data",
    "section": "4.2 Calculating proportions and 95% confidence intervals",
    "text": "4.2 Calculating proportions and 95% confidence intervals\n\n4.2.1 Calculating a proportion\nWe need two pieces of information to calculate a proportion: \\(n\\), the number of trials, and \\(k\\), the number of ‘successes’. Note that we use the term ‘success’ to describe the outcome of interest, recognising that a success may be a adverse outcome such as death or disease.\nThe following formula is used to calculate the proportion, \\(p\\):\n\\[ p = k / n \\]\nThe proportion, \\(p\\), is a number that lies between 0 and 1. Proportions and their confidence intervals can easily be converted to percentages by multiplying by 100 once computed.\nAs for all summary statistics, it is useful to compute the precision of the estimate as a 95% confidence interval (CI) to indicate the range of values in which are 95% confident that the true population value lies. In this module, we present two methods for computing a 95% confidence interval around a proportion.\n\n\n4.2.2 Calculating the 95% confidence interval of a proportion (Wald method)\nThe Wald method for calculating the 95% confidence interval is based on assuming that the proportion, \\(p\\), is Normally distributed. This assumption is reasonable if the sample is sufficiently large (for example, if \\(n&gt;30\\)) and if \\(n \\times (1-p)\\) and \\(n \\times p\\) are both larger than 5.\nThe Wald method for calculating a 95% confidence interval is given by:\n\\[\\text{95\\% CI} = p \\pm (1.96 \\times \\text{SE}(p))\\]\nwhere the standard error of a proportion is computed as:\n\\[\\text{SE}(p) = \\sqrt{\\frac{p \\times (1 - p)}{n}}\\]\n\n\n4.2.3 Worked Example 6.1\nIn a cross-sectional study of children living in a rural village, 47 children from a random sample of 215 children were found to have scabies. Here \\(n=215\\) and \\(k=47\\), so the proportion of children with scabies is estimated as:\n\\[ p = \\frac{47}{215} = 0.2186 \\]\nGiven the large sample size and the number of children with the rarer outcome is larger than 5, the Wald method is used to calculate the standard error of the proportion as:\n\\[{\\text{SE}\\left( p \\right) = \\sqrt{\\frac{0.2186 \\times (1 - 0.2186)}{215}}\n}{= 0.02819}\\]\nThen, the 95% confidence interval is estimated as:\n\\[\\text{95\\% CI} = 0.2816 \\pm 1.96 \\times 0.02819\\]\n\\[= 0.1634 \\text{ to } 0.2739\\]\nThe prevalence of scabies among children in the village is 21.9% (95% CI 16.3%, 27.4%). These values tell us that we are 95% confident that the true prevalence of scabies among children in the village is between 16.3% and 27.4%.\n\n\n4.2.4 Calculating the 95% confidence interval of a proportion (Wilson method)\nAnother method to calculate the confidence interval of a proportion is the Wilson (sometimes also called the ‘score’) method. We can use it in situations where it is not appropriate to use the normal approximation to the binomial distribution as described above i.e. if the sample size is small (\\(n &lt; 30\\)) or the number of subjects with the rarer outcome is 5 or fewer. This method much more difficult to implement by hand than the standard confidence interval, and so we will not discuss the hand calculation using the mathematical equation in this course. Instead, we use statistical software to do this (see the Stata or R notes for detail).\nWhen using software, our worked example provides a 95% confidence interval of the prevalence of scabies of 16.9% to 27.9%.\n\n\n4.2.5 Wald vs Wilson methods\nThe Wald method, which assumes that the underlying proportion follows a Normal distribution, is easy to calculate and follows the form of other confidence intervals. The Wilson method, which is difficult to calculate by hand, has nicer mathematical properties. There are also a number of other methods for calculating confidence intervals for proportions, but we do not discuss these in this course.\nA paper by Brown, Cai and DasGupta (Brown, Cai, and DasGupta (2001)) has compared the properties of the Wald and Wilson methods (among others) and concluded that the Wilson method is preferred over the Wald method. Therefore, we recommend the Wilson method be used to calculate 95% confidence intervals for a proportion."
  },
  {
    "objectID": "06-proportions.html#hypothesis-testing-for-one-sample-proportion",
    "href": "06-proportions.html#hypothesis-testing-for-one-sample-proportion",
    "title": "4  Summary statistics for binary data",
    "section": "4.3 Hypothesis testing for one sample proportion",
    "text": "4.3 Hypothesis testing for one sample proportion\nWe can carry out a hypothesis test to compare a sample proportion to a hypothesised proportion. In much the same way as a one sample t-test was used in Module 5 to test a sample mean against a hypothesised mean, we can perform a one-sample test to test a sample proportion against a hypothesised proportion. The significance test will provide a P-value to assess the evidence against the null hypothesis, while the 95% confidence interval will provide the range in which we are 95% confident that the true proportion lies.\nFor example, we can test the following null hypothesis:\nH0: sample proportion is not different from the hypothesised proportion\nMuch like constructing a 95% confidence interval, there are two main options when performing a hypothesis test on a single proportion: the first assumes that the proportion follows a Normal distribution, while the second relaxes this assumption.\n\n4.3.1 z-test for testing one sample proportion\nThe first step in the z-test is to calculate a z-statistic, which is then used to calculate a P-value. The z-statistic is calculated as the difference between the population proportion and the sample proportion divided by the standard error of the population proportion, i.e.\n\\[\nz = \\frac{(p_{sample} - p_{population})}{\\text{SE}(p_{population})}\n\\]\nThis z-statistic is then compared to the standard Normal distribution to calculate the P-value.\n\n\n4.3.2 Worked Example 6.2\nA national census in a country shows that 20% of the population are smokers. A survey of a community within the country that has received a public health anti-smoking intervention shows that 54 of 300 people sampled are smokers (18%). We can calculate a 95% confidence interval around this proportion using the Wilson method, which is calculated as 14.1% to 22.7%.\nThe researchers are interested in whether the proportion of smoking in this community is the same as the population prevalence of smoking of 20%. The null hypothesis can be written as: H0: the proportion of smokers in the community is 20% (the same as in the national census).\nWe can test this by calculating a z-statistic:\n\\[\n\\begin{aligned}\nz &= \\frac{(0.18 - 0.20)}{\\sqrt{\\frac{0.20 × (1 - 0.20)}{300}}} \\\\\n&= -0.87\n\\end{aligned}\n\\]\nThe P-value for the test above can be obtained from a Normal distribution table as \\(P = 2 × 0.192 = 0.38\\) (using Table A2.1 in the Appendix), or using the hand-calculator in Stata. This indicates that there is insufficient evidence to conclude that there is a difference between the proportion of smokers in the community and the country. This is consistent with our 95% confidence interval which crosses the null value of 20%.\n\n\n\n\n\n\n\n4.3.3 Binomial test for testing one sample proportion\nWe can use the binomial distribution to obtain an exact P-value for testing a single proportion. Historically, this was a time consuming process with much hand calculation. These days, statistical software performs the calculations quickly and efficiently, and is the preferred method.\n\n\n4.3.4 Worked example 6.3\nThe file mod06_smoking_status.csv contains the data for this example. In the data file, smokers are coded as 1 and non-smokers are coded as 0.\nIn Stata, we can use the prtest command to perform a z-test, or the bitest command to perform the exact binomial test. In R, we can use the prop.test function to perform a z-test, or the binom.test function to perform the exact binomial test.\nThe z-test provides a two-sided P-value of 0.39, while the binomial test gives a two-sided P-value of 0.43. Both tests provide little evidence against the hypothesis that the prevalence of smoking in the community is 20%."
  },
  {
    "objectID": "06-proportions.html#contingency-tables",
    "href": "06-proportions.html#contingency-tables",
    "title": "4  Summary statistics for binary data",
    "section": "4.4 Contingency tables",
    "text": "4.4 Contingency tables\nAs introduced in PHCM9794: Foundations of Epidemiology, 2-by-2 contingency tables can be used to examine associations between two binary variables, most commonly an exposure and an outcome. The traditional form of a 2-by-2 contigency table is given in Table 4.1.\n\n\n\n\nTable 4.1:  Traditional format for presenting a contingency table \n\n Outcome presentOutcome absentTotal\n\nExposure presentaba+b\n\nExposure absentcdc+d\n\nTotala+cb+dN\n\n\n\n\n\nNote for Stata users: It is important to note that Stata presents the exposure or intervention (present, absent) in the columns and the outcome or disease (present, absent) in the rows (e.g. Table 4.2). This is opposite to the way most epidemiological tables are presented, with exposure in rows and outcome in columns. Care must be taken when reading 2-by-2 tables generated from Stata.\n\n\n\n\nTable 4.2:  Stata format for presenting a contingency table \n\n Exposure presentExposure absentTotal\n\nOutcome presentaca+c\n\nOutcome absentbdb+d\n\nTotala+bc+dN\n\n\n\n\n\nWhen using a statistics program, it is recommended that the outcome and exposure variables are coded by assigning ‘absent’ as 0 and ‘present’ as 1, for example ‘No’ = 0 and ‘Yes’ = 1. This is needed for some of the commands to work (e.g. the epidemiology table commands). This coding ensures that measures of association, such as the odds ratio or relative risk, are computed correctly by Stata. While R does not require this coding to be followed, it is good practice nonetheless."
  },
  {
    "objectID": "06-proportions.html#a-brief-summary-of-epidemiological-study-types",
    "href": "06-proportions.html#a-brief-summary-of-epidemiological-study-types",
    "title": "4  Summary statistics for binary data",
    "section": "4.5 A brief summary of epidemiological study types",
    "text": "4.5 A brief summary of epidemiological study types\nIn this section, we wil present a very brief summary of three study types commonly used in population health research. This topic is covered in much more detail in PHCM9794: Foundations of Epidemiology, and more detail can be found in Chapter 4 of Essential Epidemiology (3rd or 4th edition) Webb, Bain and Page (Webb, Bain, and Page (2016)).\n\n4.5.1 Randomised controlled trial\nA randomised controlled trial addresses the research question: what is the effect of an intervention on an outcome. In the simplest form of a randomised controlled trial, a group of participants is randomly allocated to a group that receives the treatment of interest or to a control group that does not receive the treatment of interest. Participants are followed up over time, and the outcome is measured at the conclusion of the study.\n\n\n\n\n\nThe design of a randomised controlled trial [Figure 4.1, Essential Epidemiology]\n\n\n\n\n\n\n4.5.2 Cohort study\nA cohort study is an observational study that addresses the research question: what is the effect of an exposure on an outcome. This research question is similar to that studied in a randomised controlled trial, but the exposure is defined by the participants’ circumstances, and not manipulated by the researchers. In a cohort study, participants without the outcome of interest are enrolled, followed over time, and information on their exposure to a factor is measured (either at baseline or over time). At the conclusion of the study, information on the outcome is measured to identify new (incident) cases.\n\n\n\n\n\nThe design of a cohort study [Figure 4.2, Essential Epidemiology]\n\n\n\n\n\n\n4.5.3 Case control study\nWhile the randomised controlled trial and cohort study begin with a population without the outcome, a case-control study begins by assembling a group with the outcome of interest (cases), and a group without the outcome of interest (controls). The researchers then ask the cases and controls about their previous exposures.\n\n\n\n\n\nThe design of a case-control trial [Figure 4.3, Essential Epidemiology]\n\n\n\n\n\n\n4.5.4 Cross-sectional study\nIn a cross-sectional study, the exposure and the outcome are measured at the same time. While this results in a study that is relatively quick to conduct, it does not allow for any temporal relationships to be assessed.\n\n\n\n\n\nThe design of a cross-sectional study [Figure 4.4, Essential Epidemiology]"
  },
  {
    "objectID": "06-proportions.html#measures-of-effect-for-epidemiological-studies",
    "href": "06-proportions.html#measures-of-effect-for-epidemiological-studies",
    "title": "4  Summary statistics for binary data",
    "section": "4.6 Measures of effect for epidemiological studies",
    "text": "4.6 Measures of effect for epidemiological studies\nWe can calculate a relative measure of association between an exposure and an outcome as either a relative risk or odds ratio. The relative risk is a direct comparison of the risk in the exposed group with the risk in the non-exposed group, and can only be calculated for a cohort study (including a randomised controlled trial) or a cross-sectional study (where it is also called a prevalence ratio).\nFor cohort studies, randomised controlled trials and cross-section studies, we can calculate an absolute measure of association between an exposure and an outcome as a difference in proportions (also known as an attributable risk).\nFor case-control studies, as we sample participants based on their outcome, we can not estimate the risk of the outcome. Hence, calculating a relative risk or risk difference is inappropriate. Instead of calculating risks in a case-control study, we instead calculate odds, where the odds of an event are calculated as the number with the event divided by the number without the event.\n\n\n\n\nTable 4.3:  Contingency table for a case-control study \n\n CasesControlsTotal\n\nExposure presentaba+b\n\nExposure absentcdc+d\n\nTotala+cb+dN\n\n\n\n\n\nIn the example in Table Table 4.3, we can calculate the odds of being exposed in the cases as \\(a \\div c\\). Similarly, we can calculate the odds of being exposed in the controls as \\(b \\div d\\). We can the calculate the odds ratio as:\n\\[\n\\begin{aligned}\n\\text{Odds ratio} &= (a \\div c) \\div (b \\div d) \\\\\n&= \\frac{a \\times d}{b \\times c} \\\\\n&= \\frac{ad}{bc}\n\\end{aligned}\n\\]\nNote that some authors say we should think of the odds ratio being based on the odds of being a case in the exposed group compared to the odds of being a case in the unexposed group. Here, the exposed group comprises cells “a” and “b”, so the odds of being a case in the exposed group is (a/b). Similarly, for the unexposed group, the odds of being exposed is (c/d). So our odds ratio becomes (a/b) / (c/d). If we rearrange this, we get the same odds ratio as above: (ad)/(bc).\nThe interpretation of an odds ratio is discussed in detail in PHCM9794: Foundations of Epidemiology, and an excerpt is presented here: The meaning of the calculated odds ratio as a measure of association between exposure and outcome is the same as for the rate ratio (relative risk) where:\n\nAn odds ratio &gt;1 indicates that exposure is positively associated with disease (i.e. the exposure may be a cause of disease);\nAn odds ratio &lt; 1 indicates that exposure is negatively associated with disease (i.e. the exposure may be protective against disease); and\nAn odds ratio = 1 indicates no association between the exposure and the outcome.\n\nIn some situations, related to how well controls are recruited into this study, the odds ratio is a close approximation of the relative risk. Therefore, you may see in some published papers of case control studies the OR interpreted as you would interpret a RR. This should be avoided in this course.\nMore information about the problems of interpreting odds-ratios as relative risks has been presented by Deeks (1998) and Schmidt and Kohlmann (2008).\n\n4.6.1 Worked Example 6.4\nA randomised controlled trial was conducted among a group of patients to estimate the side effects of a drug. Fifty patients were randomly allocated to receive the active drug and 50 patients were allocated to receive a placebo drug. The outcome measured was the experience of nausea. The data is given in the files mod06_nausea.dta and mod06_nausea.rds.\nA summary table can be constructed as in Table 4.4.\n\n\n\n\nTable 4.4:  Nausea status by drug exposure \n\n NauseaNo nauseaTotal\n\nActive drug153550\n\nPlacebo44650\n\nTotal1981100\n\n\n\n\n\nWe can use Stata or R to calculate the relative risk (RR=3.75) and its 95% confidence interval (1.34 to 10.51). This tells us that nausea is 3.75 times more likely to occur in the active drug group compared with the placebo group. Because this is a randomised controlled trial, the relative risk would be an appropriate measure of association.\nWe can confirm the estimated relative risk:\n\\[\n\\begin{aligned}\n\\text{RR} &= \\frac{a / (a+b)}{c / (c+d)} \\\\\n  &= \\frac{15 / (15+35)}{4 / (4+46)} \\\\\n  &= \\frac{0.3}{0.08} \\\\\n  &= 3.75\n\\end{aligned}\n\\]\n\n\n4.6.2 Worked Example 6.5\nA case-control study investigated the association between human papillomavirus and oropharyngeal cancer (D'Souza, et al. NEJM 2007), and the results appear in Table 4.5.\n\n\n\n\nTable 4.5:  Association between human papillomavirus and oropharyngeal cancer \n\n CasesControlsTotal\n\n(Oropharyngeal cancer)(No oropharyngeal cancer)\n\nHPV Positive571471\n\nHPV Negative43186229\n\nTotal100200300\n\n\n\n\n\nThe odds ratio is the odds of being HPV positive in cases (those with oropharyngeal cancer) compared to the odds of being HPV positive in the controls (those without oropharyngeal cancer):\n\\[\n\\begin{aligned}\n\\text{OR} &= \\frac{a / c}{b /d} \\\\\n  &= \\frac{57 / 43}{14 / 186} \\\\\n  &= 17.6\n\\end{aligned}\n\\]\nWe can use Stata or R to estimate the odds ratio and its 95% confidence interval. We should use the Cornfield option in Stata to provide a better estimate of the 95% confidence interval. It appears that the jmv package in R does not use the Cornfield approximation to estimate the 95% confidence interval, but uses the Woolf method.\nThe odds ratio is estimated as 17.6, and its 95% confidence interval is estimated 9.0 to 34.3 (Cornfield, using Stata) or 9.0 to 34.5 (Woolf, using R).\nThe interpretation of the confidence intervals for both the relative risk and the odds ratio is the same as for the confidence intervals around other summary measures in that it shows the region in which we are 95% confident that the true population estimate lies."
  },
  {
    "objectID": "06-proportions.html#confidence-intervals-for-proportions",
    "href": "06-proportions.html#confidence-intervals-for-proportions",
    "title": "4  Summary statistics for binary data",
    "section": "4.7 95% confidence intervals for proportions",
    "text": "4.7 95% confidence intervals for proportions\nTo compute the 95% confidence interval for proportions, go to Statistics &gt; Summaries, tables, and tests &gt; Summary and descriptive statistics &gt;Proportion CI calculator. In the cii dialog box as shown below, key in 215 as the Sample size and 47 as the number of Successes. Choose Wald for the normal approximation to binomial distribution CI [Command: cii 215 47, wald] or Wilson for Wilson CI [Command: cii 215 47, wilson].\n\n\n\n\n\n\nStata Output: 95% confidence interval (Wald method)\n\n. cii proportions 215 47, wald\n\n                                                         -- Binomial Wald ---\n    Variable |        Obs  Proportion    Std. Err.       [95% Conf. Interval]\n-------------+---------------------------------------------------------------\n             |        215    .2186047    .0281868        .1633595    .2738498\n\n\n\nStata Output: 95% confidence interval (Wilson method)\n\n. cii proportions 215 47, wilson\n\n                                                         ------ Wilson ------\n    Variable |        Obs  Proportion    Std. Err.       [95% Conf. Interval]\n-------------+---------------------------------------------------------------\n             |        215    .2186047    .0281868        .1685637    .2785246"
  },
  {
    "objectID": "06-proportions.html#significance-test-for-single-proportion",
    "href": "06-proportions.html#significance-test-for-single-proportion",
    "title": "4  Summary statistics for binary data",
    "section": "4.8 Significance test for single proportion",
    "text": "4.8 Significance test for single proportion\nTo perform a binomial test using the data from mod06_smoking_status.dta, it is a good idea to check that the variable is dichotomous and numerically coded in 0 and 1 by using the codebook command. [Command: codebook smoking_status]\n\n. codebook smoking_status \n\n--------------------------------------------------------------------------------\nsmoking_status                                                    Smoking status\n--------------------------------------------------------------------------------\n\n                  type:  numeric (double)\n                 label:  smoking_status\n\n                 range:  [0,1]                        units:  1\n         unique values:  2                        missing .:  0/300\n\n            tabulation:  Freq.   Numeric  Label\n                           246         0  Non-smokers\n                            54         1  Smokers\n\nAfter checking the data, perform the binomial probability test by going to Statistics &gt; Summaries, tables, and tests &gt; Classical tests of hypotheses &gt; Binomial probability test. Select Smoking_status as the Binomial variable. The probability we want to test against is entered in the Probability of success box. To test that the sample proportion (0.18) is different from the population proportion of 0.2, we enter 0.2 as the Probability of success.\n\n\n\n\n\nClick OK or Submit to obtain the following output:\n\n. bitest smoking_status == 0.2\n\n    Variable |        N   Observed k   Expected k   Assumed p   Observed p\n-------------+------------------------------------------------------------\nsmoking_st~s |      300         54           60       0.20000      0.18000\n\n  Pr(k &gt;= 54)            = 0.825531  (one-sided test)\n  Pr(k &lt;= 54)            = 0.215202  (one-sided test)\n  Pr(k &lt;= 54 or k &gt;= 66) = 0.427280  (two-sided test)\n\nA similar process can be used to conduct a z-test for a single proportion, by choosing Statistics &gt; Summaries, tables, and tests &gt; Classical tests of hypotheses &gt; Proportion test:\n\n\n\n\n\n\n. prtest smoking_status == 0.2\n\nOne-sample test of proportion                   Number of obs      =       300\n\n------------------------------------------------------------------------------\n    Variable |       Mean   Std. Err.                     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\nsmoking_st~s |        .18   .0221811                      .1365259    .2234741\n------------------------------------------------------------------------------\n    p = proportion(smoking_st~s)                                  z =  -0.8660\nHo: p = 0.2\n\n     Ha: p &lt; 0.2                 Ha: p != 0.2                   Ha: p &gt; 0.2\n Pr(Z &lt; z) = 0.1932         Pr(|Z| &gt; |z|) = 0.3865          Pr(Z &gt; z) = 0.8068\n\nIf you have only the aggregate data then the binomial test can be carried out using the immediate command. For that you will need to go through perform the binomial probability test by going to Statistics &gt; Summaries, tables, and tests &gt; Classical tests of hypotheses &gt; Binomial probability test calculator. Enter the numbers in the appropriate fields as shown below.\n\n\n\n\n\n[Command: bitesti 300 54 0.2]"
  },
  {
    "objectID": "06-proportions.html#computing-a-relative-risk-and-its-95-confidence-interval",
    "href": "06-proportions.html#computing-a-relative-risk-and-its-95-confidence-interval",
    "title": "4  Summary statistics for binary data",
    "section": "4.9 Computing a relative risk and its 95% confidence interval",
    "text": "4.9 Computing a relative risk and its 95% confidence interval\nUsing the data file from Worked Example 6.4, to obtain relative risk and its 95% CI, go to Statistics &gt; Epidemiology and related &gt; Tables for epidemiologists &gt; Cohort study risk ratio etc…\n\n\n\n\n\nIn the cs – cohort studies dialog box, select the variable side_effect in the Case variable box, and the variable group in the Exposed variable box.\n\n\n\n\n\nClick OK or Submit to obtain the following output:\n\n                 | Group                  |\n                 |   Exposed   Unexposed  |      Total\n-----------------+------------------------+------------\n           Cases |        15           4  |         19\n        Noncases |        35          46  |         81\n-----------------+------------------------+------------\n           Total |        50          50  |        100\n                 |                        |\n            Risk |        .3         .08  |        .19\n                 |                        |\n                 |      Point estimate    |    [95% Conf. Interval]\n                 |------------------------+------------------------\n Risk difference |              .22       |    .0723899    .3676101 \n      Risk ratio |             3.75       |     1.33754     10.5137 \n Attr. frac. ex. |         .7333333       |    .2523589     .904886 \n Attr. frac. pop |         .5789474       |\n                 +-------------------------------------------------\n                               chi2(1) =     7.86  Pr&gt;chi2 = 0.0050\n\n[Command: cs side_effect group]\nIf you only have the cross-tabulated data (i.e. aggregated), you can go to Statistics &gt; Epidemiology and related &gt; Tables for epidemiologists &gt; Cohort study risk ratio etc. calculator. In the csi dialog box, key in the relevant numbers from the cross-tabulated data (similarly to the cci dialog box above). Click OK or Submit to obtain identical output.\n\n\n\n\n\n[Command: csi 15 4 35 46]"
  },
  {
    "objectID": "06-proportions.html#computing-an-odds-ratio-and-its-95ci",
    "href": "06-proportions.html#computing-an-odds-ratio-and-its-95ci",
    "title": "4  Summary statistics for binary data",
    "section": "4.10 Computing an odds ratio and its 95%CI",
    "text": "4.10 Computing an odds ratio and its 95%CI\nTo obtain an odds ratio and its 95% CI, go to Statistics &gt; Epidemiology and related &gt; Tables for epidemiologists &gt; Case-control odds ratio. The cc dialog box is completed as for the cs dialog box.\nTo obtain the Cornfield confidence interval, click on the Options tab and select the Cornfield approximation radio button.\n\n\n\n\n\n\n                                                         Proportion\n                 |   Exposed   Unexposed  |      Total      exposed\n-----------------+------------------------+------------------------\n           Cases |        57          43  |        100       0.5700\n        Controls |        14         186  |        200       0.0700\n-----------------+------------------------+------------------------\n           Total |        71         229  |        300       0.2367\n                 |                        |\n                 |      Point estimate    |    [95% Conf. Interval]\n                 |------------------------+------------------------\n      Odds ratio |          17.6113       |    9.043258    34.25468 (Cornfield)\n Attr. frac. ex. |         .9432183       |    .8894204    .9708069 (Cornfield)\n Attr. frac. pop |         .5376344       |\n                 +-------------------------------------------------\n                               chi2(1) =    92.26  Pr&gt;chi2 = 0.0000\n\nIf you only have the cross-tabulated data (i.e. aggregated), you can go to Statistics &gt; Epidemiology and related &gt; Tables for epidemiologists &gt; Case-control odds ratio calculator. In the cci dialog box, enter the numbers from the cross-tabulated data and select the Cornfield approximation radio button. For example, Worked example 6.5 would be entered as:"
  },
  {
    "objectID": "06-proportions.html#confidence-intervals-for-proportions-1",
    "href": "06-proportions.html#confidence-intervals-for-proportions-1",
    "title": "4  Summary statistics for binary data",
    "section": "4.11 95% confidence intervals for proportions",
    "text": "4.11 95% confidence intervals for proportions\nWe can use the BinomCI(x=, n=, method=) function within the DescTools package to compute 95% confidence intervals for proportions. Here we specify x: the number of successes, n: the sample size, and optionally, the method (which defaults to Wilson’s method).\n\nlibrary(DescTools)\n\nBinomCI(x=47, n=215, method='wald')\n\n           est    lwr.ci    upr.ci\n[1,] 0.2186047 0.1633595 0.2738498\n\nBinomCI(x=47, n=215, method='wilson')\n\n           est    lwr.ci    upr.ci\n[1,] 0.2186047 0.1685637 0.2785246"
  },
  {
    "objectID": "06-proportions.html#significance-test-for-single-proportion-1",
    "href": "06-proportions.html#significance-test-for-single-proportion-1",
    "title": "4  Summary statistics for binary data",
    "section": "4.12 Significance test for single proportion",
    "text": "4.12 Significance test for single proportion\nWe can use the binom.test function to perform a significance test for a single proportion: binom.test(x=, n=, p=). Here we specify x: the number of successes, n: the sample size, and p: the hypothesised proportion (which defaults to 0.5 if nothing is entered).\n\nbinom.test(x=54, n=300, p=0.2)\n\n\n    Exact binomial test\n\ndata:  54 and 300\nnumber of successes = 54, number of trials = 300, p-value = 0.4273\nalternative hypothesis: true probability of success is not equal to 0.2\n95 percent confidence interval:\n 0.1382104 0.2282394\nsample estimates:\nprobability of success \n                  0.18 \n\n\nNote that the binom.test function also produces a 95% confidence interval around the estimated proportion. This confidence interval is based on the inferior Wald method: the confidence interval derived from the Wilson method is preferred.\nWe can also conduct a z-test for a single proportion:\n\nprop.test(x=54, n=300, p=0.2, correct=FALSE)\n\n\n    1-sample proportions test without continuity correction\n\ndata:  54 out of 300, null probability 0.2\nX-squared = 0.75, df = 1, p-value = 0.3865\nalternative hypothesis: true p is not equal to 0.2\n95 percent confidence interval:\n 0.1406583 0.2274332\nsample estimates:\n   p \n0.18"
  },
  {
    "objectID": "06-proportions.html#computing-a-relative-risk-and-its-95-confidence-interval-1",
    "href": "06-proportions.html#computing-a-relative-risk-and-its-95-confidence-interval-1",
    "title": "4  Summary statistics for binary data",
    "section": "4.13 Computing a relative risk and its 95% confidence interval",
    "text": "4.13 Computing a relative risk and its 95% confidence interval\nWe will use Worked Example 6.4 to demonstrate calculating a relative risk and its 95% CI:\n\nlibrary(jmv)\n\ndrug &lt;- readRDS(\"data/examples/mod06_nausea.rds\")\n\nsummary(drug)\n\n     group       side_effect\n Placebo:50   No nausea:81  \n Active :50   Nausea   :19  \n\n\nBy using the head() function to view the first six lines of data, we see that both group and side_effect have been entered as factors. Notice the order in which the factor levels are presented: group has the Placebo level defined as the first level, and the Active level defined as the second; side_effect has No nausea defined as the first level, and the Nausea level defined as the second.\nWe will use jmv to calculate relative risks, odds ratios and risk differences. To calculate these estimates correctly, we must define the positive exposure and positive outcome to be the first level of a factor. When defining an exposure for example, we should define the active treatment or the positive exposure as the first category. When defining an outcome, we should define the category of interest (e.g. disease, or side effect) as the first category.\nIn this example, we will define Active as the first level in the group factor, and Nausea to be the first level of the side_effect factor.\nWe can do this using the relevel() function, which re-orders the levels of a factor so that the level specified is defined as the first level, and the others are moved down:\n\n# Define \"Active\" as the first level of group:\ndrug$group &lt;- relevel(drug$group, ref=\"Active\")\n\n\n# Define \"Nausea\" as the first level of side_effect:\ndrug$side_effect &lt;- relevel(drug$side_effect, ref=\"Nausea\")\n\nUpon re-leveling the factors, we can check that the levels of interest have been defined as the first levels:\n\nsummary(drug)\n\n     group       side_effect\n Active :50   Nausea   :19  \n Placebo:50   No nausea:81  \n\n\nTo construct the 2-by-2 table and calculate a relative risk, we use the contTables() function in jmv. We request the row-percents using pcRow = TRUE and the relative risk and confidence interval using relRisk = TRUE:\n\ncontTables(data=drug, \n           rows=group, cols=side_effect, \n           pcRow=TRUE, relRisk = TRUE)\n\n\n CONTINGENCY TABLES\n\n Contingency Tables                                                 \n ────────────────────────────────────────────────────────────────── \n   group                      Nausea       No nausea    Total       \n ────────────────────────────────────────────────────────────────── \n   Active     Observed               15           35           50   \n              % within row     30.00000     70.00000    100.00000   \n                                                                    \n   Placebo    Observed                4           46           50   \n              % within row      8.00000     92.00000    100.00000   \n                                                                    \n   Total      Observed               19           81          100   \n              % within row     19.00000     81.00000    100.00000   \n ────────────────────────────────────────────────────────────────── \n\n\n χ² Tests                              \n ───────────────────────────────────── \n         Value       df    p           \n ───────────────────────────────────── \n   χ²    7.862248     1    0.0050478   \n   N          100                      \n ───────────────────────────────────── \n\n\n Comparative Measures                                    \n ─────────────────────────────────────────────────────── \n                    Value         Lower       Upper      \n ─────────────────────────────────────────────────────── \n   Relative risk    3.750000 ᵃ    1.337540    10.51370   \n ─────────────────────────────────────────────────────── \n   ᵃ Rows compared\n\n\nIf you only have the cross-tabulated data (i.e. aggregated), you will need to enter your data into a new data frame. For example, to recreate the above analyses, we can re-write the 2-by-2 table as follows:\n\n\n\nGroup\nside_effect\nNumber\n\n\n\n\nActive\nNausea\n15\n\n\nActive\nNo nausea\n35\n\n\nPlacebo\nNausea\n4\n\n\nPlacebo\nNo nausea\n46\n\n\n\nWe can enter these data in a dataframe, comprising three vectors, as follows:\n\ndrug_aggregated &lt;- data.frame(\n  group = c(\"Active\", \"Active\", \"Placebo\", \"Placebo\"),\n  side_effect = c(\"Nausea\", \"No nausea\", \"Nausea\", \"No nausea\"),\n  n = c(15, 35, 4, 46)\n)\n\nWe need to define group and side_effect as factors. Here we must define the levels in the order we want the categories to appear in the table. Note that as group and side_effect are entered as text variables, we can omit labels command when defining the factors, and the factor will be labelled using the text entry:\n\ndrug_aggregated$group &lt;- factor(drug_aggregated$group, \n                                levels=c(\"Active\", \"Placebo\"))\n\ndrug_aggregated$side_effect &lt;- factor(drug_aggregated$side_effect, \n                                      levels=c(\"Nausea\", \"No nausea\"))\n\nWe can calculate the relative risk using the summarised data in the same was done previously. However, we need to include the number of observations in each cell using the counts command:\n\ncontTables(data=drug_aggregated,\n           rows=group, cols=side_effect, count=n,\n           pcRow=TRUE, relRisk = TRUE)\n\n\n CONTINGENCY TABLES\n\n Contingency Tables                                                 \n ────────────────────────────────────────────────────────────────── \n   group                      Nausea       No nausea    Total       \n ────────────────────────────────────────────────────────────────── \n   Active     Observed               15           35           50   \n              % within row     30.00000     70.00000    100.00000   \n                                                                    \n   Placebo    Observed                4           46           50   \n              % within row      8.00000     92.00000    100.00000   \n                                                                    \n   Total      Observed               19           81          100   \n              % within row     19.00000     81.00000    100.00000   \n ────────────────────────────────────────────────────────────────── \n\n\n χ² Tests                              \n ───────────────────────────────────── \n         Value       df    p           \n ───────────────────────────────────── \n   χ²    7.862248     1    0.0050478   \n   N          100                      \n ───────────────────────────────────── \n\n\n Comparative Measures                                    \n ─────────────────────────────────────────────────────── \n                    Value         Lower       Upper      \n ─────────────────────────────────────────────────────── \n   Relative risk    3.750000 ᵃ    1.337540    10.51370   \n ─────────────────────────────────────────────────────── \n   ᵃ Rows compared"
  },
  {
    "objectID": "06-proportions.html#computing-a-difference-in-proportions-and-its-95-confidence-interval",
    "href": "06-proportions.html#computing-a-difference-in-proportions-and-its-95-confidence-interval",
    "title": "4  Summary statistics for binary data",
    "section": "4.14 Computing a difference in proportions and its 95% confidence interval",
    "text": "4.14 Computing a difference in proportions and its 95% confidence interval\nWe can use the contTables function to obtain a difference in proportions and its 95% CI, by specifying diffProp=TRUE:\n\ncontTables(data=drug, \n           rows=group, cols=side_effect, \n           pcRow=TRUE, diffProp=TRUE)\n\n\n CONTINGENCY TABLES\n\n Contingency Tables                                                 \n ────────────────────────────────────────────────────────────────── \n   group                      Nausea       No nausea    Total       \n ────────────────────────────────────────────────────────────────── \n   Active     Observed               15           35           50   \n              % within row     30.00000     70.00000    100.00000   \n                                                                    \n   Placebo    Observed                4           46           50   \n              % within row      8.00000     92.00000    100.00000   \n                                                                    \n   Total      Observed               19           81          100   \n              % within row     19.00000     81.00000    100.00000   \n ────────────────────────────────────────────────────────────────── \n\n\n χ² Tests                              \n ───────────────────────────────────── \n         Value       df    p           \n ───────────────────────────────────── \n   χ²    7.862248     1    0.0050478   \n   N          100                      \n ───────────────────────────────────── \n\n\n Comparative Measures                                                      \n ───────────────────────────────────────────────────────────────────────── \n                                  Value          Lower         Upper       \n ───────────────────────────────────────────────────────────────────────── \n   Difference in 2 proportions    0.2200000 ᵃ    0.07238986    0.3676101   \n ───────────────────────────────────────────────────────────────────────── \n   ᵃ Rows compared"
  },
  {
    "objectID": "06-proportions.html#computing-an-odds-ratio-and-its-95-confidence-interval",
    "href": "06-proportions.html#computing-an-odds-ratio-and-its-95-confidence-interval",
    "title": "4  Summary statistics for binary data",
    "section": "4.15 Computing an odds ratio and its 95% confidence interval",
    "text": "4.15 Computing an odds ratio and its 95% confidence interval\nWe can use the contTables function to obtain an odds ratio and its 95% CI, by specifying odds=TRUE. Here we will use the summarised HPV data from Module 6.\n\nhpv &lt;- data.frame(\n  hpv = c(\"HPV +\", \"HPV +\", \"HPV -\", \"HPV -\"),\n  cancer = c(\"Case\", \"Control\", \"Case\", \"Control\"),\n  n = c(57, 14, 43, 186)\n)\n\nhpv$cancer &lt;- factor(hpv$cancer, levels=c(\"Case\", \"Control\"))\nhpv$hpv &lt;- factor(hpv$hpv, levels=c(\"HPV +\", \"HPV -\"))\n\ncontTables(data=hpv, \n           rows=hpv, cols=cancer, count=n,\n           odds = TRUE)\n\n\n CONTINGENCY TABLES\n\n Contingency Tables                    \n ───────────────────────────────────── \n   hpv      Case    Control    Total   \n ───────────────────────────────────── \n   HPV +      57         14       71   \n   HPV -      43        186      229   \n   Total     100        200      300   \n ───────────────────────────────────── \n\n\n χ² Tests                               \n ────────────────────────────────────── \n         Value       df    p            \n ────────────────────────────────────── \n   χ²    92.25660     1    &lt; .0000001   \n   N          300                       \n ────────────────────────────────────── \n\n\n Comparative Measures                               \n ────────────────────────────────────────────────── \n                 Value       Lower       Upper      \n ────────────────────────────────────────────────── \n   Odds ratio    17.61130    8.992580    34.49041   \n ────────────────────────────────────────────────── \n\n\nNote that 95% confidence intervals for the odds ratio based on jmv differ from those calculated by Stata. It appears that jmv uses the Woolf method to calculate confidence intervals, but this is not documented."
  },
  {
    "objectID": "07-testing-proportions.html",
    "href": "07-testing-proportions.html",
    "title": "5  Hypothesis testing for categorical data",
    "section": "",
    "text": "Stata notes"
  },
  {
    "objectID": "07-testing-proportions.html#learning-objectives",
    "href": "07-testing-proportions.html#learning-objectives",
    "title": "5  Hypothesis testing for categorical data",
    "section": "Learning objectives",
    "text": "Learning objectives\nBy the end of this module you will be able to:\n\nUse and interpret the appropriate test for testing associations between categorical data;\nConduct and interpret an appropriate test for independent proportions;\nConduct and interpret a test for paired proportions;"
  },
  {
    "objectID": "07-testing-proportions.html#optional-readings",
    "href": "07-testing-proportions.html#optional-readings",
    "title": "5  Hypothesis testing for categorical data",
    "section": "Optional readings",
    "text": "Optional readings\nKirkwood and Sterne (2001); Chapter 17. [UNSW Library Link]\nBland (2015); Chapter 13. [UNSW Library Link]\nAcock (2010); Section 7.6."
  },
  {
    "objectID": "07-testing-proportions.html#introduction",
    "href": "07-testing-proportions.html#introduction",
    "title": "5  Hypothesis testing for categorical data",
    "section": "5.1 Introduction",
    "text": "5.1 Introduction\nIn Module 6, we estimated the 95% confidence intervals of proportions and measures of association for categorical data and conducted a significance test comparing a sample proportion to a known value.\nWhen both the outcome variable and the exposure variable are categorical, a chi-squared test can be used as a formal statistical test to assess whether the exposure and outcome are related. The P-value obtained from a chi-squared test gives the probability of obtaining the observed association (or more extreme) if there is in fact no association between the exposure and outcome.\nIn this Module, we also include tests for a difference in proportion for paired data.\n\n5.1.1 Worked Example\nWe are using the randomised controlled trial as given in Worked Example 6.4 on the nauseating side effect of a drug.\nThe research question is whether the active drug resulted in a different rate of nausea than the placebo drug. This is equivalent to testing whether there is an association between nausea and type of drug received (active or placebo). Thus, we will test the null hypothesis that the experience of nausea and the treatment are not related to one another. The null hypothesis is:\n\nH0: The proportion with nausea in the active drug group is the same as the proportion with nausea in the placebo drug group.\n\nThe alternative hypothesis can be stated as:\n\nHa: The proportion with nausea in the active drug group is different to the proportion with nausea in the placebo drug group."
  },
  {
    "objectID": "07-testing-proportions.html#chi-squared-test-for-independent-proportions",
    "href": "07-testing-proportions.html#chi-squared-test-for-independent-proportions",
    "title": "5  Hypothesis testing for categorical data",
    "section": "5.2 Chi-squared test for independent proportions",
    "text": "5.2 Chi-squared test for independent proportions\nA chi-squared test is used to test the null hypothesis that of no association between two categorical variables. First a contingency table is drawn up and then we estimate the counts of each cell (i.e. a, b, c and d) that would be expected if the null hypothesis was true. The row and column totals are used to calculate expected counts in each cell of the contingency table as follows:\nExpected count = (Row count × Column count) / Total count\nStatistical software will do this for us, as described in the Stata or R sections in this Module.\nA chi-squared value is then calculated to compare the expected counts (E) in each cell with the observed (actual) cell counts (O). The calculation is as follows:\n\\(\\chi ^ 2 = \\sum \\frac{(O - E)}{E} ^2\\)\nwith [Number of rows \\(-\\) 1] \\(\\times\\) [Number of columns \\(-\\) 1] degrees of freedom.\nAs for many statistics, the deviations between the observed and expected values are squared to prevent the negative and positive values balancing one another out.\nIf the expected counts are close to the observed counts, the chi-squared statistic will be close to zero, and the P-value will be close to 1. The larger the difference between the observed and expected counts, the larger the chi-squared statistic becomes (and the smaller the P-value). A large chi-squared statistic provides more evidence of an association between the exposure and outcome.\n\n5.2.1 Assumptions for using a Pearson’s chi-squared test\nThe assumptions that must be met when using Pearson’s chi-squared test are that:\n\neach observation must be independent;\neach participant is represented in the table once only;\nat least 80% of the expected cell counts should exceed a value of five;\nall expected cell counts should exceed a value of one.\n\nThe first two assumptions are dictated by the study design. The last two assumptions relate to the numbers in the cells and should be explored when running the test. There should not be too many cells with low expected counts.\n\n\n5.2.2 Worked Example 7.1\nWe will revisit Worked Example 6.4, investigating the relationship between nausea and drug exposure:\n\n\nTable 5.1: Nausea status by drug exposure\n\n\n\nNausea\nNo nausea\nTotal\n\n\n\n\nActive\n15 (30%)\n35 (70%)\n50 (100%)\n\n\nPlacebo\n4 (8%)\n46 (92%)\n50 (100%)\n\n\nTotal\n19 (19%)\n81 (81%)\n100 (100%)\n\n\n\n\nWe can see from the row percentages that 8% of patients in the placebo group experienced nausea compared to 30% of patients in the active group. If no association existed, we would expect to find approximately the same percent of patients with nausea in each group. Statistical software can calculate the values we would expect if there was no association between nausea and drug exposure (i.e. the expected counts):\n\n\nTable 5.2: Expected counts of nausea status by drug exposure\n\n\n\nNausea\nNo nausea\nTotal\n\n\n\n\nActive\n9.5\n40.5\n50\n\n\nPlacebo\n9.5\n40.5\n50\n\n\nTotal\n19\n81\n100\n\n\n\n\nFor the data being considered from Worked Example 7.1 all cells have an expected count greater than 5 and that the minimum cell count is 9.5. Therefore, it is appropriate to use the Pearson’s Chi-Squared test. Note that the ‘Expected’ counts are higher for the groups with ‘No nausea’ because ‘No nausea’ is more prevalent in the sample than ‘Nausea’.\nThe chi-squared statistic is calculated as 7.86 with 1 df, giving a P-value of 0.005. Combining these results with the estimated relative risk (from Module 6), we can state:\nThe proportion with nausea in those who received the active drug is 30%, compared to 8% in those who received the placebo drug. Nausea was more frequent in those who received the active drug (Relative Risk = 3.75, 95% CI: 1.34 to 10.51). There is strong evidence that the proportion with nausea differs between the two groups (\\(\\chi ^2\\) = 7.86 with 1 df, P=0.005).\n\n\n5.2.3 Fisher’s exact test\nIf small expected cell counts are present, Fisher’s exact test can be used instead. More information on Fisher’s exact test can be found in Chapter 13 of An Introduction to Medical Statistics, Bland (2015), or Section 17.3 of Essential Medical Statistics, Kirkwood and Sterne (2001). The computation of Fisher’s exact test is complex, and best conducted by statistical software.\nA reasonable question could be posed: why not conduct Fisher’s exact test by default? The answer to this is complex.\nFisher’s exact test has quite a restrictive assumption: we assume that the totals of the rows and columns are fixed before we conduct the study.\nFrom Worked Example 7.1, this would be saying that we knew we would end up with 50 people in the active treatment arm, and 50 people in the placebo. This seems reasonable, we can design our study to randomise equal groups. However, Fisher’s exact test also assumes that we know we will obtain 19 people with nausea and 81 people without nausea. We cannot possibly know this before we do the study.\nIn the case where we cannot assume that the totals of the rows and columns are fixed before we conduct the study, it can be shown that Fisher’s exact test will be conservative (we will be less likely to reject the null hypothesis when it is false, or in other words, the P-value will be larger than it should be).\nWhile there are other tests that perform better than Fisher’s exact test, most of the time we live with this conservative test when we have to (i.e. for small expected cell counts) because Fisher’s exact test is so widely known.\nPragmatically, we use the standard (Pearson) chi-square when we can, and Fisher’s exact test only when we have small expected cell counts."
  },
  {
    "objectID": "07-testing-proportions.html#chi-squared-tests-for-tables-larger-than-2-by-2",
    "href": "07-testing-proportions.html#chi-squared-tests-for-tables-larger-than-2-by-2",
    "title": "5  Hypothesis testing for categorical data",
    "section": "5.3 Chi-squared tests for tables larger than 2-by-2",
    "text": "5.3 Chi-squared tests for tables larger than 2-by-2\nChi-squared tests can also be used for tables larger than a 2-by-2 dimension. When a contingency table larger than 2-by-2 is used, say a 4-by-2 table if there were 4 exposure groups, the Pearson’s chi-squared can still be used.\n\n5.3.1 Worked Example 7.2\nThe files mod07_allergy.dta and mod07_allergy.rds contain information about the severity of allergic reaction, coded as absent, slight, moderate or severe. We can test the hypothesis that the severity of allergy is not different between males and females. To do this we can use a two-way tabulation to obtain Table 5.3 which shows the counts, expected counts and the percent of females and males who fall into each severity group for allergy. The table shows that the percentage of males is higher in each of the categories of severity (slight, moderate, severe) than the percentage of females.\n\n\nTable 5.3: Allergy data\n\n\n\n\n(a) Observed counts\n\n\n\n\n\n\n\n\n\n\nSex\nNon-allergenic\nSlight allergy\nModerate allergy\nSevere allergy\nTotal\n\n\n\n\nFemale\n150 (62.0%)\n50 (20.7%)\n27 (11.2%)\n15 (6.2%)\n242 (100%)\n\n\nMale\n137 (53.1%)\n70 (27.1%)\n32 (12.4%)\n19 (7.4%)\n258 (100%)\n\n\nTotal\n287 (57.4%)\n120 (24.0%)\n59 (11.8%)\n34 (6.8%)\n500 (100.0%)\n\n\n\n\n\n\n\n\n(b) Expected counts\n\n\n\n\n\n\n\n\n\n\nSex\nNon-allergenic\nSlight allergy\nModerate allergy\nSevere allergy\nTotal\n\n\n\n\nFemale\n138.9\n58.1\n28.6\n16.5\n242.0\n\n\nMale\n148.1\n61.9\n30.4\n17.5\n258.0\n\n\nTotal\n287.0\n120.0\n59.0\n34.0\n500.0\n\n\n\n\n\n\nThe Pearson chi-squared statistic is calculated as 4.31, with 3 degrees of freedom, providing a P-value of 0.23. Therefore, there is little evidence of an association between gender and the severity of allergy."
  },
  {
    "objectID": "07-testing-proportions.html#mcnemars-test-for-categorical-paired-data",
    "href": "07-testing-proportions.html#mcnemars-test-for-categorical-paired-data",
    "title": "5  Hypothesis testing for categorical data",
    "section": "5.4 McNemar’s test for categorical paired data",
    "text": "5.4 McNemar’s test for categorical paired data\nIf a binary categorical outcome is measured in a paired study design, McNemar’s statistic is used. This statistic is a form of chi-square applied to a paired situation. A Pearson’s chi-squared test cannot be used because the measurements are not independent. However, McNemar’s test can be used to assess whether there is a significant change in proportions between two time points or between two conditions, or whether there is a significant difference in proportions between matched cases and controls.\nFor McNemar’s test, the data are displayed as shown in Table 5.4. Cells ‘a’ and ‘d’ called concordant cells because the response was the same at both baseline and follow-up or between matched cases and controls. Cells ‘b’ and ‘c’ are called discordant cells because the responses between the pairs were different. For a follow-up study, the participants in cell ‘c’ had a positive response at baseline and a negative response at follow-up. Conversely, the participants in cell ‘b’ had a negative response at baseline and a positive response at follow-up.\nFor other types of paired data such as twins or matched cases and controls, the data are similarly displayed with the responses of one of the pairs in the columns and the responses for the other of the pairs in the rows. For paired data, the grand total ‘N’ is always the number of pairs and not the total number of participants.\n\n\n\n\nTable 5.4:  Table layout for testing matched proportions \n\n Negative at follow-upPositive at follow-upTotal\n\nNegative at baselineaba + b\n\nPositive at baselinecdc + d\n\nTotala + cb + dN\n\n\n\n\n\n\n5.4.1 Worked Example 7.3\nTwo drugs labelled A and B have been administered to patients in random order so that each patient acts as their own control. The datasets mod07_drug_response.dta and mod07_drug_response.rds are available on Moodle. The null hypothesis is as follows:\n\nH0: The proportion of patients who do better on drug A is the same as the proportion of patients who do better on drug B\n\nCounts and overall percentages are presented in . From the “Total” row in the table, we can see that the number of patients who respond to drug A is 41 (68%) and from the “Total” column the number who respond to drug B is less at 35 (58%), that is there is a difference of 10%.\n\nPaired data\n\n\n\n\n\n\n\n\n\nResponse to Drug B\nNo response to Drug B\nTotal\n\n\n\n\nResponse to Drug A\n21 (35%)\n20 (33%)\n41 (68%)\n\n\nNo response to Drug A\n14 (23%)\n5 (8%)\n19 (32%)\n\n\nTotal\n35 (58%)\n25 (42%)\n60 (100%)\n\n\n\nThe difference in the paired proportions is calculated using the simple equation:\n\\[ p_{A} - p_{B} = \\frac{(b - c)}{N} \\]\nHere, \\(p_{A} - p_{B} = \\frac{(20 - 14)}{60} = 0.1\\)\nThe cell counts show that 20 patients responded to Drug A but not to drug B, and 14 patients responded to Drug B but not to drug A. McNemar’s statistic is computed from these two discordant pairs (labelled as ‘b’ and ‘c’) as follows:\n\\[ X^2 = \\frac{(b-c)^2}{b+c} \\]\nwith 1 degree of freedom. Using our worked example, the McNemar’s chi-squared statistic is calculated as 1.06 with 1 degree of freedom, giving a P-value of 0.3.\nNote that some packages also calculate an “Exact P-Value”. The standard McNemar’s chi-squared statistic is generally recommended, unless the sum of the discordant cells is small (Kirkwood and Sterne define small as less than 10; Section 21.3, Kirkwood and Sterne 2001)). Here, \\(b + c = 34\\), so reporting the standard McNemar’s chi-squared statistic is appropriate.\nAs described above, the difference in proportions can be calculated. A 95% confidence interval for this difference can be obtained using statistical software.\nIn this study of 60 participants, where each participant received both drugs, 41 (68%) responded to Drug A and 35 (58%) responded to Drug B. The difference in the proportions responding is estimated as 10% (95% CI -11% to 31%). There is no evidence that the response differed between the two drugs (McNemar’s chi-square=1.06 with 1 degree of freedom, P=0.3)."
  },
  {
    "objectID": "07-testing-proportions.html#summary",
    "href": "07-testing-proportions.html#summary",
    "title": "5  Hypothesis testing for categorical data",
    "section": "5.5 Summary",
    "text": "5.5 Summary\nIn Module 6, we estimated proportions and measures of association for categorical data and conducted a one-sample test of proportions. In this module, we conduct significance tests for two or more independent proportions using the chi-squared test. The chi-squared test can also be used to conduct a significance test when there are more than two categories in both variables. The McNemar’s test is used when we have paired data."
  },
  {
    "objectID": "07-testing-proportions.html#pearsons-chi-squared-test-for-individual-level-data",
    "href": "07-testing-proportions.html#pearsons-chi-squared-test-for-individual-level-data",
    "title": "5  Hypothesis testing for categorical data",
    "section": "5.6 Pearson’s chi-squared test for individual-level data",
    "text": "5.6 Pearson’s chi-squared test for individual-level data\nWe can use Stata to perform the Chi-squared test on the individual-level data in mod06_nausea.dta. In this dataset group is coded as “1=Active” and “0 = Placebo” and side_effect is coded as “1 = Nausea” and “0 = No nausea”.\nFollow the steps: Statistics &gt; Summaries, tables, and tests &gt; Frequency tables &gt; Two-way table with measures of association. In the tabulate2 dialog box, select the variable group in the Row variable box, and the variable side_effect in the Column variable box as shown below. Tick Pearson’s chi-squared to obtain the Pearson’s chi-squared test or tick Fisher’s exact test if you have small expected cell counts (see Course Notes).\nTo request for expected frequencies, tick the box for Expected frequencies and to request for the row percentage tick the box for Within-row relative frequencies.\n\n\n\n\n\n[Command: tab group side_effect, chi2 expected row]\nClick OK or Submit to obtain the following output.\n\n+--------------------+\n| Key                |\n|--------------------|\n|     frequency      |\n| expected frequency |\n|   row percentage   |\n+--------------------+\n\n           |      Side effect\n     Group | No nausea     Nausea |     Total\n-----------+----------------------+----------\n   Placebo |        46          4 |        50 \n           |      40.5        9.5 |      50.0 \n           |     92.00       8.00 |    100.00 \n-----------+----------------------+----------\n    Active |        35         15 |        50 \n           |      40.5        9.5 |      50.0 \n           |     70.00      30.00 |    100.00 \n-----------+----------------------+----------\n     Total |        81         19 |       100 \n           |      81.0       19.0 |     100.0 \n           |     81.00      19.00 |    100.00 \n\n          Pearson chi2(1) =   7.8622   Pr = 0.005\n\nThe last line labelled Pearson chi2(1) reports the appropriate Chi-squared test statistic which has a value of 7.862 with 1 degree of freedom and a P value of 0.005.\nNote that while the tab2 command will perform the chi-square test, the measure of effect is best obtained using the cs or cc commands, as discussed in Module 6."
  },
  {
    "objectID": "07-testing-proportions.html#pearsons-chi-squared-test-for-summarised-data",
    "href": "07-testing-proportions.html#pearsons-chi-squared-test-for-summarised-data",
    "title": "5  Hypothesis testing for categorical data",
    "section": "5.7 Pearson’s chi-squared test for summarised data",
    "text": "5.7 Pearson’s chi-squared test for summarised data\nWhen you only have the cross-tabulated data, you can use the tabi command from Statistics &gt; Summaries, tables, and tests &gt; Frequency tables &gt; Table calculator. In the tabi dialog box, enter the table in the User-supplied cell frequencies box as 46 4 \\ 35 15 to obtain the same table as above. As explained in the dialog box, each row of frequencies is separated by the backslash “”. You can tick the same additional outputs and tests as in the tab2 command above.\n\n\n\n\n\n[Command: tabi 46 4 \\ 35 15, chi2 exp row]\nWhen you are done, click OK or Submit."
  },
  {
    "objectID": "07-testing-proportions.html#chi-squared-test-for-tables-larger-than-2-by-2",
    "href": "07-testing-proportions.html#chi-squared-test-for-tables-larger-than-2-by-2",
    "title": "5  Hypothesis testing for categorical data",
    "section": "5.8 Chi-squared test for tables larger than 2-by-2",
    "text": "5.8 Chi-squared test for tables larger than 2-by-2\nUse the data in mod07_allergy.dta. We use similar steps as described above for a 2-by-2 table. Here we have defined the column based on sex, and we would like to obtain allergy severity by sex, so we choose “Within-column relative frequencies”. We also request the expected cell counts to check the Pearson chi-squared test assumption that &lt;20% of cells have expected cell count &lt;5 and the minimum expected cell count is &gt;1.\n\n\n\n\n\n[Command: tab allergy_severity sex, chi2 col exp]\nClick OK or Submit to obtain the following output.\n\n. tabulate allergy_severity sex, column expected\n\n+--------------------+\n| Key                |\n|--------------------|\n|     frequency      |\n| expected frequency |\n| column percentage  |\n+--------------------+\n\n     Severity of |          Sex\n         allergy |    Female       Male |     Total\n-----------------+----------------------+----------\n    Non-allergic |       150        137 |       287 \n                 |     138.9      148.1 |     287.0 \n                 |     61.98      53.10 |     57.40 \n-----------------+----------------------+----------\n  Slight allergy |        50         70 |       120 \n                 |      58.1       61.9 |     120.0 \n                 |     20.66      27.13 |     24.00 \n-----------------+----------------------+----------\nModerate allergy |        27         32 |        59 \n                 |      28.6       30.4 |      59.0 \n                 |     11.16      12.40 |     11.80 \n-----------------+----------------------+----------\n  Severe allergy |        15         19 |        34 \n                 |      16.5       17.5 |      34.0 \n                 |      6.20       7.36 |      6.80 \n-----------------+----------------------+----------\n           Total |       242        258 |       500 \n                 |     242.0      258.0 |     500.0 \n                 |    100.00     100.00 |    100.00\n\n          Pearson chi2(3) =   4.3089   Pr = 0.230\n\nThe Pearson chi-squared statistic provides a P-value of 0.23. Therefore, there is little evidence of an association between gender and the severity of allergy."
  },
  {
    "objectID": "07-testing-proportions.html#mcnemars-test-for-paired-proportions",
    "href": "07-testing-proportions.html#mcnemars-test-for-paired-proportions",
    "title": "5  Hypothesis testing for categorical data",
    "section": "5.9 McNemar’s test for paired proportions",
    "text": "5.9 McNemar’s test for paired proportions\nTo perform this test in Stata, we will use the dataset mod07_drug_response.dta. Responses to each drug should be in separate variables in the dataset as shown in Table 7.2 using the tabulate2 command (Statistics &gt; Summaries, tables, and tests &gt; Frequency tables &gt; Two-way table with measures of association). In the tabulate2 dialog box, tick Relative frequencies under Cell contents as shown below.\n\n\n\n\n\n[Command: tab2 drugb druga , cell]\nClick OK or Submit to obtain the following output:\n\n. tabulate druga drugb, cell\n\n+-----------------+\n| Key             |\n|-----------------|\n|    frequency    |\n| cell percentage |\n+-----------------+\n\n  Response |  Response to Drug B\n to Drug A |        No        Yes |     Total\n-----------+----------------------+----------\n        No |         5         14 |        19 \n           |      8.33      23.33 |     31.67 \n-----------+----------------------+----------\n       Yes |        20         21 |        41 \n           |     33.33      35.00 |     68.33 \n-----------+----------------------+----------\n     Total |        25         35 |        60 \n           |     41.67      58.33 |    100.00 \n\nTo perform the McNemar’s test, go to Statistics &gt; Epidemiology and related &gt; Tables for epidemiologists &gt; Matched case-control studies. In the mcc dialog box, select the variable druga as the Exposed case variable and drugb as the Exposed control variable as shown below.\n\n\n\n\n\n[Command: mcc druga drugb]\nClick OK or Submit when you are done to obtain the following output:\n\n. mcc druga drugb\n\n                 |        Controls        |\nCases            |   Exposed   Unexposed  |      Total\n-----------------+------------------------+-----------\n         Exposed |        21          20  |         41\n       Unexposed |        14           5  |         19\n-----------------+------------------------+-----------\n           Total |        35          25  |         60\n\nMcNemar's chi2(1) =      1.06    Prob &gt; chi2 = 0.3035\nExact McNemar significance probability       = 0.3915\n\nProportion with factor\n        Cases       .6833333\n        Controls    .5833333     [95% Conf. Interval]\n                   ---------     --------------------\n        difference        .1     -.1054528   .3054528\n        ratio       1.171429      .8663498   1.583939\n        rel. diff.       .24     -.1585239   .6385239\n\n        odds ratio  1.428571      .6862537   3.057277   (exact)\n\nTwo versions of the McNemar’s test are given in the output. The McNemar’s chi-squared statistic is generally recommended, unless the sum of the discordant cells is small (Kirkwood and Sterne define small as less than &lt;10; Section 21.3, Kirkwood and Sterne 2001)). The P value for the McNemar test is 0.3, providing no evidence against the null hyopthesis."
  },
  {
    "objectID": "07-testing-proportions.html#pearsons-chi-squared-test-for-individual-level-data-1",
    "href": "07-testing-proportions.html#pearsons-chi-squared-test-for-individual-level-data-1",
    "title": "5  Hypothesis testing for categorical data",
    "section": "5.10 Pearson’s chi-squared test for individual-level data",
    "text": "5.10 Pearson’s chi-squared test for individual-level data\nWe will demonstrate how to use R to conduct a Pearson chi-squared test using Worked Example 7.1.\n\nlibrary(jmv)\n\nnausea &lt;- readRDS(\"data/examples/mod06_nausea.rds\")\n\nhead(nausea)\n\n\n\n\ngroupside_effect\n\nPlaceboNausea\n\nPlaceboNausea\n\nPlaceboNausea\n\nPlaceboNausea\n\nPlaceboNo nausea\n\nPlaceboNo nausea\n\n\n\nstr(nausea$group)\n\n Factor w/ 2 levels \"Placebo\",\"Active\": 1 1 1 1 1 1 1 1 1 1 ...\n - attr(*, \"label\")= chr \"Group\"\n\nstr(nausea$side_effect)\n\n Factor w/ 2 levels \"No nausea\",\"Nausea\": 2 2 2 2 1 1 1 1 1 1 ...\n - attr(*, \"label\")= chr \"Side effect\"\n\n\nThe columns group and side_effect have been entered as factors, with “Placebo” and “No nausea” as the first levels. We should use the relevel() command to re-order the factor levels.\n\nnausea$group &lt;- relevel(nausea$group, ref=\"Active\")\nnausea$side_effect &lt;- relevel(nausea$side_effect, ref=\"Nausea\")\n\nstr(nausea$group)\n\n Factor w/ 2 levels \"Active\",\"Placebo\": 2 2 2 2 2 2 2 2 2 2 ...\n\nstr(nausea$side_effect)\n\n Factor w/ 2 levels \"Nausea\",\"No nausea\": 1 1 1 1 2 2 2 2 2 2 ...\n\n\nAfter confirming the factors are appropriately defined, we can construct our 2-by-2 table and view the expected frequencies.\n\ncontTables(data=nausea,\n           rows=group, cols=side_effect,\n           exp=TRUE)\n\n\n CONTINGENCY TABLES\n\n Contingency Tables                                             \n ────────────────────────────────────────────────────────────── \n   group                  Nausea       No nausea    Total       \n ────────────────────────────────────────────────────────────── \n   Active     Observed           15           35           50   \n              Expected     9.500000     40.50000     50.00000   \n                                                                \n   Placebo    Observed            4           46           50   \n              Expected     9.500000     40.50000     50.00000   \n                                                                \n   Total      Observed           19           81          100   \n              Expected    19.000000     81.00000    100.00000   \n ────────────────────────────────────────────────────────────── \n\n\n χ² Tests                              \n ───────────────────────────────────── \n         Value       df    p           \n ───────────────────────────────────── \n   χ²    7.862248     1    0.0050478   \n   N          100                      \n ───────────────────────────────────── \n\n\nAfter confirming that there are no cells with small expected frequencies, we can interpret the chi-square test. The last section reports the chi-squared test statistic which has a value of 7.86 with 1 degree of freedom and a P-value of 0.005.\nIf there are small values of expected frequencies, Fisher’s exact test can be requested using fisher = TRUE:\n\ncontTables(data=nausea,\n           rows=group, cols=side_effect,\n           fisher = TRUE)\n\n\n CONTINGENCY TABLES\n\n Contingency Tables                          \n ─────────────────────────────────────────── \n   group      Nausea    No nausea    Total   \n ─────────────────────────────────────────── \n   Active         15           35       50   \n   Placebo         4           46       50   \n   Total          19           81      100   \n ─────────────────────────────────────────── \n\n\n χ² Tests                                               \n ────────────────────────────────────────────────────── \n                          Value       df    p           \n ────────────────────────────────────────────────────── \n   χ²                     7.862248     1    0.0050478   \n   Fisher's exact test                      0.0094886   \n   N                           100                      \n ──────────────────────────────────────────────────────"
  },
  {
    "objectID": "07-testing-proportions.html#pearsons-chi-squared-test-for-summarised-data-1",
    "href": "07-testing-proportions.html#pearsons-chi-squared-test-for-summarised-data-1",
    "title": "5  Hypothesis testing for categorical data",
    "section": "5.11 Pearson’s chi-squared test for summarised data",
    "text": "5.11 Pearson’s chi-squared test for summarised data\nWhen you only have the summarised date (for example, the cross-tabulated data), you need to enter the summarised data manually. As we did in Module 6, the 2-by-2 table can be entered as four lines of data:\n\ndrug_aggregated &lt;- data.frame(\n  group = c(\"Active\", \"Active\", \"Placebo\", \"Placebo\"),\n  side_effect = c(\"Nausea\", \"No nausea\", \"Nausea\", \"No nausea\"),\n  n = c(15, 35, 4, 46)\n)\n\nThe contTables() function is used in the usual way, specifying count=n."
  },
  {
    "objectID": "07-testing-proportions.html#chi-squared-test-for-tables-larger-than-2-by-2-1",
    "href": "07-testing-proportions.html#chi-squared-test-for-tables-larger-than-2-by-2-1",
    "title": "5  Hypothesis testing for categorical data",
    "section": "5.12 Chi-squared test for tables larger than 2-by-2",
    "text": "5.12 Chi-squared test for tables larger than 2-by-2\nUse the data in mod07_allergy.rds. We use similar steps as described above for a 2-by-2 table.\n\nallergy &lt;- readRDS(\"data/examples/mod07_allergy.rds\")\n\nhead(allergy)\n\n\n\n\nidasthmahdmallergycatallergyinfectionsexmaternalasthmaallergy_severity\n\n1NoYesNoYesFemaleNoModerate allergy\n\n2YesNoNoNoFemaleNoNon-allergic\n\n3YesNoNoNoFemaleNoNon-allergic\n\n4NoNoNoNoMaleNoNon-allergic\n\n4YesYesYesNoFemaleNoModerate allergy\n\n5YesYesYesNoFemaleNoModerate allergy\n\n\n\n\n\ncontTables(data=allergy, \n           rows=allergy_severity, cols=sex, \n           pcCol=TRUE)\n\n\n CONTINGENCY TABLES\n\n Contingency Tables                                                             \n ────────────────────────────────────────────────────────────────────────────── \n   allergy_severity                       Female       Male         Total       \n ────────────────────────────────────────────────────────────────────────────── \n   Non-allergic        Observed                 150          137          287   \n                       % within column     61.98347     53.10078     57.40000   \n                                                                                \n   Slight allergy      Observed                  50           70          120   \n                       % within column     20.66116     27.13178     24.00000   \n                                                                                \n   Moderate allergy    Observed                  27           32           59   \n                       % within column     11.15702     12.40310     11.80000   \n                                                                                \n   Severe allergy      Observed                  15           19           34   \n                       % within column      6.19835      7.36434      6.80000   \n                                                                                \n   Total               Observed                 242          258          500   \n                       % within column    100.00000    100.00000    100.00000   \n ────────────────────────────────────────────────────────────────────────────── \n\n\n χ² Tests                              \n ───────────────────────────────────── \n         Value       df    p           \n ───────────────────────────────────── \n   χ²    4.308913     3    0.2299813   \n   N          500                      \n ─────────────────────────────────────"
  },
  {
    "objectID": "07-testing-proportions.html#mcnemars-test-for-paired-proportions-1",
    "href": "07-testing-proportions.html#mcnemars-test-for-paired-proportions-1",
    "title": "5  Hypothesis testing for categorical data",
    "section": "5.13 McNemar’s test for paired proportions",
    "text": "5.13 McNemar’s test for paired proportions\nTo perform this test in R, we will use the dataset mod07_drug_response.rds.\n\ndrug &lt;- readRDS(\"data/examples/mod07_drug_response.rds\")\n\nhead(drug)\n\n\n\n\ndrugadrugb\n\nYesYes\n\nYesYes\n\nYesYes\n\nYesYes\n\nYesYes\n\nYesYes\n\n\n\n\nAs usual, we should check that the variables being tabulated are factors, with the first level of the factor being the outcome of interest.\n\nstr(drug$druga)\n\n Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 2 2 2 2 2 2 ...\n - attr(*, \"label\")= chr \"Response to Drug A\"\n\nstr(drug$drugb)\n\n Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 2 2 2 2 2 2 ...\n - attr(*, \"label\")= chr \"Response to Drug B\"\n\n\nHere we see that the first level of the factor is “No” - we need to use the relevel() function to re-order the levels so “Yes” is the first level:\n\ndrug$druga &lt;- relevel(drug$druga, ref=\"Yes\")\ndrug$drugb &lt;- relevel(drug$drugb, ref=\"Yes\")\n\nstr(drug$druga)\n\n Factor w/ 2 levels \"Yes\",\"No\": 1 1 1 1 1 1 1 1 1 1 ...\n\nstr(drug$drugb)\n\n Factor w/ 2 levels \"Yes\",\"No\": 1 1 1 1 1 1 1 1 1 1 ...\n\n\nWe can use the contTablesPaired() function within the jmv library to conduct McNemar’s test of paired proportions:\n\ncontTablesPaired(data=drug, rows=druga, cols=drugb)\n\n\n PAIRED SAMPLES CONTINGENCY TABLES\n\n Contingency Tables              \n ─────────────────────────────── \n   druga    Yes    No    Total   \n ─────────────────────────────── \n   Yes       21    20       41   \n   No        14     5       19   \n   Total     35    25       60   \n ─────────────────────────────── \n\n\n McNemar Test                          \n ───────────────────────────────────── \n         Value       df    p           \n ───────────────────────────────────── \n   χ²    1.058824     1    0.3034837   \n   N           60                      \n ───────────────────────────────────── \n\n\nNote that contTablesPaired() does not calculate an exact P-value.\nTo estimate the proportion in each of the paired samples, its difference, and the 95% confidence interval of the difference, we can use the mcNemarDiff() function which can be downloaded here.\n\n### Copied from gist.githubusercontent.com\nmcNemarDiff &lt;- function(data, var1, var2, digits = 3) {\n  if (!requireNamespace(\"epibasix\", quietly = TRUE)) {\n    stop(\"This function requires epibasix to be installed\")\n  }\n  \n  tab &lt;- table(data[[var1]], data[[var2]])\n  p1 &lt;- (tab[1, 1] + tab[1, 2]) / sum(tab)\n  p2 &lt;- (tab[1, 1] + tab[2, 1]) / sum(tab)\n  pd &lt;- epibasix::mcNemar(tab)$rd\n  pd.cil &lt;- epibasix::mcNemar(tab)$rd.CIL\n  pd.ciu &lt;- epibasix::mcNemar(tab)$rd.CIU\n  print(paste0(\n    \"Proportion 1: \",\n    format(round(p1, digits = digits), nsmall = digits),\n    \"; Proportion 2: \", format(round(p2, digits = digits), nsmall = digits)\n  ))\n  print(paste0(\n    \"Difference in paired proportions: \",\n    format(round(pd, digits = digits), nsmall = digits),\n    \"; 95% CI: \", format(round(pd.cil, digits = digits), nsmall = digits),\n    \" to \", format(round(pd.ciu, digits = digits), nsmall = digits)\n  ))\n}\n### End copy\n\nmcNemarDiff(data = drug, var1 = \"druga\", var2 = \"drugb\", digits = 2)\n\n[1] \"Proportion 1: 0.68; Proportion 2: 0.58\"\n[1] \"Difference in paired proportions: 0.10; 95% CI: -0.11 to 0.31\"\n\n\nIn this study of 60 participants, where each participant received both drugs, 41 (68%) responded to Drug A and 35 (58%) responded to Drug B. The difference in the proportions responding is estimated as 10% (95% CI -11% to 31%). There is no evidence that the response differed between the two drugs (McNemar’s chi-squared = 1.06 with 1df, P=0.30)."
  },
  {
    "objectID": "08-correlation-and-regression.html",
    "href": "08-correlation-and-regression.html",
    "title": "6  Correlation and simple linear regression",
    "section": "",
    "text": "Stata notes\nWe will demonstrate using R for correlation and simple linear regression using the dataset mod08_lung_function.rds.\nlung &lt;- readRDS(\"data/examples/mod08_lung_function.rds\")"
  },
  {
    "objectID": "08-correlation-and-regression.html#learning-objectives",
    "href": "08-correlation-and-regression.html#learning-objectives",
    "title": "6  Correlation and simple linear regression",
    "section": "Learning objectives",
    "text": "Learning objectives\nBy the end of this module you will be able to:\n\nExplore the association between two continuous variables using a scatter plot;\nEstimate and interpret correlation coefficients;\nEstimate and interpret parameters from a simple linear regression;\nAssess the assumptions of simple linear regression;\nTest a hypothesis using regression coefficients."
  },
  {
    "objectID": "08-correlation-and-regression.html#optional-readings",
    "href": "08-correlation-and-regression.html#optional-readings",
    "title": "6  Correlation and simple linear regression",
    "section": "Optional readings",
    "text": "Optional readings\nKirkwood and Sterne (2001); Chapter 10. [UNSW Library Link]\nBland (2015); Chapter 11. [UNSW Library Link]\nAcock (2010); Chapter 8."
  },
  {
    "objectID": "08-correlation-and-regression.html#introduction",
    "href": "08-correlation-and-regression.html#introduction",
    "title": "6  Correlation and simple linear regression",
    "section": "6.1 Introduction",
    "text": "6.1 Introduction\nIn Module 5, we saw how to test whether the means from two groups are equal - in other words, whether a continuous variable is related to a categorical variable. Sometimes we are interested in how closely two continuous variables are related. For example, we may want to know how closely blood cholesterol levels are related to dietary fat intake in adult men. To measure the strength of association between two continuously distributed variables, a correlation coefficient is used.\nWe may also want to predict a value of a continuous measurement from another continuous measurement. For example, we may want to know predict values of lung capacity from height in a community of adults. A regression model allows us to use one measurement to predict another measurement.\nAlthough both correlation coefficients and regression models can be used to describe the degree of association between two continuous variables, the two methods provide different information. It is important to note that both methods summarise the strength of an association between variables, and do not imply a causal relationship."
  },
  {
    "objectID": "08-correlation-and-regression.html#notation",
    "href": "08-correlation-and-regression.html#notation",
    "title": "6  Correlation and simple linear regression",
    "section": "6.2 Notation",
    "text": "6.2 Notation\nIn this module, we will be focussing on the association between two variables, denoted \\(x\\) and \\(y\\).\nThere may be cases where it does not matter which variable is denoted \\(x\\) and which is denoted \\(y\\), however this is rare. We are usually interested in whether one variable is associated with another. If we believe that a change in \\(x\\) will lead to a change in \\(y\\), or that \\(y\\) is influenced by \\(x\\), we define \\(y\\) as the outcome variable and \\(x\\) as the explanatory variable."
  },
  {
    "objectID": "08-correlation-and-regression.html#correlation",
    "href": "08-correlation-and-regression.html#correlation",
    "title": "6  Correlation and simple linear regression",
    "section": "6.3 Correlation",
    "text": "6.3 Correlation\nWe use correlation to measure the strength of a linear relationship between two variables. Before calculating a correlation coefficient, a scatter plot should first be obtained to give an understanding of the nature of the relationship between the two variables.\n\n6.3.1 Worked Example\nThe file mod08_lung_function.csv has information about height and lung function collected from a random sample of 120 adults. Information was collected on height (cm) and lung function, which was measured as forced vital capacity (FVC), measured in litres. We can obtain a scatter-plot shown in Figure 6.1, where the outcome variable (\\(y\\)) is plotted on the vertical axis, and the explanatory variable (\\(x\\)) is plotted on the horizontal axis.\nFigure 6.1 shows that as height increases, lung function also increases, which is as expected. One or two of the data points are separated from the rest of the data but are not so far away as to be considered outliers because they do not seem to stand out of other observations.\n\n\n\n\n\nFigure 6.1: Association between height and lung function in 120 adults\n\n\n\n\n\n\n6.3.2 Correlation coefficients\nA correlation coefficient (r) describes how closely the variables are related, that is the strength of linear association between two continuous variables. The range of the coefficient is from +1 to −1 where +1 is a perfect positive association, 0 is no association and −1 is a perfect inverse association. In general, an absolute (disregarding the sign) r value below 0.3 indicates a weak association, 0.3 to &lt; 0.6 is fair association, 0.6 to &lt; 0.8 is a moderate association, and \\(\\ge\\) 0.8 indicates a strong association.\nThe correlation coefficient is positive when large values of one variable tend to occur with large values of the other, and small values of one variable (y) tend to occur with small values of the other (x) (Figure 6.2 (a and b)). For example, height and weight in healthy children or age and blood pressure.\nThe correlation coefficient is negative when large values of one variable tend to occur with small values of the other, and small values of one variable tend to occur with large values of the other (Figure 6.2 (c and d)). For example, percentage immunised against infectious diseases and under-five mortality rate.\n\n\n\n\n\nFigure 6.2: Scatter plots demonstrating strong and weak, positive and negative associations\n\n\n\n\nIt is possible to calculate a P-value associated with a correlation coefficient to test whether the correlation coefficient is different from zero. However, a correlation coefficient with a large P-value does not imply that there is no relationship between \\(x\\) and \\(y\\), because the correlation coefficient only tests for a linear association and there may be a non-linear relationship such as a curved or irregular relationship.\nThe assumptions for using a Pearson’s correlation coefficient are that:\n\nobservations are independent;\nboth variables are continuous variables;\nthe relationship between the two variables is linear.\n\nThere is a further assumption that the data follow a bivariate normal distribution. This assumes: y follows a normal distribution for given values of x; and x follows a normal distribution for given values of y. This is quite a technical assumption that we do not discuss further.\nThere are two types of correlation coefficients– the correct one to use is determined by the nature of the variables as shown in Table 6.1.\n\n\n\n\nTable 6.1:  Correlation coefficients and their application \n\nCorrelation coefficientApplication\n\nPearson’s correlation coefficient: rBoth variables are continuous and a bivariate normal distribution can be assumed\n\nSpearman’s rank correlation: rhoBivariate normality cannot be assumed. Also useful when at least one of the variables is ordinal\n\n\n\n\n\nSpearman’s \\(\\rho\\) is calculated using the ranks of the data, rather than the actual values of the data. We will see further examples of such methods in Module 9, when we consider non-parametric tests, which are often based on ranks.\nCorrelation coefficients are often presented in the form of a correlation matrix which can display the correlation between a number of variables in a single table (Table 6.2).\n\n\nTable 6.2: Correlation matrix for Height and FVC\n\n\n\n\n\n\n\n\nHeight\nFVC\n\n\nHeight\n1\n0.70\nP &lt; 0.0001\n\n\nFVC\n0.70\nP &lt; 0.0001\n1\n\n\n\n\nThis correlation matrix shows that the Pearson’s correlation coefficient between height and lung function is 0.70 with P&lt;0.0001 indicating very strong evidence of a linear association between height and FVC. A correlation matrix sometimes includes correlations between the same variable, indicated as a correlation coefficient of 1. For example, \\(Height\\) is perfectly correlated with itself (i.e. has a correlation coefficient of 1). Similarly, \\(FVC\\) is perfectly correlated with itself.\nCorrelation coefficients are rarely used as important statistics in their own right because they do not fully explain the relationship between the two variables and the range of the data has an important influence on the size of the coefficient. In addition, the statistical significance of the correlation coefficient is often over interpreted because a small correlation which is of no clinical importance can become statistically significant even with a relatively small sample size. For example, a poor correlation of 0.3 will be statistically significant if the sample size is large enough."
  },
  {
    "objectID": "08-correlation-and-regression.html#linear-regression",
    "href": "08-correlation-and-regression.html#linear-regression",
    "title": "6  Correlation and simple linear regression",
    "section": "6.4 Linear regression",
    "text": "6.4 Linear regression\nThe nature of a relationship between two variables is more fully described using regression, where the relationship is described by a straight line.\nFigure 6.3 shows our lung data with a fitted regression line.\n\n\n\n\n\nFigure 6.3: Association between height and lung function in 120 adults\n\n\n\n\nThe line through the plot is called the line of ‘best fit’ because the size of the deviations between the data points and the line is minimised in estimating the line.\n\n6.4.1 Regression equations\nThe mathematical equation for the line explains the relationship between two variables: \\(y\\), the outcome variable, and \\(x\\), the explanatory variable. The equation of the regression line is as follows:\n\\[y = \\beta_{0} + \\beta_{1}x\\]\nThis line is shown in Figure 6.4 using the notation shown in Table 6.3.\n\n\n\n\n\nFigure 6.4: Coefficients of a linear regression equation\n\n\n\n\n\n\n\n\nTable 6.3:  Notation for linear regression equation \n\nSymbolInterpretation\n\n\\(y \\)The outcome variable\n\n\\(x \\)The explanatory variable\n\n\\(\\beta_0\\)Intercept of the regression line\n\n\\(\\beta_1\\)Slope of the regression line\n\n\n\n\n\nThe intercept is the point at which the regression line intersects with the y-axis when the value of \\(x\\) is zero. In most cases, the intercept does not have a biologically meaningful interpretation as the explanatory variable cannot take a value of zero. In our working example, the intercept is not meaningful as it is not possible for an adult to have a height of 0cm.\nThe slope of the line is the predicted change in the outcome variable \\(y\\) as the explanatory explanatory variable \\(x\\) increases by 1 unit.\nAn important concept is that regression predicts an expected value of \\(y\\) given an observed value of \\(x\\): any error around the explanatory variable is not taken into account."
  },
  {
    "objectID": "08-correlation-and-regression.html#regression-coefficients-estimation",
    "href": "08-correlation-and-regression.html#regression-coefficients-estimation",
    "title": "6  Correlation and simple linear regression",
    "section": "6.5 Regression coefficients: estimation",
    "text": "6.5 Regression coefficients: estimation\nThe regression parameters \\(\\beta_{0}\\) and \\(\\beta_{1}\\) are true, unknown quantities (similar to \\(\\mu\\) and \\(\\sigma\\)), which are estimated using statistical software using the method of least squares. This method estimates the intercept and the slope, and also their variability (i.e. standard errors). Software is always used to estimate the regression parameters from a set of data.\nUsing the method of least squares:\n\nthe intercept is estimated as \\(b_0\\);\nthe slope is estimated as \\(b_1\\)."
  },
  {
    "objectID": "08-correlation-and-regression.html#regression-coefficients-inference",
    "href": "08-correlation-and-regression.html#regression-coefficients-inference",
    "title": "6  Correlation and simple linear regression",
    "section": "6.6 Regression coefficients: inference",
    "text": "6.6 Regression coefficients: inference\nWe can use the estimated regression coefficients and their variability to calculate 95% confidence intervals. Here, a t-value from a t-distribution with \\(n - 2\\) degrees of freedom is used:\n\n95% confidence interval for intercept: \\(b_0 \\pm t_{n-2} \\times SE(b_0)\\)\n95% confidence interval for slope: \\(b_1 \\pm t_{n-2} \\times SE(b_1)\\)\n\nNote that as the constant (\\(b_0\\)) is not often biologically plausible, the 95% confidence interval for the constant is often not reported.\nThe significance of the estimated slope (and less commonly, intercept) can be tested using a t-test. The null hypotheses and the alternative hypothesis for testing the slope of a simple linear regression model are:\n\nH0: \\(\\beta_1 = 0\\)\nH1: \\(\\beta_1 \\ne 0\\)\n\nTo test the null hypothesis for the regression coefficient \\(\\beta_1\\), the following t-test is used:\n\\[t = b_1 /SE(b_1)\\]\nThis will give a t statistic which can be referred to a t distribution with n − 2 degrees of freedom to calculate the corresponding P-value.\nTable 6.4 shows the estimated regression coefficients for our working example.\n\n\n\n\nTable 6.4:  Estimated regression coefficients \n\nTermEstimateStandard errort valueP value95% Confidence interval\n\nIntercept-18.9 2.19 t=-8.60, 118df&lt;0.001-23.22 to -14.53\n\nHeight0.140.013t=10.58, 118df&lt;0.0010.11 to 0.17\n\n\n\n\n\nFrom this output, we see that the slope is estimated as 0.14 with an estimated intercept of -18.87. Therefore, the regression equation is estimated as:\nFVC (L) = − 18.87 + (0.14 \\(\\times\\) Height in cm)\nThere is very strong evidence of a linear association between FVC and height in cm (P &lt; 0.001).\nThis equation can be used to predict FVC for a person of a given height. For example, the predicted FVC for a person 165 cm tall is estimated as:\nFVC = − 18.87347 + (0.1407567 \\(\\times\\) 165.0) = 4.40 L.\nNote that for the purpose of prediction we have kept all the decimal places in the coefficients to avoid rounding error in the intermediate calculation.\n\n6.6.1 Fit of a linear regression model\nAfter fitting a linear regression model, it is important to know how well the model fits the observed data. One way of assessing the model fit is to compute a statistic called coefficient of determination, denoted by \\(R^2\\). It is the square of the Pearson correlation coefficient \\(r: r^2 = R^2\\). Since the range of \\(r\\) is from −1 to 1, \\(R^2\\) must lie between 0 and 1.\n\\(R^2\\) can be interpreted as the proportion of variability in y that can be explained by variability in x. Hence, the following conditions may arise:\nIf \\(R^2 = 1\\), then all variation in y can be explained by variation of x and all data points fall on the regression line.\nIf \\(R^2 = 0\\), then none of the variation in y is related to x at all, and the variable x explains none of the variability in y.\nIf \\(0 &lt; R^2 &lt;1\\), then the variability of y can be partially explained by the variability in x. The larger the \\(R^2\\) value, the better is the fit of the regression model."
  },
  {
    "objectID": "08-correlation-and-regression.html#assumptions-for-linear-regression",
    "href": "08-correlation-and-regression.html#assumptions-for-linear-regression",
    "title": "6  Correlation and simple linear regression",
    "section": "6.7 Assumptions for linear regression",
    "text": "6.7 Assumptions for linear regression\nRegression is robust to moderate degrees of non-normality in the variables, provided that the sample size is large enough and that there are no influential outliers. Also, the regression equation describes the relationship between the variables and this is not influenced as much by the spread of the data as the correlation coefficient is.\nThe assumptions that must be met when using linear regression are as follows:\n\nobservations are independent;\nthe relationship between the explanatory and the outcome variable is linear;\nthe residuals are normally distributed.\n\nA residual is defined as the difference between the observed and predicted outcome from the regression model. If the predicted value of the outcome variable is denoted by \\(\\hat y\\) then:\n\\[ \\text{Residual} = \\text{observed} - \\text{predicted} = y - \\hat y\\]\nIt is important for regression modelling that the data are collected in a period when the relationship remains constant. For example, in building a model to predict normal values for lung function the data must be collected when the participants have been resting and not exercising and people taking bronchodilator medications that influence lung capacity should be excluded. In regression, it is not so important that the variables themselves are normally distributed, but it is important that the residuals are. Scatter plots and specific diagnostic tests can be used to check the regression assumptions. Some of these will not be covered in this introductory course but will be discussed in detail in the Regression Methods in Biostatistics course.\nThe distribution of the residuals should always be checked. Large residuals can indicate unusual points or points that may exert undue influence on the estimated regression slope.\nThe histogram of residuals from the model is shown in Figure 6.5. The residuals are approximately normally distributed, with no outlying values.\n\n\n\n\n\nFigure 6.5: Histogram of regression residuals"
  },
  {
    "objectID": "08-correlation-and-regression.html#multiple-linear-regression",
    "href": "08-correlation-and-regression.html#multiple-linear-regression",
    "title": "6  Correlation and simple linear regression",
    "section": "6.8 Multiple linear regression",
    "text": "6.8 Multiple linear regression\nIn the above example, we have only used a simple linear regression model of two continuous variables. Other more complex models can be built from this e.g. if we wanted to look at the effect of gender (male vs. female) as binary indicator in the model while adjusting for the effect of height. In that case we would include both the variables in the model as explanatory variables. In the same way we can include any number of explanatory variables (both continuous and categorical) in the model: this is called a multivariable model. Multivariable models are often used for building predictive equations, for example by using age, height, gender and smoking history to predict lung function, or to adjust for confounding and detect effect modification to investigate the association between an exposure and an outcome factor.\nMultiple regression has an important role in investigating causality in epidemiology. The exposure variable under investigation must stay in the model and the effects of other variables which can be confounders or effect-modifiers are tested. The biological, psychological or social meaning of the variables in the model and their interactions are of great importance for interpreting theories of causality.\nOther multivariable models include binary logistic regression for use with a binary outcome variable, or Cox regression for survival analyses. These models, together with multiple regression, will be taught in PHCM9517: Regression Methods in Biostatistics."
  },
  {
    "objectID": "08-correlation-and-regression.html#creating-a-scatter-plot",
    "href": "08-correlation-and-regression.html#creating-a-scatter-plot",
    "title": "6  Correlation and simple linear regression",
    "section": "6.9 Creating a scatter plot",
    "text": "6.9 Creating a scatter plot\nWe will demonstrate using Stata for correlation and simple linear regression using the dataset mod08_lung_function.dta.\nTo create a scatter plot to explore the association between height and FVC click: Graphics &gt; Twoway graph (scatter, line, etc.). In the twoway dialog box, click Create…\n\n\n\n\n\nA new dialog box will open. Select the Basic plots radio button and highlight Scatter under Basic plots: (select type). Choose FVC for the Y variable and Height for the X variable.\n\n\n\n\n\nClick the Accept button in the Plot 1 dialog box to return to the twoway dialog box, then click the OK or Submit button to produce the scatter plot shown in Figure 8.1.\n[Command: twoway (scatter FVC Height)]\nTo add a fitted line, go back to the twoway dialog box. If you clicked the OK button, you can go to Graphics &gt; Twoway graph (scatter, line, etc.) to bring it back again.\n\n\n\n\n\nClick Create…, then select the Fit plots radio button and Linear prediction under Fit plots: (select type). Choose FVC for the Y variable and Height for the X variable.\n\n\n\n\n\nClick the Accept button, then the OK or Submit button to produce the scatterplot below.\n[Command: twoway (scatter FVC Height) (lfit FVC Height)]\n\n\n\n\n\nNotice that a legend now appears, and the y-axis title is missing. To add a y-axis title, go to the Y axis tab in the twoway dialog box to enter your title as shown below.\n\n\n\n\n\nYou can click the Submit button to check how the scatter plot looks like. Next go the Legend tab and select the Hide legend radio button.\n\n\n\n\n\nClick the OK or Submit button when you are finished to produce Figure 8.3.\n[Command: twoway (scatter FVC Height) (lfit FVC Height), ytitle(Forced vital capacity (L)) legend(off)]\nTo save your graph, go to File &gt; Save in the Graph window, and be sure to save your file as a PNG file:"
  },
  {
    "objectID": "08-correlation-and-regression.html#calculating-a-correlation-coefficient",
    "href": "08-correlation-and-regression.html#calculating-a-correlation-coefficient",
    "title": "6  Correlation and simple linear regression",
    "section": "6.10 Calculating a correlation coefficient",
    "text": "6.10 Calculating a correlation coefficient\nTo calculate the Pearson’s correlation using the dataset mod08_lung_function.dta go to: Statistics &gt; Summaries, tables, and tests &gt; Summary and descriptive statistics &gt; Pairwise correlations\nSelect the two variables, FVC and Height in the Variables box. You can click the Submit button to check the output. Next, tick the box for Print significance level for each entry to obtain the P-value and the box for Print number of observations for each entry to obtain the number of observations used as shown below.\n\n\n\n\n\nClick the OK or the Submit button when you are done to produce Output 8.1,\n[Command: pwcorr Height FVC, obs sig]"
  },
  {
    "objectID": "08-correlation-and-regression.html#fitting-a-simple-linear-regression-model",
    "href": "08-correlation-and-regression.html#fitting-a-simple-linear-regression-model",
    "title": "6  Correlation and simple linear regression",
    "section": "6.11 Fitting a simple linear regression model",
    "text": "6.11 Fitting a simple linear regression model\nWe will fit a simple linear regression model with mod08_lung_function.dta to quantify the relationship between FVC and height.\nChoose Statistics &gt; Linear models and related &gt; Linear regression\nIn the regress dialog box, select FVC as the Dependent variable, and Height as the Independent variable.\n\n\n\n\n\nClick the OK or the Submit button when you are done to produce Outputs 8.2 and 8.3.\n[Command: reg FVC Height]"
  },
  {
    "objectID": "08-correlation-and-regression.html#plotting-residuals-from-a-simple-linear-regression",
    "href": "08-correlation-and-regression.html#plotting-residuals-from-a-simple-linear-regression",
    "title": "6  Correlation and simple linear regression",
    "section": "6.12 Plotting residuals from a simple linear regression",
    "text": "6.12 Plotting residuals from a simple linear regression\nTo obtain the residuals, go to Statistics &gt; Post estimation after running the regress command.\nIn the Postestimation Selector dialog box, select Predictions and their SEs, leverage statistics, distance statistics, etc. in the list under Predictions as shown below.\n\n\n\n\n\nIn the predict dialog box, choose the Residuals button and enter a New variable name (e.g. FVC_resid) for the residuals from the regression model.\n\n\n\n\n\nClick OK button when you are done.\n[Command: predict FVC_resid, residuals]\nYou can now check the assumption that the residuals are normally distributed by creating a histogram with the normal curve using Graphics &gt; Histogram as shown in Stata Notes section for Module 2. Below is the histogram dialog box used to produce the graph in Figure 8.5.\n\n\n\n\n\n[Command: histogram FVC_resid, bin(12) frequency normal]"
  },
  {
    "objectID": "08-correlation-and-regression.html#creating-a-scatter-plot-1",
    "href": "08-correlation-and-regression.html#creating-a-scatter-plot-1",
    "title": "6  Correlation and simple linear regression",
    "section": "6.13 Creating a scatter plot",
    "text": "6.13 Creating a scatter plot\nWe can use the plot function to create a scatter plot to explore the association between height and FVC, assigning meaningful labels with the xlab and ylab commands:\n\nplot(x=lung$Height, y=lung$FVC, \n     xlab=\"Height (cm)\", \n     ylab=\"Forced vital capacity (L)\")\n\n\n\n\n\ngf_point(FVC ~ Height, data=lung,\n     xlab=\"Height (cm)\", \n     ylab=\"Forced vital capacity (L)\") +\n  gf_theme(theme_minimal())\n\n\n\n\nTo add a fitted line, we can use the abline() function which adds a straight line to the plot. The equation of this straight line will be determined from the estimated regression line, which we specify with the lm() function, which fits a linear model.\nThe basic syntax of the lm() function is: lm(y ~ x) where y represents the outcome variable, and x represents the explanatory variable. Putting this all together:\n\nplot(x=lung$Height, y=lung$FVC,\n     xlab=\"Height (cm)\",\n     ylab=\"Forced vital capacity (L)\")\n\nabline(lm(lung$FVC ~ lung$Height))\n\n\n\n\n\ngf_point(FVC ~ Height, data=lung,\n     xlab=\"Height (cm)\", \n     ylab=\"Forced vital capacity (L)\") +\n  geom_lm() +\n  gf_theme(theme_minimal())\n\nWarning: Using the `size` aesthetic with geom_line was deprecated in ggplot2 3.4.0.\nℹ Please use the `linewidth` aesthetic instead."
  },
  {
    "objectID": "08-correlation-and-regression.html#calculating-a-correlation-coefficient-1",
    "href": "08-correlation-and-regression.html#calculating-a-correlation-coefficient-1",
    "title": "6  Correlation and simple linear regression",
    "section": "Calculating a correlation coefficient",
    "text": "Calculating a correlation coefficient\nWe can use the corrMatrix function in the Jamovi package to calculate a Pearson’s correlation coefficient:\n\ncorrMatrix(data=lung, vars=c(Height, FVC))\n\n\n CORRELATION MATRIX\n\n Correlation Matrix                                   \n ──────────────────────────────────────────────────── \n                            Height        FVC         \n ──────────────────────────────────────────────────── \n   Height    Pearson's r             —                \n             df                      —                \n             p-value                 —                \n                                                      \n   FVC       Pearson's r     0.6976280            —   \n             df                    118            —   \n             p-value        &lt; .0000001            —   \n ────────────────────────────────────────────────────"
  },
  {
    "objectID": "08-correlation-and-regression.html#fitting-a-simple-linear-regression-model-1",
    "href": "08-correlation-and-regression.html#fitting-a-simple-linear-regression-model-1",
    "title": "6  Correlation and simple linear regression",
    "section": "6.14 Fitting a simple linear regression model",
    "text": "6.14 Fitting a simple linear regression model\nWe can use the lm function to fit a simple linear regression model, specifying the model as y ~ x where y represents the outcome variable, and x represents the explanatory variable. Using mod08_lung_function.rds, we can quantify the relationship between FVC and height:\n\nlm(FVC ~ Height, data=lung)\n\n\nCall:\nlm(formula = FVC ~ Height, data = lung)\n\nCoefficients:\n(Intercept)       Height  \n   -18.8735       0.1408  \n\n\nThe default output from the lm function is rather sparse. We can obtain much more useful information by defining the linear regression model as an object, then using the summary() function:\n\nmodel &lt;- lm(FVC ~ Height, data=lung)\nsummary(model)\n\n\nCall:\nlm(formula = FVC ~ Height, data = lung)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.01139 -0.23643 -0.02082  0.24918  1.31786 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -18.87347    2.19365  -8.604 3.89e-14 ***\nHeight        0.14076    0.01331  10.577  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3965 on 118 degrees of freedom\nMultiple R-squared:  0.4867,    Adjusted R-squared:  0.4823 \nF-statistic: 111.9 on 1 and 118 DF,  p-value: &lt; 2.2e-16\n\n\nFinally, we can obtain 95% confidence intervals for the regression coefficients using the confint function:\n\nconfint(model)\n\n                  2.5 %      97.5 %\n(Intercept) -23.2174967 -14.5294444\nHeight        0.1144042   0.1671092"
  },
  {
    "objectID": "08-correlation-and-regression.html#plotting-residuals-from-a-simple-linear-regression-1",
    "href": "08-correlation-and-regression.html#plotting-residuals-from-a-simple-linear-regression-1",
    "title": "6  Correlation and simple linear regression",
    "section": "6.15 Plotting residuals from a simple linear regression",
    "text": "6.15 Plotting residuals from a simple linear regression\nWe can use the resid function to obtain the residuals from a saved model. These residuals can then be plotted using a histogram in the usual way:\n\nresiduals &lt;- resid(model)\nhist(residuals)\n\n\n\ngf_histogram(~ residuals, bins = 10) +\n  gf_theme(theme_minimal())\n\n\n\n\nA Normal curve can be overlaid if we plot the residuals using a probability scale.\n\nhist(residuals, probability = TRUE,\n     ylim = c(0, 1))\n\ncurve(dnorm(x, mean=mean(residuals), sd=sd(residuals)), \n      col=\"darkblue\", lwd=2, add=TRUE)\n\n\n\ngf_dhistogram(~ residuals, bins = 10)  |&gt; \n  gf_fitdistr(dist = \"dnorm\")\n\n\n\ngf_dhistogram(~ residuals, bins = 10, alpha=0.3)  |&gt; \n  gf_dens(~ residuals, colour= ~\"Distribution of residuals\")  |&gt; \n  gf_fitdistr(dist = \"dnorm\", colour = ~\"Normal distribution\") |&gt; \n  gf_labs(color = \"\") |&gt; \n  gf_theme(theme_light())"
  },
  {
    "objectID": "98.1-appendix1.html",
    "href": "98.1-appendix1.html",
    "title": "Appendix",
    "section": "",
    "text": "Analysis flowchart"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Acock, Alan C. 2010. A Gentle Introduction to\nStata. 3rd ed. College Station, Tex: Stata Press.\n\n\nBland, Martin. 2015. An Introduction to Medical\nStatistics. 4th Edition. Oxford, New York: Oxford University\nPress.\n\n\nBrown, Lawrence D., T. Tony Cai, and Anirban DasGupta. 2001.\n“Interval Estimation for a Binomial\nProportion.” Statistical Science 16 (2): 101–17.\nhttps://www.jstor.org/stable/2676784.\n\n\nDeeks, Jon. 1998. “When Can Odds Ratios Mislead?”\nBMJ 317 (7166): 1155. https://doi.org/10.1136/bmj.317.7166.1155a.\n\n\nDelacre, Marie, Daniël Lakens, and Christophe Leys. 2017. “Why\nPsychologists Should by Default Use Welch’s\nt-Test Instead of Student’s t-Test” 30\n(1): 92. https://doi.org/10.5334/irsp.82.\n\n\nKirkwood, Betty, and Jonathan Sterne. 2001. Essentials of\nMedical Statistics. 2nd edition. Malden, Mass:\nWiley-Blackwell.\n\n\nRuxton, Graeme D. 2006. “The Unequal Variance t-Test Is an\nUnderused Alternative to Student’s t-Test and the\nMann–Whitney U Test.” Behavioral\nEcology 17 (4): 688–90. https://doi.org/10.1093/beheco/ark016.\n\n\nSchmidt, Carsten Oliver, and Thomas Kohlmann. 2008. “When to Use\nthe Odds Ratio or the Relative Risk?” International Journal\nof Public Health 53 (3): 165–67. https://doi.org/10.1007/s00038-008-7068-3.\n\n\nWebb, Penny, Chris Bain, and Andrew Page. 2016. Essential\nEpidemiology: An Introduction for\nStudents and Health Professionals. 3rd\nedition. Cambridge: Cambridge University Press.\n\n\nWest, Robert M. 2021. “Best Practice in Statistics:\nUse the Welch t-Test When Testing the\nDifference Between Two Groups.” Annals of Clinical\nBiochemistry 58 (4): 267–69. https://doi.org/10.1177/0004563221992088."
  }
]